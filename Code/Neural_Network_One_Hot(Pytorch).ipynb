{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Network for CLA Project"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import statements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "from sklearn import model_selection\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.utils.data as utils\n",
    "from torch.autograd import Variable\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import errno\n",
    "import os\n",
    "import sys\n",
    "import Constants\n",
    "import copy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# yearly summer dataset parameters\n",
    "# data processing\n",
    "# sample_bias = 0     # adjust the difference in the number of the two types of samples (no algae vs algae)\n",
    "# test_size = 0.2\n",
    "# batch_size = 96    # batch size for the DataLoaders. previously was 100\n",
    "\n",
    "# NN model\n",
    "# num_features = 21\n",
    "# input_size = num_features     # size of input layer\n",
    "# multiplier = 100         # multiplied by num_features to determine the size of each hidden layer. previously was 100\n",
    "# hidden_size = multiplier * input_size\n",
    "# output_size = 1\n",
    "# learning_rate = 0.01         # learning rate of optimizer. previously was 0.01\n",
    "# num_epochs = 100                # number of epochs\n",
    "\n",
    "# all data summer data set hyperparameters\n",
    "# data processing\n",
    "sample_bias = 0     # adjust the difference in the number of the two types of samples (no algae vs algae)\n",
    "test_size = 0.2\n",
    "batch_size = 96    # batch size for the DataLoaders. previously was 100\n",
    "\n",
    "# NN model\n",
    "num_features = 21\n",
    "input_size = num_features     # size of input layer\n",
    "multiplier = 100         # multiplied by num_features to determine the size of each hidden layer. previously was 100\n",
    "hidden_size = multiplier * input_size\n",
    "output_size = 1\n",
    "learning_rate = 0.1         # learning rate of optimizer. previously was 0.01\n",
    "num_epochs = 10                # number of epochs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read in data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.set_printoptions(threshold=np.inf)  # prints a full matrix rather than an abbreviated matrix\n",
    "\n",
    "# define data and destination paths\n",
    "dest_path = \"/Users/Alliot/Documents/CLA-Project/Data/all-data-no-na/neural-network/\"\n",
    "data_path = \"/Users/Alliot/Documents/CLA-Project/Data/data-sets/\"\n",
    "data_set = \"all_data_summer\"\n",
    "\n",
    "# load data sets\n",
    "X = np.load(data_path + data_set + \"_one_hot.npy\")\n",
    "y = np.load(data_path + data_set + \"_labels.npy\")\n",
    "\n",
    "# manipulate data set. labels are converted to 0, +1 for binary classification; samples are removed uniformly \n",
    "# from the data set so that the disproportionately large number of negative samples (no algae) does \n",
    "# not bias the model.\n",
    "\n",
    "num_alg = 0  # count the number of algae instances\n",
    "num_no_alg = 0  # count the number of no algae instances\n",
    "\n",
    "# Convert labels to binary: -1 for no algae and 1 for algae\n",
    "for i in range(0, len(y)):\n",
    "    if y[i] == 0:\n",
    "        num_no_alg += 1\n",
    "    if y[i] == 1 or y[i] == 2:\n",
    "        y[i] = 1\n",
    "        num_alg += 1\n",
    "\n",
    "# oversample the data set by randomly adding occurences of algae until the difference between the number of algae\n",
    "# samples and no algae samples equals sample_bias (defined below)\n",
    "idx = 0\n",
    "sample_bias = 0\n",
    "length_y = len(y)\n",
    "while num_alg != (num_no_alg + sample_bias):\n",
    "    # circle through the data sets until the difference of num_no_alg and num_alg equals\n",
    "    # the value specified by sample_bias\n",
    "    if idx == (length_y - 1):\n",
    "        idx = 0\n",
    "\n",
    "    if y[idx] == 1:\n",
    "        if np.random.rand() >= 0.5:  # add this sample with some probability\n",
    "            y = np.append(y, y[idx])\n",
    "            X = np.append(X, np.reshape(X[idx, :], newshape=(1, num_features)), axis=0)\n",
    "            num_alg += 1\n",
    "        else:\n",
    "            idx += 1\n",
    "    else:\n",
    "        idx += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Process and split data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# standardize data: remove the mean and variance in each sample\n",
    "num_splits = 2   # do not change\n",
    "sss = model_selection.StratifiedShuffleSplit(n_splits=num_splits, test_size=test_size)\n",
    "\n",
    "idx, _ = sss.split(X, y);\n",
    "train_idx = idx[0]\n",
    "test_idx = idx[1]\n",
    "X_train, X_test = X[train_idx], X[test_idx]\n",
    "y_train, y_test = y[train_idx], y[test_idx]\n",
    "\n",
    "X_train = preprocessing.scale(X_train, axis=1, with_mean=True, with_std=True)\n",
    "X_test = preprocessing.scale(X_test, axis=1, with_mean=True, with_std=True)\n",
    "\n",
    "# convert numpy arrays to pytorch tensors\n",
    "train_set_size = X_train.shape\n",
    "test_set_size = X_test.shape\n",
    "X_train, X_test = torch.from_numpy(X_train), torch.from_numpy(X_test)\n",
    "y_train, y_test = torch.from_numpy(y_train), torch.from_numpy(y_test)\n",
    "\n",
    "# convert pytorch tensors to pytorch TensorDataset\n",
    "train_set = utils.TensorDataset(X_train, y_train)\n",
    "test_set = utils.TensorDataset(X_test, y_test)\n",
    "\n",
    "# create DataLoaders\n",
    "train_loader = utils.DataLoader(train_set, batch_size=batch_size, shuffle=True)\n",
    "test_loader = utils.DataLoader(test_set, batch_size=test_set_size[0], shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define neural network model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CLANet(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(CLANet, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(hidden_size, hidden_size)\n",
    "        self.tanh2 = nn.Tanh()\n",
    "        self.fc3 = nn.Linear(hidden_size, hidden_size)\n",
    "        self.relu3 = nn.ReLU()\n",
    "        self.fc4 = nn.Linear(hidden_size, hidden_size)\n",
    "        self.tanh4 = nn.Tanh()\n",
    "        self.fc5 = nn.Linear(hidden_size, hidden_size)\n",
    "        self.relu5 = nn.ReLU()\n",
    "        self.fc6 = nn.Linear(hidden_size, output_size)     # previously, this was output_size\n",
    "#         self.tanh6 = nn.Tanh()                             # previously, this was the line which was commented out\n",
    "#         self.fc7 = nn.Linear(hidden_size, output_size)\n",
    "#         self.relu7 = nn.ReLU()\n",
    "#         self.fc8 = nn.Linear(hidden_size, hidden_size)\n",
    "#         self.relu8 = nn.ReLU()\n",
    "#         self.fc9 = nn.Linear(hidden_size, output_size)\n",
    "#         self.relu9 = nn.ReLU()\n",
    "#         self.fc10 = nn.Linear(hidden_size, hidden_size)\n",
    "#         self.relu10 = nn.ReLU()\n",
    "#         self.fc11 = nn.Linear(hidden_size, hidden_size)\n",
    "#         self.relu11 = nn.ReLU()\n",
    "#         self.fc12 = nn.Linear(hidden_size, output_size)\n",
    "        self.sig1 = nn.Sigmoid()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        out = self.fc1(x)\n",
    "        out = self.relu1(out)\n",
    "        out = self.fc2(out)\n",
    "        out = self.tanh2(out)\n",
    "        out = self.fc3(out)\n",
    "        out = self.relu3(out)\n",
    "        out = self.fc4(out)\n",
    "        out = self.tanh4(out)\n",
    "        out = self.fc5(out)\n",
    "        out = self.relu5(out)\n",
    "        out = self.fc6(out)\n",
    "#         out = self.tanh6(out)\n",
    "#         out = self.fc7(out)\n",
    "#         out = self.relu7(out)\n",
    "#         out = self.fc8(out)\n",
    "#         out = self.relu8(out)\n",
    "#         out = self.fc9(out)\n",
    "#         out = self.relu9(out)\n",
    "#         out = self.fc10(out)\n",
    "#         out = self.relu10(out)\n",
    "#         out = self.fc11(out)\n",
    "#         out = self.relu11(out)\n",
    "#         out = self.fc12(out)\n",
    "        out = self.sig1(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Instantiate the neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = CLANet(input_size, hidden_size, output_size)\n",
    "criterion = nn.BCELoss()\n",
    "\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate, nesterov=True, momentum=1, dampening=0)\n",
    "# optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate, betas=(0.9, 0.999), eps=1e-8)\n",
    "model.double();     # cast model parameters to double"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/10\n",
      "  Iteration: 1/57, Loss: 14.679, Error: 0.5312\n",
      "  Iteration: 2/57, Loss: 14.679, Error: 0.5312\n",
      "  Iteration: 3/57, Loss: 13.8155, Error: 0.5000\n",
      "  Iteration: 4/57, Loss: 14.9668, Error: 0.5417\n",
      "  Iteration: 5/57, Loss: 14.679, Error: 0.5312\n",
      "  Iteration: 6/57, Loss: 13.5277, Error: 0.4896\n",
      "  Iteration: 7/57, Loss: 13.2399, Error: 0.4792\n",
      "  Iteration: 8/57, Loss: 13.5277, Error: 0.4896\n",
      "  Iteration: 9/57, Loss: 13.2399, Error: 0.4792\n",
      "  Iteration: 10/57, Loss: 12.6642, Error: 0.4583\n",
      "  Iteration: 11/57, Loss: 13.2399, Error: 0.4792\n",
      "  Iteration: 12/57, Loss: 13.8155, Error: 0.5000\n",
      "  Iteration: 13/57, Loss: 15.2546, Error: 0.5521\n",
      "  Iteration: 14/57, Loss: 16.6937, Error: 0.6042\n",
      "  Iteration: 15/57, Loss: 11.8007, Error: 0.4271\n",
      "  Iteration: 16/57, Loss: 13.2399, Error: 0.4792\n",
      "  Iteration: 17/57, Loss: 11.5129, Error: 0.4167\n",
      "  Iteration: 18/57, Loss: 11.8007, Error: 0.4271\n",
      "  Iteration: 19/57, Loss: 13.2399, Error: 0.4792\n",
      "  Iteration: 20/57, Loss: 14.1033, Error: 0.5104\n",
      "  Iteration: 21/57, Loss: 13.5277, Error: 0.4896\n",
      "  Iteration: 22/57, Loss: 16.1181, Error: 0.5833\n",
      "  Iteration: 23/57, Loss: 13.2399, Error: 0.4792\n",
      "  Iteration: 24/57, Loss: 17.2694, Error: 0.6250\n",
      "  Iteration: 25/57, Loss: 13.5277, Error: 0.4896\n",
      "  Iteration: 26/57, Loss: 12.952, Error: 0.4688\n",
      "  Iteration: 27/57, Loss: 13.5277, Error: 0.4896\n",
      "  Iteration: 28/57, Loss: 12.952, Error: 0.4688\n",
      "  Iteration: 29/57, Loss: 13.8155, Error: 0.5000\n",
      "  Iteration: 30/57, Loss: 15.8303, Error: 0.5729\n",
      "  Iteration: 31/57, Loss: 15.8303, Error: 0.5729\n",
      "  Iteration: 32/57, Loss: 16.1181, Error: 0.5833\n",
      "  Iteration: 33/57, Loss: 14.1033, Error: 0.5104\n",
      "  Iteration: 34/57, Loss: 11.8007, Error: 0.4271\n",
      "  Iteration: 35/57, Loss: 12.952, Error: 0.4688\n",
      "  Iteration: 36/57, Loss: 13.5277, Error: 0.4896\n",
      "  Iteration: 37/57, Loss: 12.3764, Error: 0.4479\n",
      "  Iteration: 38/57, Loss: 12.0886, Error: 0.4375\n",
      "  Iteration: 39/57, Loss: 13.5277, Error: 0.4896\n",
      "  Iteration: 40/57, Loss: 13.2399, Error: 0.4792\n",
      "  Iteration: 41/57, Loss: 16.1181, Error: 0.5833\n",
      "  Iteration: 42/57, Loss: 14.3912, Error: 0.5208\n",
      "  Iteration: 43/57, Loss: 11.2251, Error: 0.4062\n",
      "  Iteration: 44/57, Loss: 13.8155, Error: 0.5000\n",
      "  Iteration: 45/57, Loss: 12.952, Error: 0.4688\n",
      "  Iteration: 46/57, Loss: 15.2546, Error: 0.5521\n",
      "  Iteration: 47/57, Loss: 12.6642, Error: 0.4583\n",
      "  Iteration: 48/57, Loss: 11.8007, Error: 0.4271\n",
      "  Iteration: 49/57, Loss: 14.9668, Error: 0.5417\n",
      "  Iteration: 50/57, Loss: 13.5277, Error: 0.4896\n",
      "  Iteration: 51/57, Loss: 12.952, Error: 0.4688\n",
      "  Iteration: 52/57, Loss: 14.3912, Error: 0.5208\n",
      "  Iteration: 53/57, Loss: 16.4059, Error: 0.5938\n",
      "  Iteration: 54/57, Loss: 12.0886, Error: 0.4375\n",
      "  Iteration: 55/57, Loss: 15.2546, Error: 0.5521\n",
      "  Iteration: 56/57, Loss: 14.3912, Error: 0.5208\n",
      "  Iteration: 57/57, Loss: 12.6036, Error: 0.4561\n",
      "Average Error for this Epoch: 0.4996\n",
      "found a better model!\n",
      "Epoch: 2/10\n",
      "  Iteration: 1/57, Loss: 13.8155, Error: 0.5000\n",
      "  Iteration: 2/57, Loss: 11.5129, Error: 0.4167\n",
      "  Iteration: 3/57, Loss: 16.1181, Error: 0.5833\n",
      "  Iteration: 4/57, Loss: 13.8155, Error: 0.5000\n",
      "  Iteration: 5/57, Loss: 16.1181, Error: 0.5833\n",
      "  Iteration: 6/57, Loss: 14.3912, Error: 0.5208\n",
      "  Iteration: 7/57, Loss: 13.5277, Error: 0.4896\n",
      "  Iteration: 8/57, Loss: 14.1033, Error: 0.5104\n",
      "  Iteration: 9/57, Loss: 14.9668, Error: 0.5417\n",
      "  Iteration: 10/57, Loss: 12.3764, Error: 0.4479\n",
      "  Iteration: 11/57, Loss: 12.952, Error: 0.4688\n",
      "  Iteration: 12/57, Loss: 13.8155, Error: 0.5000\n",
      "  Iteration: 13/57, Loss: 13.5277, Error: 0.4896\n",
      "  Iteration: 14/57, Loss: 12.3764, Error: 0.4479\n",
      "  Iteration: 15/57, Loss: 14.1033, Error: 0.5104\n",
      "  Iteration: 16/57, Loss: 12.952, Error: 0.4688\n",
      "  Iteration: 17/57, Loss: 15.5424, Error: 0.5625\n",
      "  Iteration: 18/57, Loss: 11.2251, Error: 0.4062\n",
      "  Iteration: 19/57, Loss: 14.1033, Error: 0.5104\n",
      "  Iteration: 20/57, Loss: 14.3912, Error: 0.5208\n",
      "  Iteration: 21/57, Loss: 13.5277, Error: 0.4896\n",
      "  Iteration: 22/57, Loss: 14.679, Error: 0.5312\n",
      "  Iteration: 23/57, Loss: 12.6642, Error: 0.4583\n",
      "  Iteration: 24/57, Loss: 14.1033, Error: 0.5104\n",
      "  Iteration: 25/57, Loss: 15.8303, Error: 0.5729\n",
      "  Iteration: 26/57, Loss: 16.4059, Error: 0.5938\n",
      "  Iteration: 27/57, Loss: 14.3912, Error: 0.5208\n",
      "  Iteration: 28/57, Loss: 13.5277, Error: 0.4896\n",
      "  Iteration: 29/57, Loss: 12.952, Error: 0.4688\n",
      "  Iteration: 30/57, Loss: 16.1181, Error: 0.5833\n",
      "  Iteration: 31/57, Loss: 12.952, Error: 0.4688\n",
      "  Iteration: 32/57, Loss: 13.5277, Error: 0.4896\n",
      "  Iteration: 33/57, Loss: 10.9373, Error: 0.3958\n",
      "  Iteration: 34/57, Loss: 12.6642, Error: 0.4583\n",
      "  Iteration: 35/57, Loss: 12.3764, Error: 0.4479\n",
      "  Iteration: 36/57, Loss: 12.3764, Error: 0.4479\n",
      "  Iteration: 37/57, Loss: 14.9668, Error: 0.5417\n",
      "  Iteration: 38/57, Loss: 14.9668, Error: 0.5417\n",
      "  Iteration: 39/57, Loss: 13.2399, Error: 0.4792\n",
      "  Iteration: 40/57, Loss: 13.8155, Error: 0.5000\n",
      "  Iteration: 41/57, Loss: 14.9668, Error: 0.5417\n",
      "  Iteration: 42/57, Loss: 14.3912, Error: 0.5208\n",
      "  Iteration: 43/57, Loss: 14.3912, Error: 0.5208\n",
      "  Iteration: 44/57, Loss: 12.952, Error: 0.4688\n",
      "  Iteration: 45/57, Loss: 14.9668, Error: 0.5417\n",
      "  Iteration: 46/57, Loss: 15.5424, Error: 0.5625\n",
      "  Iteration: 47/57, Loss: 14.1033, Error: 0.5104\n",
      "  Iteration: 48/57, Loss: 11.8007, Error: 0.4271\n",
      "  Iteration: 49/57, Loss: 14.1033, Error: 0.5104\n",
      "  Iteration: 50/57, Loss: 14.1033, Error: 0.5104\n",
      "  Iteration: 51/57, Loss: 14.9668, Error: 0.5417\n",
      "  Iteration: 52/57, Loss: 9.49816, Error: 0.3438\n",
      "  Iteration: 53/57, Loss: 14.1033, Error: 0.5104\n",
      "  Iteration: 54/57, Loss: 13.5277, Error: 0.4896\n",
      "  Iteration: 55/57, Loss: 14.1033, Error: 0.5104\n",
      "  Iteration: 56/57, Loss: 14.679, Error: 0.5312\n",
      "  Iteration: 57/57, Loss: 13.0884, Error: 0.4737\n",
      "Average Error for this Epoch: 0.4997\n",
      "Epoch: 3/10\n",
      "  Iteration: 1/57, Loss: 13.5277, Error: 0.4896\n",
      "  Iteration: 2/57, Loss: 13.8155, Error: 0.5000\n",
      "  Iteration: 3/57, Loss: 14.1033, Error: 0.5104\n",
      "  Iteration: 4/57, Loss: 12.952, Error: 0.4688\n",
      "  Iteration: 5/57, Loss: 15.8303, Error: 0.5729\n",
      "  Iteration: 6/57, Loss: 11.5129, Error: 0.4167\n",
      "  Iteration: 7/57, Loss: 12.0886, Error: 0.4375\n",
      "  Iteration: 8/57, Loss: 11.5129, Error: 0.4167\n",
      "  Iteration: 9/57, Loss: 13.8155, Error: 0.5000\n",
      "  Iteration: 10/57, Loss: 13.5277, Error: 0.4896\n",
      "  Iteration: 11/57, Loss: 14.679, Error: 0.5312\n",
      "  Iteration: 12/57, Loss: 12.3764, Error: 0.4479\n",
      "  Iteration: 13/57, Loss: 15.2546, Error: 0.5521\n",
      "  Iteration: 14/57, Loss: 12.3764, Error: 0.4479\n",
      "  Iteration: 15/57, Loss: 16.4059, Error: 0.5938\n",
      "  Iteration: 16/57, Loss: 16.1181, Error: 0.5833\n",
      "  Iteration: 17/57, Loss: 14.3912, Error: 0.5208\n",
      "  Iteration: 18/57, Loss: 12.3764, Error: 0.4479\n",
      "  Iteration: 19/57, Loss: 12.0886, Error: 0.4375\n",
      "  Iteration: 20/57, Loss: 13.5277, Error: 0.4896\n",
      "  Iteration: 21/57, Loss: 15.2546, Error: 0.5521\n",
      "  Iteration: 22/57, Loss: 14.3912, Error: 0.5208\n",
      "  Iteration: 23/57, Loss: 12.0886, Error: 0.4375\n",
      "  Iteration: 24/57, Loss: 12.0886, Error: 0.4375\n",
      "  Iteration: 25/57, Loss: 12.3764, Error: 0.4479\n",
      "  Iteration: 26/57, Loss: 14.679, Error: 0.5312\n",
      "  Iteration: 27/57, Loss: 14.679, Error: 0.5312\n",
      "  Iteration: 28/57, Loss: 13.5277, Error: 0.4896\n",
      "  Iteration: 29/57, Loss: 12.952, Error: 0.4688\n",
      "  Iteration: 30/57, Loss: 15.8303, Error: 0.5729\n",
      "  Iteration: 31/57, Loss: 14.9668, Error: 0.5417\n",
      "  Iteration: 32/57, Loss: 14.3912, Error: 0.5208\n",
      "  Iteration: 33/57, Loss: 12.952, Error: 0.4688\n",
      "  Iteration: 34/57, Loss: 13.8155, Error: 0.5000\n",
      "  Iteration: 35/57, Loss: 15.2546, Error: 0.5521\n",
      "  Iteration: 36/57, Loss: 14.3912, Error: 0.5208\n",
      "  Iteration: 37/57, Loss: 12.0886, Error: 0.4375\n",
      "  Iteration: 38/57, Loss: 14.9668, Error: 0.5417\n",
      "  Iteration: 39/57, Loss: 13.2399, Error: 0.4792\n",
      "  Iteration: 40/57, Loss: 12.952, Error: 0.4688\n",
      "  Iteration: 41/57, Loss: 12.6642, Error: 0.4583\n",
      "  Iteration: 42/57, Loss: 13.8155, Error: 0.5000\n",
      "  Iteration: 43/57, Loss: 14.3912, Error: 0.5208\n",
      "  Iteration: 44/57, Loss: 15.8303, Error: 0.5729\n",
      "  Iteration: 45/57, Loss: 13.5277, Error: 0.4896\n",
      "  Iteration: 46/57, Loss: 14.679, Error: 0.5312\n",
      "  Iteration: 47/57, Loss: 12.3764, Error: 0.4479\n",
      "  Iteration: 48/57, Loss: 14.1033, Error: 0.5104\n",
      "  Iteration: 49/57, Loss: 12.6642, Error: 0.4583\n",
      "  Iteration: 50/57, Loss: 16.1181, Error: 0.5833\n",
      "  Iteration: 51/57, Loss: 14.1033, Error: 0.5104\n",
      "  Iteration: 52/57, Loss: 14.679, Error: 0.5312\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Iteration: 53/57, Loss: 12.952, Error: 0.4688\n",
      "  Iteration: 54/57, Loss: 15.2546, Error: 0.5521\n",
      "  Iteration: 55/57, Loss: 13.8155, Error: 0.5000\n",
      "  Iteration: 56/57, Loss: 11.8007, Error: 0.4271\n",
      "  Iteration: 57/57, Loss: 16.4817, Error: 0.5965\n",
      "Average Error for this Epoch: 0.5006\n",
      "Epoch: 4/10\n",
      "  Iteration: 1/57, Loss: 12.0886, Error: 0.4375\n",
      "  Iteration: 2/57, Loss: 14.3912, Error: 0.5208\n",
      "  Iteration: 3/57, Loss: 12.6642, Error: 0.4583\n",
      "  Iteration: 4/57, Loss: 13.5277, Error: 0.4896\n",
      "  Iteration: 5/57, Loss: 14.3912, Error: 0.5208\n",
      "  Iteration: 6/57, Loss: 15.8303, Error: 0.5729\n",
      "  Iteration: 7/57, Loss: 12.952, Error: 0.4688\n",
      "  Iteration: 8/57, Loss: 14.9668, Error: 0.5417\n",
      "  Iteration: 9/57, Loss: 14.9668, Error: 0.5417\n",
      "  Iteration: 10/57, Loss: 11.5129, Error: 0.4167\n",
      "  Iteration: 11/57, Loss: 13.2399, Error: 0.4792\n",
      "  Iteration: 12/57, Loss: 12.3764, Error: 0.4479\n",
      "  Iteration: 13/57, Loss: 15.2546, Error: 0.5521\n",
      "  Iteration: 14/57, Loss: 12.952, Error: 0.4688\n",
      "  Iteration: 15/57, Loss: 13.8155, Error: 0.5000\n",
      "  Iteration: 16/57, Loss: 14.9668, Error: 0.5417\n",
      "  Iteration: 17/57, Loss: 12.952, Error: 0.4688\n",
      "  Iteration: 18/57, Loss: 12.6642, Error: 0.4583\n",
      "  Iteration: 19/57, Loss: 12.6642, Error: 0.4583\n",
      "  Iteration: 20/57, Loss: 14.9668, Error: 0.5417\n",
      "  Iteration: 21/57, Loss: 15.2546, Error: 0.5521\n",
      "  Iteration: 22/57, Loss: 11.8007, Error: 0.4271\n",
      "  Iteration: 23/57, Loss: 13.2399, Error: 0.4792\n",
      "  Iteration: 24/57, Loss: 12.952, Error: 0.4688\n",
      "  Iteration: 25/57, Loss: 14.9668, Error: 0.5417\n",
      "  Iteration: 26/57, Loss: 16.6937, Error: 0.6042\n",
      "  Iteration: 27/57, Loss: 14.1033, Error: 0.5104\n",
      "  Iteration: 28/57, Loss: 13.5277, Error: 0.4896\n",
      "  Iteration: 29/57, Loss: 13.8155, Error: 0.5000\n",
      "  Iteration: 30/57, Loss: 15.5424, Error: 0.5625\n",
      "  Iteration: 31/57, Loss: 15.5424, Error: 0.5625\n",
      "  Iteration: 32/57, Loss: 14.679, Error: 0.5312\n",
      "  Iteration: 33/57, Loss: 14.1033, Error: 0.5104\n",
      "  Iteration: 34/57, Loss: 15.5424, Error: 0.5625\n",
      "  Iteration: 35/57, Loss: 12.952, Error: 0.4688\n",
      "  Iteration: 36/57, Loss: 14.1033, Error: 0.5104\n",
      "  Iteration: 37/57, Loss: 14.1033, Error: 0.5104\n",
      "  Iteration: 38/57, Loss: 13.2399, Error: 0.4792\n",
      "  Iteration: 39/57, Loss: 12.6642, Error: 0.4583\n",
      "  Iteration: 40/57, Loss: 16.1181, Error: 0.5833\n",
      "  Iteration: 41/57, Loss: 14.9668, Error: 0.5417\n",
      "  Iteration: 42/57, Loss: 10.9373, Error: 0.3958\n",
      "  Iteration: 43/57, Loss: 12.3764, Error: 0.4479\n",
      "  Iteration: 44/57, Loss: 14.9668, Error: 0.5417\n",
      "  Iteration: 45/57, Loss: 14.1033, Error: 0.5104\n",
      "  Iteration: 46/57, Loss: 12.6642, Error: 0.4583\n",
      "  Iteration: 47/57, Loss: 13.8155, Error: 0.5000\n",
      "  Iteration: 48/57, Loss: 12.0886, Error: 0.4375\n",
      "  Iteration: 49/57, Loss: 13.8155, Error: 0.5000\n",
      "  Iteration: 50/57, Loss: 15.5424, Error: 0.5625\n",
      "  Iteration: 51/57, Loss: 12.952, Error: 0.4688\n",
      "  Iteration: 52/57, Loss: 14.1033, Error: 0.5104\n",
      "  Iteration: 53/57, Loss: 13.2399, Error: 0.4792\n",
      "  Iteration: 54/57, Loss: 13.2399, Error: 0.4792\n",
      "  Iteration: 55/57, Loss: 12.3764, Error: 0.4479\n",
      "  Iteration: 56/57, Loss: 14.3912, Error: 0.5208\n",
      "  Iteration: 57/57, Loss: 13.5731, Error: 0.4912\n",
      "Average Error for this Epoch: 0.4998\n",
      "Epoch: 5/10\n",
      "  Iteration: 1/57, Loss: 15.5424, Error: 0.5625\n",
      "  Iteration: 2/57, Loss: 12.952, Error: 0.4688\n",
      "  Iteration: 3/57, Loss: 12.952, Error: 0.4688\n",
      "  Iteration: 4/57, Loss: 13.2399, Error: 0.4792\n",
      "  Iteration: 5/57, Loss: 13.5277, Error: 0.4896\n",
      "  Iteration: 6/57, Loss: 14.679, Error: 0.5312\n",
      "  Iteration: 7/57, Loss: 12.952, Error: 0.4688\n",
      "  Iteration: 8/57, Loss: 10.9373, Error: 0.3958\n",
      "  Iteration: 9/57, Loss: 11.5129, Error: 0.4167\n",
      "  Iteration: 10/57, Loss: 15.2546, Error: 0.5521\n",
      "  Iteration: 11/57, Loss: 14.1033, Error: 0.5104\n",
      "  Iteration: 12/57, Loss: 11.8007, Error: 0.4271\n",
      "  Iteration: 13/57, Loss: 15.2546, Error: 0.5521\n",
      "  Iteration: 14/57, Loss: 14.9668, Error: 0.5417\n",
      "  Iteration: 15/57, Loss: 12.0886, Error: 0.4375\n",
      "  Iteration: 16/57, Loss: 13.8155, Error: 0.5000\n",
      "  Iteration: 17/57, Loss: 12.6642, Error: 0.4583\n",
      "  Iteration: 18/57, Loss: 13.2399, Error: 0.4792\n",
      "  Iteration: 19/57, Loss: 14.3912, Error: 0.5208\n",
      "  Iteration: 20/57, Loss: 12.6642, Error: 0.4583\n",
      "  Iteration: 21/57, Loss: 15.8303, Error: 0.5729\n",
      "  Iteration: 22/57, Loss: 16.4059, Error: 0.5938\n",
      "  Iteration: 23/57, Loss: 14.1033, Error: 0.5104\n",
      "  Iteration: 24/57, Loss: 15.2546, Error: 0.5521\n",
      "  Iteration: 25/57, Loss: 12.3764, Error: 0.4479\n",
      "  Iteration: 26/57, Loss: 13.2399, Error: 0.4792\n",
      "  Iteration: 27/57, Loss: 14.3912, Error: 0.5208\n",
      "  Iteration: 28/57, Loss: 12.952, Error: 0.4688\n",
      "  Iteration: 29/57, Loss: 12.3764, Error: 0.4479\n",
      "  Iteration: 30/57, Loss: 14.1033, Error: 0.5104\n",
      "  Iteration: 31/57, Loss: 14.1033, Error: 0.5104\n",
      "  Iteration: 32/57, Loss: 12.0886, Error: 0.4375\n",
      "  Iteration: 33/57, Loss: 16.6937, Error: 0.6042\n",
      "  Iteration: 34/57, Loss: 15.2546, Error: 0.5521\n",
      "  Iteration: 35/57, Loss: 13.8155, Error: 0.5000\n",
      "  Iteration: 36/57, Loss: 15.2546, Error: 0.5521\n",
      "  Iteration: 37/57, Loss: 15.5424, Error: 0.5625\n",
      "  Iteration: 38/57, Loss: 14.1033, Error: 0.5104\n",
      "  Iteration: 39/57, Loss: 14.3912, Error: 0.5208\n",
      "  Iteration: 40/57, Loss: 14.679, Error: 0.5312\n",
      "  Iteration: 41/57, Loss: 12.6642, Error: 0.4583\n",
      "  Iteration: 42/57, Loss: 14.9668, Error: 0.5417\n",
      "  Iteration: 43/57, Loss: 12.6642, Error: 0.4583\n",
      "  Iteration: 44/57, Loss: 13.2399, Error: 0.4792\n",
      "  Iteration: 45/57, Loss: 11.2251, Error: 0.4062\n",
      "  Iteration: 46/57, Loss: 12.6642, Error: 0.4583\n",
      "  Iteration: 47/57, Loss: 15.8303, Error: 0.5729\n",
      "  Iteration: 48/57, Loss: 16.1181, Error: 0.5833\n",
      "  Iteration: 49/57, Loss: 15.2546, Error: 0.5521\n",
      "  Iteration: 50/57, Loss: 16.4059, Error: 0.5938\n",
      "  Iteration: 51/57, Loss: 10.3616, Error: 0.3750\n",
      "  Iteration: 52/57, Loss: 12.952, Error: 0.4688\n",
      "  Iteration: 53/57, Loss: 13.2399, Error: 0.4792\n",
      "  Iteration: 54/57, Loss: 14.679, Error: 0.5312\n",
      "  Iteration: 55/57, Loss: 12.6642, Error: 0.4583\n",
      "  Iteration: 56/57, Loss: 13.2399, Error: 0.4792\n",
      "  Iteration: 57/57, Loss: 13.5731, Error: 0.4912\n",
      "Average Error for this Epoch: 0.4998\n",
      "Epoch: 6/10\n",
      "  Iteration: 1/57, Loss: 14.1033, Error: 0.5104\n",
      "  Iteration: 2/57, Loss: 13.2399, Error: 0.4792\n",
      "  Iteration: 3/57, Loss: 12.952, Error: 0.4688\n",
      "  Iteration: 4/57, Loss: 12.0886, Error: 0.4375\n",
      "  Iteration: 5/57, Loss: 14.3912, Error: 0.5208\n",
      "  Iteration: 6/57, Loss: 12.6642, Error: 0.4583\n",
      "  Iteration: 7/57, Loss: 14.1033, Error: 0.5104\n",
      "  Iteration: 8/57, Loss: 15.8303, Error: 0.5729\n",
      "  Iteration: 9/57, Loss: 13.5277, Error: 0.4896\n",
      "  Iteration: 10/57, Loss: 12.3764, Error: 0.4479\n",
      "  Iteration: 11/57, Loss: 14.9668, Error: 0.5417\n",
      "  Iteration: 12/57, Loss: 14.679, Error: 0.5312\n",
      "  Iteration: 13/57, Loss: 14.9668, Error: 0.5417\n",
      "  Iteration: 14/57, Loss: 16.9816, Error: 0.6146\n",
      "  Iteration: 15/57, Loss: 13.5277, Error: 0.4896\n",
      "  Iteration: 16/57, Loss: 15.5424, Error: 0.5625\n",
      "  Iteration: 17/57, Loss: 13.5277, Error: 0.4896\n",
      "  Iteration: 18/57, Loss: 14.679, Error: 0.5312\n",
      "  Iteration: 19/57, Loss: 13.2399, Error: 0.4792\n",
      "  Iteration: 20/57, Loss: 13.5277, Error: 0.4896\n",
      "  Iteration: 21/57, Loss: 11.5129, Error: 0.4167\n",
      "  Iteration: 22/57, Loss: 14.1033, Error: 0.5104\n",
      "  Iteration: 23/57, Loss: 12.0886, Error: 0.4375\n",
      "  Iteration: 24/57, Loss: 12.6642, Error: 0.4583\n",
      "  Iteration: 25/57, Loss: 13.8155, Error: 0.5000\n",
      "  Iteration: 26/57, Loss: 12.0886, Error: 0.4375\n",
      "  Iteration: 27/57, Loss: 13.5277, Error: 0.4896\n",
      "  Iteration: 28/57, Loss: 13.8155, Error: 0.5000\n",
      "  Iteration: 29/57, Loss: 13.8155, Error: 0.5000\n",
      "  Iteration: 30/57, Loss: 13.8155, Error: 0.5000\n",
      "  Iteration: 31/57, Loss: 15.5424, Error: 0.5625\n",
      "  Iteration: 32/57, Loss: 11.8007, Error: 0.4271\n",
      "  Iteration: 33/57, Loss: 13.8155, Error: 0.5000\n",
      "  Iteration: 34/57, Loss: 14.679, Error: 0.5312\n",
      "  Iteration: 35/57, Loss: 12.952, Error: 0.4688\n",
      "  Iteration: 36/57, Loss: 13.2399, Error: 0.4792\n",
      "  Iteration: 37/57, Loss: 14.9668, Error: 0.5417\n",
      "  Iteration: 38/57, Loss: 15.5424, Error: 0.5625\n",
      "  Iteration: 39/57, Loss: 13.5277, Error: 0.4896\n",
      "  Iteration: 40/57, Loss: 13.8155, Error: 0.5000\n",
      "  Iteration: 41/57, Loss: 14.679, Error: 0.5312\n",
      "  Iteration: 42/57, Loss: 15.8303, Error: 0.5729\n",
      "  Iteration: 43/57, Loss: 13.2399, Error: 0.4792\n",
      "  Iteration: 44/57, Loss: 11.8007, Error: 0.4271\n",
      "  Iteration: 45/57, Loss: 15.5424, Error: 0.5625\n",
      "  Iteration: 46/57, Loss: 12.952, Error: 0.4688\n",
      "  Iteration: 47/57, Loss: 12.0886, Error: 0.4375\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Iteration: 48/57, Loss: 12.3764, Error: 0.4479\n",
      "  Iteration: 49/57, Loss: 12.3764, Error: 0.4479\n",
      "  Iteration: 50/57, Loss: 16.1181, Error: 0.5833\n",
      "  Iteration: 51/57, Loss: 11.5129, Error: 0.4167\n",
      "  Iteration: 52/57, Loss: 13.2399, Error: 0.4792\n",
      "  Iteration: 53/57, Loss: 14.9668, Error: 0.5417\n",
      "  Iteration: 54/57, Loss: 14.9668, Error: 0.5417\n",
      "  Iteration: 55/57, Loss: 14.3912, Error: 0.5208\n",
      "  Iteration: 56/57, Loss: 14.3912, Error: 0.5208\n",
      "  Iteration: 57/57, Loss: 15.5122, Error: 0.5614\n",
      "Average Error for this Epoch: 0.5003\n",
      "Epoch: 7/10\n",
      "  Iteration: 1/57, Loss: 10.0738, Error: 0.3646\n",
      "  Iteration: 2/57, Loss: 15.2546, Error: 0.5521\n",
      "  Iteration: 3/57, Loss: 12.952, Error: 0.4688\n",
      "  Iteration: 4/57, Loss: 14.679, Error: 0.5312\n",
      "  Iteration: 5/57, Loss: 14.1033, Error: 0.5104\n",
      "  Iteration: 6/57, Loss: 13.8155, Error: 0.5000\n",
      "  Iteration: 7/57, Loss: 15.5424, Error: 0.5625\n",
      "  Iteration: 8/57, Loss: 12.6642, Error: 0.4583\n",
      "  Iteration: 9/57, Loss: 14.9668, Error: 0.5417\n",
      "  Iteration: 10/57, Loss: 13.5277, Error: 0.4896\n",
      "  Iteration: 11/57, Loss: 14.679, Error: 0.5312\n",
      "  Iteration: 12/57, Loss: 12.952, Error: 0.4688\n",
      "  Iteration: 13/57, Loss: 15.2546, Error: 0.5521\n",
      "  Iteration: 14/57, Loss: 13.5277, Error: 0.4896\n",
      "  Iteration: 15/57, Loss: 14.679, Error: 0.5312\n",
      "  Iteration: 16/57, Loss: 14.9668, Error: 0.5417\n",
      "  Iteration: 17/57, Loss: 13.8155, Error: 0.5000\n",
      "  Iteration: 18/57, Loss: 13.8155, Error: 0.5000\n",
      "  Iteration: 19/57, Loss: 12.952, Error: 0.4688\n",
      "  Iteration: 20/57, Loss: 13.2399, Error: 0.4792\n",
      "  Iteration: 21/57, Loss: 11.8007, Error: 0.4271\n",
      "  Iteration: 22/57, Loss: 13.2399, Error: 0.4792\n",
      "  Iteration: 23/57, Loss: 12.3764, Error: 0.4479\n",
      "  Iteration: 24/57, Loss: 15.5424, Error: 0.5625\n",
      "  Iteration: 25/57, Loss: 14.9668, Error: 0.5417\n",
      "  Iteration: 26/57, Loss: 14.1033, Error: 0.5104\n",
      "  Iteration: 27/57, Loss: 14.679, Error: 0.5312\n",
      "  Iteration: 28/57, Loss: 12.6642, Error: 0.4583\n",
      "  Iteration: 29/57, Loss: 13.5277, Error: 0.4896\n",
      "  Iteration: 30/57, Loss: 12.3764, Error: 0.4479\n",
      "  Iteration: 31/57, Loss: 16.1181, Error: 0.5833\n",
      "  Iteration: 32/57, Loss: 13.8155, Error: 0.5000\n",
      "  Iteration: 33/57, Loss: 14.9668, Error: 0.5417\n",
      "  Iteration: 34/57, Loss: 12.6642, Error: 0.4583\n",
      "  Iteration: 35/57, Loss: 14.679, Error: 0.5312\n",
      "  Iteration: 36/57, Loss: 15.8303, Error: 0.5729\n",
      "  Iteration: 37/57, Loss: 14.1033, Error: 0.5104\n",
      "  Iteration: 38/57, Loss: 14.679, Error: 0.5312\n",
      "  Iteration: 39/57, Loss: 11.2251, Error: 0.4062\n",
      "  Iteration: 40/57, Loss: 12.3764, Error: 0.4479\n",
      "  Iteration: 41/57, Loss: 13.8155, Error: 0.5000\n",
      "  Iteration: 42/57, Loss: 13.8155, Error: 0.5000\n",
      "  Iteration: 43/57, Loss: 11.8007, Error: 0.4271\n",
      "  Iteration: 44/57, Loss: 14.9668, Error: 0.5417\n",
      "  Iteration: 45/57, Loss: 15.5424, Error: 0.5625\n",
      "  Iteration: 46/57, Loss: 13.8155, Error: 0.5000\n",
      "  Iteration: 47/57, Loss: 12.952, Error: 0.4688\n",
      "  Iteration: 48/57, Loss: 13.8155, Error: 0.5000\n",
      "  Iteration: 49/57, Loss: 14.3912, Error: 0.5208\n",
      "  Iteration: 50/57, Loss: 14.679, Error: 0.5312\n",
      "  Iteration: 51/57, Loss: 12.952, Error: 0.4688\n",
      "  Iteration: 52/57, Loss: 13.2399, Error: 0.4792\n",
      "  Iteration: 53/57, Loss: 12.952, Error: 0.4688\n",
      "  Iteration: 54/57, Loss: 12.6642, Error: 0.4583\n",
      "  Iteration: 55/57, Loss: 15.5424, Error: 0.5625\n",
      "  Iteration: 56/57, Loss: 12.3764, Error: 0.4479\n",
      "  Iteration: 57/57, Loss: 15.5122, Error: 0.5614\n",
      "Average Error for this Epoch: 0.5003\n",
      "Epoch: 8/10\n",
      "  Iteration: 1/57, Loss: 13.2399, Error: 0.4792\n",
      "  Iteration: 2/57, Loss: 14.3912, Error: 0.5208\n",
      "  Iteration: 3/57, Loss: 15.2546, Error: 0.5521\n",
      "  Iteration: 4/57, Loss: 15.2546, Error: 0.5521\n",
      "  Iteration: 5/57, Loss: 13.2399, Error: 0.4792\n",
      "  Iteration: 6/57, Loss: 14.679, Error: 0.5312\n",
      "  Iteration: 7/57, Loss: 11.2251, Error: 0.4062\n",
      "  Iteration: 8/57, Loss: 15.8303, Error: 0.5729\n",
      "  Iteration: 9/57, Loss: 16.4059, Error: 0.5938\n",
      "  Iteration: 10/57, Loss: 11.5129, Error: 0.4167\n",
      "  Iteration: 11/57, Loss: 12.0886, Error: 0.4375\n",
      "  Iteration: 12/57, Loss: 14.1033, Error: 0.5104\n",
      "  Iteration: 13/57, Loss: 15.2546, Error: 0.5521\n",
      "  Iteration: 14/57, Loss: 13.5277, Error: 0.4896\n",
      "  Iteration: 15/57, Loss: 12.0886, Error: 0.4375\n",
      "  Iteration: 16/57, Loss: 12.6642, Error: 0.4583\n",
      "  Iteration: 17/57, Loss: 16.4059, Error: 0.5938\n",
      "  Iteration: 18/57, Loss: 12.0886, Error: 0.4375\n",
      "  Iteration: 19/57, Loss: 14.1033, Error: 0.5104\n",
      "  Iteration: 20/57, Loss: 13.2399, Error: 0.4792\n",
      "  Iteration: 21/57, Loss: 14.679, Error: 0.5312\n",
      "  Iteration: 22/57, Loss: 10.9373, Error: 0.3958\n",
      "  Iteration: 23/57, Loss: 11.8007, Error: 0.4271\n",
      "  Iteration: 24/57, Loss: 14.1033, Error: 0.5104\n",
      "  Iteration: 25/57, Loss: 13.2399, Error: 0.4792\n",
      "  Iteration: 26/57, Loss: 12.952, Error: 0.4688\n",
      "  Iteration: 27/57, Loss: 14.1033, Error: 0.5104\n",
      "  Iteration: 28/57, Loss: 12.0886, Error: 0.4375\n",
      "  Iteration: 29/57, Loss: 14.679, Error: 0.5312\n",
      "  Iteration: 30/57, Loss: 14.3912, Error: 0.5208\n",
      "  Iteration: 31/57, Loss: 13.8155, Error: 0.5000\n",
      "  Iteration: 32/57, Loss: 16.4059, Error: 0.5938\n",
      "  Iteration: 33/57, Loss: 12.3764, Error: 0.4479\n",
      "  Iteration: 34/57, Loss: 14.1033, Error: 0.5104\n",
      "  Iteration: 35/57, Loss: 13.5277, Error: 0.4896\n",
      "  Iteration: 36/57, Loss: 17.5572, Error: 0.6354\n",
      "  Iteration: 37/57, Loss: 14.679, Error: 0.5312\n",
      "  Iteration: 38/57, Loss: 13.5277, Error: 0.4896\n",
      "  Iteration: 39/57, Loss: 14.1033, Error: 0.5104\n",
      "  Iteration: 40/57, Loss: 12.3764, Error: 0.4479\n",
      "  Iteration: 41/57, Loss: 12.6642, Error: 0.4583\n",
      "  Iteration: 42/57, Loss: 14.9668, Error: 0.5417\n",
      "  Iteration: 43/57, Loss: 14.679, Error: 0.5312\n",
      "  Iteration: 44/57, Loss: 14.679, Error: 0.5312\n",
      "  Iteration: 45/57, Loss: 14.1033, Error: 0.5104\n",
      "  Iteration: 46/57, Loss: 15.8303, Error: 0.5729\n",
      "  Iteration: 47/57, Loss: 13.2399, Error: 0.4792\n",
      "  Iteration: 48/57, Loss: 14.679, Error: 0.5312\n",
      "  Iteration: 49/57, Loss: 12.6642, Error: 0.4583\n",
      "  Iteration: 50/57, Loss: 13.8155, Error: 0.5000\n",
      "  Iteration: 51/57, Loss: 12.952, Error: 0.4688\n",
      "  Iteration: 52/57, Loss: 14.1033, Error: 0.5104\n",
      "  Iteration: 53/57, Loss: 12.3764, Error: 0.4479\n",
      "  Iteration: 54/57, Loss: 13.5277, Error: 0.4896\n",
      "  Iteration: 55/57, Loss: 14.9668, Error: 0.5417\n",
      "  Iteration: 56/57, Loss: 13.2399, Error: 0.4792\n",
      "  Iteration: 57/57, Loss: 12.1189, Error: 0.4386\n",
      "Average Error for this Epoch: 0.4995\n",
      "found a better model!\n",
      "Epoch: 9/10\n",
      "  Iteration: 1/57, Loss: 15.2546, Error: 0.5521\n",
      "  Iteration: 2/57, Loss: 15.2546, Error: 0.5521\n",
      "  Iteration: 3/57, Loss: 11.8007, Error: 0.4271\n",
      "  Iteration: 4/57, Loss: 16.6937, Error: 0.6042\n",
      "  Iteration: 5/57, Loss: 13.2399, Error: 0.4792\n",
      "  Iteration: 6/57, Loss: 13.8155, Error: 0.5000\n",
      "  Iteration: 7/57, Loss: 12.3764, Error: 0.4479\n",
      "  Iteration: 8/57, Loss: 13.5277, Error: 0.4896\n",
      "  Iteration: 9/57, Loss: 12.952, Error: 0.4688\n",
      "  Iteration: 10/57, Loss: 13.2399, Error: 0.4792\n",
      "  Iteration: 11/57, Loss: 12.0886, Error: 0.4375\n",
      "  Iteration: 12/57, Loss: 12.6642, Error: 0.4583\n",
      "  Iteration: 13/57, Loss: 11.8007, Error: 0.4271\n",
      "  Iteration: 14/57, Loss: 14.1033, Error: 0.5104\n",
      "  Iteration: 15/57, Loss: 14.9668, Error: 0.5417\n",
      "  Iteration: 16/57, Loss: 13.5277, Error: 0.4896\n",
      "  Iteration: 17/57, Loss: 16.4059, Error: 0.5938\n",
      "  Iteration: 18/57, Loss: 16.4059, Error: 0.5938\n",
      "  Iteration: 19/57, Loss: 14.9668, Error: 0.5417\n",
      "  Iteration: 20/57, Loss: 15.8303, Error: 0.5729\n",
      "  Iteration: 21/57, Loss: 14.3912, Error: 0.5208\n",
      "  Iteration: 22/57, Loss: 13.2399, Error: 0.4792\n",
      "  Iteration: 23/57, Loss: 14.9668, Error: 0.5417\n",
      "  Iteration: 24/57, Loss: 15.2546, Error: 0.5521\n",
      "  Iteration: 25/57, Loss: 12.952, Error: 0.4688\n",
      "  Iteration: 26/57, Loss: 15.2546, Error: 0.5521\n",
      "  Iteration: 27/57, Loss: 11.5129, Error: 0.4167\n",
      "  Iteration: 28/57, Loss: 14.1033, Error: 0.5104\n",
      "  Iteration: 29/57, Loss: 12.3764, Error: 0.4479\n",
      "  Iteration: 30/57, Loss: 11.8007, Error: 0.4271\n",
      "  Iteration: 31/57, Loss: 13.5277, Error: 0.4896\n",
      "  Iteration: 32/57, Loss: 15.8303, Error: 0.5729\n",
      "  Iteration: 33/57, Loss: 12.952, Error: 0.4688\n",
      "  Iteration: 34/57, Loss: 11.5129, Error: 0.4167\n",
      "  Iteration: 35/57, Loss: 13.8155, Error: 0.5000\n",
      "  Iteration: 36/57, Loss: 14.9668, Error: 0.5417\n",
      "  Iteration: 37/57, Loss: 14.1033, Error: 0.5104\n",
      "  Iteration: 38/57, Loss: 14.679, Error: 0.5312\n",
      "  Iteration: 39/57, Loss: 13.2399, Error: 0.4792\n",
      "  Iteration: 40/57, Loss: 14.3912, Error: 0.5208\n",
      "  Iteration: 41/57, Loss: 13.2399, Error: 0.4792\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Iteration: 42/57, Loss: 14.679, Error: 0.5312\n",
      "  Iteration: 43/57, Loss: 13.2399, Error: 0.4792\n",
      "  Iteration: 44/57, Loss: 15.2546, Error: 0.5521\n",
      "  Iteration: 45/57, Loss: 12.3764, Error: 0.4479\n",
      "  Iteration: 46/57, Loss: 12.0886, Error: 0.4375\n",
      "  Iteration: 47/57, Loss: 15.5424, Error: 0.5625\n",
      "  Iteration: 48/57, Loss: 13.8155, Error: 0.5000\n",
      "  Iteration: 49/57, Loss: 15.2546, Error: 0.5521\n",
      "  Iteration: 50/57, Loss: 14.1033, Error: 0.5104\n",
      "  Iteration: 51/57, Loss: 10.6495, Error: 0.3854\n",
      "  Iteration: 52/57, Loss: 14.679, Error: 0.5312\n",
      "  Iteration: 53/57, Loss: 16.1181, Error: 0.5833\n",
      "  Iteration: 54/57, Loss: 12.6642, Error: 0.4583\n",
      "  Iteration: 55/57, Loss: 12.952, Error: 0.4688\n",
      "  Iteration: 56/57, Loss: 11.2251, Error: 0.4062\n",
      "  Iteration: 57/57, Loss: 13.5731, Error: 0.4912\n",
      "Average Error for this Epoch: 0.4998\n",
      "Epoch: 10/10\n",
      "  Iteration: 1/57, Loss: 12.952, Error: 0.4688\n",
      "  Iteration: 2/57, Loss: 14.9668, Error: 0.5417\n",
      "  Iteration: 3/57, Loss: 13.5277, Error: 0.4896\n",
      "  Iteration: 4/57, Loss: 13.5277, Error: 0.4896\n",
      "  Iteration: 5/57, Loss: 12.6642, Error: 0.4583\n",
      "  Iteration: 6/57, Loss: 14.679, Error: 0.5312\n",
      "  Iteration: 7/57, Loss: 14.1033, Error: 0.5104\n",
      "  Iteration: 8/57, Loss: 15.2546, Error: 0.5521\n",
      "  Iteration: 9/57, Loss: 12.3764, Error: 0.4479\n",
      "  Iteration: 10/57, Loss: 12.3764, Error: 0.4479\n",
      "  Iteration: 11/57, Loss: 16.1181, Error: 0.5833\n",
      "  Iteration: 12/57, Loss: 13.8155, Error: 0.5000\n",
      "  Iteration: 13/57, Loss: 11.5129, Error: 0.4167\n",
      "  Iteration: 14/57, Loss: 13.5277, Error: 0.4896\n",
      "  Iteration: 15/57, Loss: 14.679, Error: 0.5312\n",
      "  Iteration: 16/57, Loss: 12.0886, Error: 0.4375\n",
      "  Iteration: 17/57, Loss: 11.5129, Error: 0.4167\n",
      "  Iteration: 18/57, Loss: 12.6642, Error: 0.4583\n",
      "  Iteration: 19/57, Loss: 14.679, Error: 0.5312\n",
      "  Iteration: 20/57, Loss: 14.9668, Error: 0.5417\n",
      "  Iteration: 21/57, Loss: 14.1033, Error: 0.5104\n",
      "  Iteration: 22/57, Loss: 11.8007, Error: 0.4271\n",
      "  Iteration: 23/57, Loss: 16.1181, Error: 0.5833\n",
      "  Iteration: 24/57, Loss: 12.952, Error: 0.4688\n",
      "  Iteration: 25/57, Loss: 14.9668, Error: 0.5417\n",
      "  Iteration: 26/57, Loss: 14.3912, Error: 0.5208\n",
      "  Iteration: 27/57, Loss: 13.5277, Error: 0.4896\n",
      "  Iteration: 28/57, Loss: 15.2546, Error: 0.5521\n",
      "  Iteration: 29/57, Loss: 14.3912, Error: 0.5208\n",
      "  Iteration: 30/57, Loss: 14.9668, Error: 0.5417\n",
      "  Iteration: 31/57, Loss: 12.6642, Error: 0.4583\n",
      "  Iteration: 32/57, Loss: 11.8007, Error: 0.4271\n",
      "  Iteration: 33/57, Loss: 13.8155, Error: 0.5000\n",
      "  Iteration: 34/57, Loss: 14.679, Error: 0.5312\n",
      "  Iteration: 35/57, Loss: 16.6937, Error: 0.6042\n",
      "  Iteration: 36/57, Loss: 12.0886, Error: 0.4375\n",
      "  Iteration: 37/57, Loss: 13.8155, Error: 0.5000\n",
      "  Iteration: 38/57, Loss: 12.952, Error: 0.4688\n",
      "  Iteration: 39/57, Loss: 11.8007, Error: 0.4271\n",
      "  Iteration: 40/57, Loss: 10.0738, Error: 0.3646\n",
      "  Iteration: 41/57, Loss: 15.5424, Error: 0.5625\n",
      "  Iteration: 42/57, Loss: 14.3912, Error: 0.5208\n",
      "  Iteration: 43/57, Loss: 15.5424, Error: 0.5625\n",
      "  Iteration: 44/57, Loss: 16.4059, Error: 0.5938\n",
      "  Iteration: 45/57, Loss: 13.8155, Error: 0.5000\n",
      "  Iteration: 46/57, Loss: 10.9373, Error: 0.3958\n",
      "  Iteration: 47/57, Loss: 14.9668, Error: 0.5417\n",
      "  Iteration: 48/57, Loss: 13.5277, Error: 0.4896\n",
      "  Iteration: 49/57, Loss: 13.2399, Error: 0.4792\n",
      "  Iteration: 50/57, Loss: 14.1033, Error: 0.5104\n",
      "  Iteration: 51/57, Loss: 14.9668, Error: 0.5417\n",
      "  Iteration: 52/57, Loss: 14.9668, Error: 0.5417\n",
      "  Iteration: 53/57, Loss: 14.1033, Error: 0.5104\n",
      "  Iteration: 54/57, Loss: 14.679, Error: 0.5312\n",
      "  Iteration: 55/57, Loss: 14.9668, Error: 0.5417\n",
      "  Iteration: 56/57, Loss: 13.2399, Error: 0.4792\n",
      "  Iteration: 57/57, Loss: 12.6036, Error: 0.4561\n",
      "Average Error for this Epoch: 0.4996\n"
     ]
    }
   ],
   "source": [
    "model.train()     # training mode\n",
    "training_loss = []\n",
    "avg_error = 0\n",
    "avg_error_vec = []\n",
    "best_avg_error = 1\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    print(\"Epoch: %d/%d\" % (epoch+1, num_epochs))\n",
    "\n",
    "    for i, (samples, labels) in enumerate(train_loader):\n",
    "        samples = Variable(samples)\n",
    "        labels = Variable(labels)\n",
    "        output = model(samples)                # forward pass\n",
    "        output = torch.flatten(output)         # resize predicted labels\n",
    "        labels = labels.type(torch.DoubleTensor)\n",
    "        \n",
    "        loss = criterion(output, labels)  # calculate loss\n",
    "        optimizer.zero_grad()     # clear gradient\n",
    "        loss.backward()           # calculate gradients\n",
    "        optimizer.step()          # update weights\n",
    "        \n",
    "        # calculate and print error\n",
    "        out = output\n",
    "\n",
    "        for j in range(0, out.size()[0]):\n",
    "            if out[j] < 0.5:\n",
    "                out[j] = 0\n",
    "            else:\n",
    "                out[j] = 1\n",
    "        error = 1 - torch.sum(output == labels).item() / labels.size()[0]\n",
    "        avg_error += error\n",
    "        training_loss.append(loss.data.numpy())\n",
    "        print(\"  Iteration: %d/%d, Loss: %g, Error: %0.4f\" % \n",
    "              (i+1, np.ceil(X_train.size()[0] / batch_size).astype(int), loss.item(), error))\n",
    "    \n",
    "    avg_error = avg_error / np.ceil(X_train.size()[0] / batch_size)\n",
    "    avg_error_vec.append(avg_error)\n",
    "    print(\"Average Error for this Epoch: %0.4f\" % avg_error)\n",
    "\n",
    "    if avg_error < best_avg_error:\n",
    "        print(\"found a better model!\")\n",
    "        best_avg_error = avg_error\n",
    "        best_model = copy.deepcopy(model)\n",
    "    \n",
    "    avg_error = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure()\n",
    "plt.plot(np.linspace(start=1, stop=num_epochs, num=num_epochs), avg_error_vec)\n",
    "plt.xlabel(\"Number of Epochs\")\n",
    "plt.ylabel(\"Average Training Error\")\n",
    "plt.xlim([1, num_epochs])\n",
    "plt.title(\"Average Training Error for Epochs=1:100\")\n",
    "plt.grid(True)\n",
    "# plt.savefig(\"Neural Net Training Error.jpg\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate model on testing set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model.eval()\n",
    "\n",
    "for i, (samples, labels) in enumerate(test_loader):\n",
    "    samples = Variable(samples)\n",
    "    labels = Variable(labels)\n",
    "    predictions = best_model(samples)\n",
    "    predictions = torch.flatten(predictions)\n",
    "    labels = labels.type(torch.DoubleTensor)\n",
    "\n",
    "    for j in range(0, predictions.size()[0]):\n",
    "        if predictions[j] < 0.5:\n",
    "            predictions[j] = 0\n",
    "        else:\n",
    "            predictions[j] = 1\n",
    "    \n",
    "    error = 1 - torch.sum(predictions == labels).item() / labels.size()[0]\n",
    "    \n",
    "    print(\"Testing set Error: %0.4f\" % error)\n",
    "\n",
    "model_path = dest_path + \"torch_model_3_24_19_lr=\" + str(learning_rate) + \"all_data_one_hot_dict.pt\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load and Evaluate previous models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = CLANet(input_size, hidden_size, output_size)\n",
    "model.load_state_dict(torch.load(dest_path + \"torch_model_3_23_19_lr=0.01_one_hot_dict.pt\"))\n",
    "model.double()     # cast model parameters to double\n",
    "model.eval()\n",
    "\n",
    "for i, (samples, labels) in enumerate(test_loader):\n",
    "    samples = Variable(samples)\n",
    "    labels = Variable(labels)\n",
    "    conf = model(samples)\n",
    "    conf = torch.flatten(conf)\n",
    "    labels = labels.type(torch.DoubleTensor)\n",
    "    \n",
    "    for j in range(conf.size()[0]):\n",
    "        if conf[j] < 0.5:\n",
    "            conf[j] = 0\n",
    "        else:\n",
    "            conf[j] = 1\n",
    "                \n",
    "    error = 1 - torch.sum(conf == labels).item() / labels.size()[0]\n",
    "    \n",
    "    print(\"Testing set Error: %0.4f\" % error)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Best Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = CLANet(input_size, hidden_size, output_size)\n",
    "model.load_state_dict(torch.load(dest_path + \"torch_model_3_23_19_lr=0.01_one_hot_dict.pt\"))\n",
    "model.double()     # cast model parameters to double\n",
    "model.eval()\n",
    "\n",
    "for i, (samples, labels) in enumerate(test_loader):\n",
    "    samples = Variable(samples)\n",
    "    labels = Variable(labels)\n",
    "    conf = model(samples)    # confidence that a certain instance is predicted correctly\n",
    "    conf = torch.flatten(conf)\n",
    "    labels = labels.type(torch.DoubleTensor)\n",
    "    \n",
    "# convert to numpy arrays\n",
    "conf = conf.detach().numpy()\n",
    "labels = labels.numpy()\n",
    "\n",
    "# sort arrays according to the predicted confidence (high confidence to low confidence)\n",
    "sort_idx = np.argsort(-conf, kind='mergesort')\n",
    "conf = conf[sort_idx]\n",
    "labels = labels[sort_idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ROC Curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_pos = 0\n",
    "num_neg = 0\n",
    "\n",
    "for label in labels:\n",
    "    if label < 0.5:\n",
    "        num_neg += 1\n",
    "    else:\n",
    "        num_pos += 1\n",
    "        \n",
    "tp = 0\n",
    "fp = 0\n",
    "last_tp = 0\n",
    "fpr = []\n",
    "tpr = []\n",
    "\n",
    "for i in range(len(labels)):\n",
    "    if (i > 1) and (conf[i] != conf[i-1]) and (labels[i] == 0) and (tp > last_tp):\n",
    "        fpr.append(fp / num_neg)\n",
    "        tpr.append(tp / num_pos)\n",
    "        last_tp = tp\n",
    "    if labels[i] == 1:\n",
    "        tp += 1\n",
    "    else:\n",
    "        fp += 1\n",
    "        \n",
    "fpr.append(fp / num_neg)\n",
    "tpr.append(tp / num_pos)\n",
    "\n",
    "plt.figure(figsize=(12, 8))\n",
    "plt.plot(fpr, tpr)\n",
    "plt.xlabel('False Postive Rate (FPR)', fontsize=20)\n",
    "plt.ylabel('True Positive Rate (TPR)', fontsize=20)\n",
    "plt.axis([0, 1, 0, 1.001])\n",
    "plt.title('ROC Curve', fontsize=20)\n",
    "plt.tight_layout()\n",
    "plt.grid(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PR Curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, (samples, labels) in enumerate(test_loader):\n",
    "    samples = Variable(samples)\n",
    "    labels = Variable(labels)\n",
    "    pred = model(samples)\n",
    "    pred = torch.flatten(pred)\n",
    "    labels = labels.type(torch.DoubleTensor)\n",
    "\n",
    "# convert to numpy arrays\n",
    "pred = pred.detach().numpy()\n",
    "labels = labels.numpy()\n",
    "\n",
    "# sort arrays according to the predicted confidence (high confidence to low confidence)\n",
    "sort_idx = np.argsort(-pred, kind='mergesort')\n",
    "pred = pred[sort_idx]\n",
    "labels = labels[sort_idx]\n",
    "\n",
    "num_pred_pos = 0\n",
    "num_actual_pos = 0\n",
    "num_tp = 0\n",
    "precision = []\n",
    "recall = []\n",
    "\n",
    "for confidence in conf:\n",
    "    for i in range(len(pred)):\n",
    "        if pred[i] >= confidence:\n",
    "            num_pred_pos += 1\n",
    "            \n",
    "            if labels[i] == 1:\n",
    "                num_tp += 1\n",
    "        \n",
    "        if labels[i] == 1:\n",
    "            num_actual_pos += 1\n",
    "\n",
    "    precision.append(num_tp / num_pred_pos)\n",
    "    recall.append(num_tp / num_actual_pos)\n",
    "    \n",
    "    num_pred_pos = 0\n",
    "    num_actual_pos = 0\n",
    "    num_tp = 0\n",
    "\n",
    "plt.figure(figsize=(12, 8))\n",
    "plt.plot(recall, precision)\n",
    "plt.xlabel('Recall', fontsize=20)\n",
    "plt.ylabel('Precision', fontsize=20)\n",
    "plt.axis([0, 1.001, 0.4, 1])\n",
    "plt.title('PR Curve', fontsize=20)\n",
    "plt.tight_layout()\n",
    "plt.grid(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test best model on other data sets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read in data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define data path\n",
    "data_path = \"/Users/Alliot/Documents/CLA-Project/Data/data-sets/\"\n",
    "data_set = \"data_2016_summer\"\n",
    "\n",
    "# load data sets\n",
    "X = np.load(data_path + data_set + \"_one_hot.npy\")\n",
    "y = np.load(data_path + data_set + \"_labels.npy\")\n",
    "\n",
    "# manipulate data set. labels are converted to 0, +1 for binary classification; samples are removed uniformly \n",
    "# from the data set so that the disproportionately large number of negative samples (no algae) does \n",
    "# not bias the model.\n",
    "\n",
    "num_alg = 0  # count the number of algae instances\n",
    "num_no_alg = 0  # count the number of no algae instances\n",
    "\n",
    "# Convert labels to binary: -1 for no algae and 1 for algae\n",
    "for i in range(0, len(y)):\n",
    "    if y[i] == 0:\n",
    "        num_no_alg += 1\n",
    "    if y[i] == 1 or y[i] == 2:\n",
    "        y[i] = 1\n",
    "        num_alg += 1\n",
    "\n",
    "# oversample the data set by randomly adding occurences of algae until the difference between the number of algae\n",
    "# samples and no algae samples equals sample_bias (defined below)\n",
    "idx = 0\n",
    "sample_bias = 0\n",
    "length_y = len(y)\n",
    "while num_alg != (num_no_alg + sample_bias):\n",
    "    # circle through the data sets until the difference of num_no_alg and num_alg equals\n",
    "    # the value specified by sample_bias\n",
    "    if idx == (length_y - 1):\n",
    "        idx = 0\n",
    "\n",
    "    if y[idx] == 1:\n",
    "        if np.random.rand() >= 0.5:  # add this sample with some probability\n",
    "            y = np.append(y, y[idx])\n",
    "            X = np.append(X, np.reshape(X[idx, :], newshape=(1, num_features)), axis=0)\n",
    "            num_alg += 1\n",
    "        else:\n",
    "            idx += 1\n",
    "    else:\n",
    "        idx += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Process and split data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# standardize data: remove the mean and variance in each sample\n",
    "num_splits = 2   # do not change\n",
    "sss = model_selection.StratifiedShuffleSplit(n_splits=num_splits, test_size=test_size)\n",
    "\n",
    "idx, _ = sss.split(X, y);\n",
    "train_idx = idx[0]\n",
    "test_idx = idx[1]\n",
    "X_train, X_test = X[train_idx], X[test_idx]\n",
    "y_train, y_test = y[train_idx], y[test_idx]\n",
    "\n",
    "X_train = preprocessing.scale(X_train, axis=1, with_mean=True, with_std=True)\n",
    "X_test = preprocessing.scale(X_test, axis=1, with_mean=True, with_std=True)\n",
    "\n",
    "# convert numpy arrays to pytorch tensors\n",
    "train_set_size = X_train.shape\n",
    "test_set_size = X_test.shape\n",
    "X_train, X_test = torch.from_numpy(X_train), torch.from_numpy(X_test)\n",
    "y_train, y_test = torch.from_numpy(y_train), torch.from_numpy(y_test)\n",
    "\n",
    "# convert pytorch tensors to pytorch TensorDataset\n",
    "train_set = utils.TensorDataset(X_train, y_train)\n",
    "test_set = utils.TensorDataset(X_test, y_test)\n",
    "\n",
    "# create DataLoaders\n",
    "train_loader = utils.DataLoader(train_set, batch_size=batch_size, shuffle=True)\n",
    "test_loader = utils.DataLoader(test_set, batch_size=test_set_size[0], shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate best model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, (samples, labels) in enumerate(test_loader):\n",
    "    samples = Variable(samples)\n",
    "    labels = Variable(labels)\n",
    "    conf = model(samples)\n",
    "    conf = torch.flatten(conf)\n",
    "    labels = labels.type(torch.DoubleTensor)\n",
    "    \n",
    "    for j in range(conf.size()[0]):\n",
    "        if conf[j] < 0.5:\n",
    "            conf[j] = 0\n",
    "        else:\n",
    "            conf[j] = 1\n",
    "                \n",
    "    error = 1 - torch.sum(conf == labels).item() / labels.size()[0]\n",
    "    \n",
    "    print(\"Testing set Error: %0.4f\" % error)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimizer parameter adjust"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "for p in optimizer.param_groups:\n",
    "    p[\"lr\"] = 0.01\n",
    "    p[\"momentum\"] = 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for p in optimizer.param_groups:\n",
    "    p[\"lr\"] = 0.003\n",
    "    p[\"momentum\"] = 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for p in optimizer.param_groups:\n",
    "    p[\"lr\"] = 0.001\n",
    "    p[\"momentum\"] = 0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for p in optimizer.param_groups:\n",
    "    p[\"lr\"] = 0.00001\n",
    "    p[\"momentum\"] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for p in optimizer.param_groups:\n",
    "    p[\"lr\"] = 0.0000005"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
