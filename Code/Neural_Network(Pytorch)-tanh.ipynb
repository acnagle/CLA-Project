{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Network for CLA Project"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import statements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "from sklearn import model_selection\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.utils.data as utils\n",
    "from torch.autograd import Variable\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import errno\n",
    "import os\n",
    "import sys\n",
    "import Constants\n",
    "import copy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data processing\n",
    "sample_bias = 0     # adjust the difference in the number of the two types of samples (no algae vs algae)\n",
    "test_size = 0.2\n",
    "batch_size = 32    # batch size for the DataLoaders. previously was 100\n",
    "\n",
    "# NN model\n",
    "num_features = 17\n",
    "input_size = num_features     # size of input layer\n",
    "multiplier = 100         # multiplied by num_features to determine the size of each hidden layer. previously was 100\n",
    "hidden_size = multiplier * input_size\n",
    "output_size = 1\n",
    "learning_rate = 0.01         # learning rate of optimizer. previously was 0.01\n",
    "num_epochs = 100                # number of epochs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read in data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.set_printoptions(threshold=np.inf)  # prints a full matrix rather than an abbreviated matrix\n",
    "\n",
    "# define data and destination paths\n",
    "dest_path = \"/Users/Alliot/Documents/CLA-Project/Data/all-data-no-na/neural-network/\"\n",
    "data_path = \"/Users/Alliot/Documents/CLA-Project/Data/data-sets/\"\n",
    "data_set = \"data_2017_summer\"\n",
    "\n",
    "# if dest_path does not exist, create it\n",
    "if not os.path.exists(dest_path):\n",
    "    try:\n",
    "        os.makedirs(dest_path)\n",
    "    except OSError as e:\n",
    "        if e.errno != errno.EEXIST:\n",
    "            raise\n",
    "\n",
    "# load data sets\n",
    "X = np.load(data_path + data_set + \".npy\")\n",
    "y = np.load(data_path + data_set + \"_labels.npy\")\n",
    "\n",
    "# manipulate data set. labels are converted to -1, +1 for binary classification; samples are removed uniformly \n",
    "# from the data set so that the disproportionately large number of negative samples (no algae) does \n",
    "# not bias the model.\n",
    "\n",
    "num_alg = 0  # count the number of algae instances\n",
    "num_no_alg = 0  # count the number of no algae instances\n",
    "\n",
    "# Convert labels to binary: -1 for no algae and 1 for algae\n",
    "for i in range(0, len(y)):\n",
    "    if y[i] == 0:\n",
    "        num_no_alg += 1\n",
    "    if y[i] == 1 or y[i] == 2:\n",
    "        y[i] = 1\n",
    "        num_alg += 1\n",
    "\n",
    "# oversample the data set by randomly adding occurences of algae until the difference between the number of algae\n",
    "# samples and no algae samples equals sample_bias (defined below)\n",
    "idx = 0\n",
    "sample_bias = 0\n",
    "length_y = len(y)\n",
    "while num_alg != (num_no_alg + sample_bias):\n",
    "    # circle through the data sets until the difference of num_no_alg and num_alg equals\n",
    "    # the value specified by sample_bias\n",
    "    if idx == (length_y - 1):\n",
    "        idx = 0\n",
    "\n",
    "    if y[idx] == 1:\n",
    "        if np.random.rand() >= 0.5:  # add this sample with some probability\n",
    "            y = np.append(y, y[idx])\n",
    "            X = np.append(X, np.reshape(X[idx, :], newshape=(1, num_features)), axis=0)\n",
    "            num_alg += 1\n",
    "        else:\n",
    "            idx += 1\n",
    "    else:\n",
    "        idx += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Process and split data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# standardize data: remove the mean and variance in each sample\n",
    "num_splits = 2   # do not change\n",
    "sss = model_selection.StratifiedShuffleSplit(n_splits=num_splits, test_size=test_size)\n",
    "\n",
    "idx, _ = sss.split(X, y);\n",
    "train_idx = idx[0]\n",
    "test_idx = idx[1]\n",
    "X_train, X_test = X[train_idx], X[test_idx]\n",
    "y_train, y_test = y[train_idx], y[test_idx]\n",
    "\n",
    "X_train = preprocessing.scale(X_train, axis=1, with_mean=True, with_std=True)\n",
    "X_test = preprocessing.scale(X_test, axis=1, with_mean=True, with_std=True)\n",
    "\n",
    "# convert numpy arrays to pytorch tensors\n",
    "train_set_size = X_train.shape\n",
    "test_set_size = X_test.shape\n",
    "X_train, X_test = torch.from_numpy(X_train), torch.from_numpy(X_test)\n",
    "y_train, y_test = torch.from_numpy(y_train), torch.from_numpy(y_test)\n",
    "\n",
    "# convert pytorch tensors to pytorch TensorDataset\n",
    "train_set = utils.TensorDataset(X_train, y_train)\n",
    "test_set = utils.TensorDataset(X_test, y_test)\n",
    "\n",
    "# create DataLoaders\n",
    "train_loader = utils.DataLoader(train_set, batch_size=batch_size, shuffle=True)\n",
    "test_loader = utils.DataLoader(test_set, batch_size=test_set_size[0], shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define neural network model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CLANet(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(CLANet, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size)\n",
    "        self.tanh1 = nn.Tanh()\n",
    "        self.fc2 = nn.Linear(hidden_size, hidden_size)\n",
    "        self.tanh2 = nn.Tanh()\n",
    "        self.fc3 = nn.Linear(hidden_size, hidden_size)\n",
    "        self.tanh3 = nn.Tanh()\n",
    "        self.fc4 = nn.Linear(hidden_size, hidden_size)\n",
    "        self.tanh4 = nn.Tanh()\n",
    "        self.fc5 = nn.Linear(hidden_size, hidden_size)\n",
    "        self.tanh5 = nn.Tanh()\n",
    "        self.fc6 = nn.Linear(hidden_size, output_size)     # previously, this was output_size\n",
    "#         self.relu6 = nn.ReLU()                             # previously, this was the line which was commented out\n",
    "#         self.fc7 = nn.Linear(hidden_size, hidden_size)\n",
    "#         self.relu7 = nn.ReLU()\n",
    "#         self.fc8 = nn.Linear(hidden_size, hidden_size)\n",
    "#         self.relu8 = nn.ReLU()\n",
    "#         self.fc9 = nn.Linear(hidden_size, output_size)\n",
    "#         self.relu9 = nn.ReLU()\n",
    "#         self.fc10 = nn.Linear(hidden_size, hidden_size)\n",
    "#         self.relu10 = nn.ReLU()\n",
    "#         self.fc11 = nn.Linear(hidden_size, hidden_size)\n",
    "#         self.relu11 = nn.ReLU()\n",
    "#         self.fc12 = nn.Linear(hidden_size, output_size)\n",
    "        self.sig1 = nn.Sigmoid()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        out = self.fc1(x)\n",
    "        out = self.tanh1(out)\n",
    "        out = self.fc2(out)\n",
    "        out = self.tanh2(out)\n",
    "        out = self.fc3(out)\n",
    "        out = self.tanh3(out)\n",
    "        out = self.fc4(out)\n",
    "        out = self.tanh4(out)\n",
    "        out = self.fc5(out)\n",
    "        out = self.tanh5(out)\n",
    "        out = self.fc6(out)\n",
    "#         out = self.relu6(out)\n",
    "#         out = self.fc7(out)\n",
    "#         out = self.relu7(out)\n",
    "#         out = self.fc8(out)\n",
    "#         out = self.relu8(out)\n",
    "#         out = self.fc9(out)\n",
    "#         out = self.relu9(out)\n",
    "#         out = self.fc10(out)\n",
    "#         out = self.relu10(out)\n",
    "#         out = self.fc11(out)\n",
    "#         out = self.relu11(out)\n",
    "#         out = self.fc12(out)\n",
    "        out = self.sig1(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Instantiate the neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = CLANet(input_size, hidden_size, output_size)\n",
    "criterion = nn.BCELoss()\n",
    "\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate, nesterov=True, momentum=1, dampening=0)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=num_epochs/10, gamma=0.0005)\n",
    "model.double();     # cast model parameters to double"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/100\n",
      "  Iteration: 1/52, Loss: 0.696155, Error: 0.5938\n",
      "  Iteration: 2/52, Loss: 0.685018, Error: 0.2500\n",
      "  Iteration: 3/52, Loss: 0.723624, Error: 0.6875\n",
      "  Iteration: 4/52, Loss: 0.696233, Error: 0.5000\n",
      "  Iteration: 5/52, Loss: 0.667855, Error: 0.3125\n",
      "  Iteration: 6/52, Loss: 0.649394, Error: 0.2812\n",
      "  Iteration: 7/52, Loss: 0.717358, Error: 0.5312\n",
      "  Iteration: 8/52, Loss: 0.696268, Error: 0.4688\n",
      "  Iteration: 9/52, Loss: 0.704555, Error: 0.4688\n",
      "  Iteration: 10/52, Loss: 0.690682, Error: 0.4375\n",
      "  Iteration: 11/52, Loss: 0.71962, Error: 0.5000\n",
      "  Iteration: 12/52, Loss: 0.736522, Error: 0.5625\n",
      "  Iteration: 13/52, Loss: 0.707297, Error: 0.5000\n",
      "  Iteration: 14/52, Loss: 0.701729, Error: 0.5000\n",
      "  Iteration: 15/52, Loss: 0.699538, Error: 0.5000\n",
      "  Iteration: 16/52, Loss: 0.692015, Error: 0.4688\n",
      "  Iteration: 17/52, Loss: 0.715436, Error: 0.7188\n",
      "  Iteration: 18/52, Loss: 0.695615, Error: 0.4688\n",
      "  Iteration: 19/52, Loss: 0.690088, Error: 0.5000\n",
      "  Iteration: 20/52, Loss: 0.686317, Error: 0.4375\n",
      "  Iteration: 21/52, Loss: 0.712471, Error: 0.5312\n",
      "  Iteration: 22/52, Loss: 0.70094, Error: 0.4688\n",
      "  Iteration: 23/52, Loss: 0.718244, Error: 0.5312\n",
      "  Iteration: 24/52, Loss: 0.725074, Error: 0.5000\n",
      "  Iteration: 25/52, Loss: 0.70367, Error: 0.4688\n",
      "  Iteration: 26/52, Loss: 0.699201, Error: 0.4688\n",
      "  Iteration: 27/52, Loss: 0.736733, Error: 0.5625\n",
      "  Iteration: 28/52, Loss: 0.726064, Error: 0.5625\n",
      "  Iteration: 29/52, Loss: 0.684754, Error: 0.4375\n",
      "  Iteration: 30/52, Loss: 0.74335, Error: 0.6562\n",
      "  Iteration: 31/52, Loss: 0.664805, Error: 0.3125\n",
      "  Iteration: 32/52, Loss: 0.703978, Error: 0.5000\n",
      "  Iteration: 33/52, Loss: 0.690507, Error: 0.5625\n",
      "  Iteration: 34/52, Loss: 0.705822, Error: 0.6250\n",
      "  Iteration: 35/52, Loss: 0.691307, Error: 0.5000\n",
      "  Iteration: 36/52, Loss: 0.705871, Error: 0.5938\n",
      "  Iteration: 37/52, Loss: 0.708124, Error: 0.5312\n",
      "  Iteration: 38/52, Loss: 0.676346, Error: 0.4688\n",
      "  Iteration: 39/52, Loss: 0.660354, Error: 0.3750\n",
      "  Iteration: 40/52, Loss: 0.647943, Error: 0.3438\n",
      "  Iteration: 41/52, Loss: 0.74618, Error: 0.6250\n",
      "  Iteration: 42/52, Loss: 0.727355, Error: 0.5625\n",
      "  Iteration: 43/52, Loss: 0.710406, Error: 0.5000\n",
      "  Iteration: 44/52, Loss: 0.735636, Error: 0.5625\n",
      "  Iteration: 45/52, Loss: 0.724001, Error: 0.5312\n",
      "  Iteration: 46/52, Loss: 0.673398, Error: 0.3438\n",
      "  Iteration: 47/52, Loss: 0.747896, Error: 0.6562\n",
      "  Iteration: 48/52, Loss: 0.732219, Error: 0.6250\n",
      "  Iteration: 49/52, Loss: 0.726089, Error: 0.7188\n",
      "  Iteration: 50/52, Loss: 0.691196, Error: 0.4375\n",
      "  Iteration: 51/52, Loss: 0.701686, Error: 0.5312\n",
      "  Iteration: 52/52, Loss: 0.696587, Error: 0.5333\n",
      "Average Error for this Epoch: 0.5060\n",
      "found a better model!\n",
      "Epoch: 2/100\n",
      "  Iteration: 1/52, Loss: 0.692766, Error: 0.4688\n",
      "  Iteration: 2/52, Loss: 0.682189, Error: 0.4375\n",
      "  Iteration: 3/52, Loss: 0.639556, Error: 0.3125\n",
      "  Iteration: 4/52, Loss: 0.715062, Error: 0.5000\n",
      "  Iteration: 5/52, Loss: 0.710661, Error: 0.4688\n",
      "  Iteration: 6/52, Loss: 0.69505, Error: 0.4375\n",
      "  Iteration: 7/52, Loss: 0.832099, Error: 0.6875\n",
      "  Iteration: 8/52, Loss: 0.6793, Error: 0.4062\n",
      "  Iteration: 9/52, Loss: 0.738715, Error: 0.5312\n",
      "  Iteration: 10/52, Loss: 0.717002, Error: 0.5000\n",
      "  Iteration: 11/52, Loss: 0.720555, Error: 0.5312\n",
      "  Iteration: 12/52, Loss: 0.722044, Error: 0.5625\n",
      "  Iteration: 13/52, Loss: 0.706395, Error: 0.5312\n",
      "  Iteration: 14/52, Loss: 0.701257, Error: 0.5625\n",
      "  Iteration: 15/52, Loss: 0.697358, Error: 0.6250\n",
      "  Iteration: 16/52, Loss: 0.678298, Error: 0.3750\n",
      "  Iteration: 17/52, Loss: 0.737003, Error: 0.6250\n",
      "  Iteration: 18/52, Loss: 0.720076, Error: 0.5312\n",
      "  Iteration: 19/52, Loss: 0.659103, Error: 0.3750\n",
      "  Iteration: 20/52, Loss: 0.665009, Error: 0.3750\n",
      "  Iteration: 21/52, Loss: 0.763793, Error: 0.5312\n",
      "  Iteration: 22/52, Loss: 0.670952, Error: 0.4062\n",
      "  Iteration: 23/52, Loss: 0.75217, Error: 0.5312\n",
      "  Iteration: 24/52, Loss: 0.66225, Error: 0.3750\n",
      "  Iteration: 25/52, Loss: 0.770414, Error: 0.5625\n",
      "  Iteration: 26/52, Loss: 0.725671, Error: 0.5000\n",
      "  Iteration: 27/52, Loss: 0.71614, Error: 0.5312\n",
      "  Iteration: 28/52, Loss: 0.703582, Error: 0.5000\n",
      "  Iteration: 29/52, Loss: 0.687346, Error: 0.4375\n",
      "  Iteration: 30/52, Loss: 0.692835, Error: 0.5000\n",
      "  Iteration: 31/52, Loss: 0.692246, Error: 0.5312\n",
      "  Iteration: 32/52, Loss: 0.702859, Error: 0.5000\n",
      "  Iteration: 33/52, Loss: 0.737695, Error: 0.6250\n",
      "  Iteration: 34/52, Loss: 0.677637, Error: 0.4062\n",
      "  Iteration: 35/52, Loss: 0.677105, Error: 0.4062\n",
      "  Iteration: 36/52, Loss: 0.663342, Error: 0.3750\n",
      "  Iteration: 37/52, Loss: 0.769553, Error: 0.5625\n",
      "  Iteration: 38/52, Loss: 0.626375, Error: 0.3125\n",
      "  Iteration: 39/52, Loss: 0.75222, Error: 0.5312\n",
      "  Iteration: 40/52, Loss: 0.797866, Error: 0.6250\n",
      "  Iteration: 41/52, Loss: 0.718923, Error: 0.5000\n",
      "  Iteration: 42/52, Loss: 0.657621, Error: 0.3750\n",
      "  Iteration: 43/52, Loss: 0.67937, Error: 0.4375\n",
      "  Iteration: 44/52, Loss: 0.677187, Error: 0.4062\n",
      "  Iteration: 45/52, Loss: 0.690581, Error: 0.4375\n",
      "  Iteration: 46/52, Loss: 0.67034, Error: 0.3750\n",
      "  Iteration: 47/52, Loss: 0.697571, Error: 0.4688\n",
      "  Iteration: 48/52, Loss: 0.695942, Error: 0.5000\n",
      "  Iteration: 49/52, Loss: 0.693245, Error: 0.5312\n",
      "  Iteration: 50/52, Loss: 0.699827, Error: 0.6250\n",
      "  Iteration: 51/52, Loss: 0.681678, Error: 0.4062\n",
      "  Iteration: 52/52, Loss: 0.695721, Error: 0.5000\n",
      "Average Error for this Epoch: 0.4838\n",
      "found a better model!\n",
      "Epoch: 3/100\n",
      "  Iteration: 1/52, Loss: 0.667144, Error: 0.3750\n",
      "  Iteration: 2/52, Loss: 0.706887, Error: 0.5312\n",
      "  Iteration: 3/52, Loss: 0.698588, Error: 0.4688\n",
      "  Iteration: 4/52, Loss: 0.741624, Error: 0.5625\n",
      "  Iteration: 5/52, Loss: 0.673335, Error: 0.4062\n",
      "  Iteration: 6/52, Loss: 0.716574, Error: 0.5625\n",
      "  Iteration: 7/52, Loss: 0.672366, Error: 0.4062\n",
      "  Iteration: 8/52, Loss: 0.708478, Error: 0.5000\n",
      "  Iteration: 9/52, Loss: 0.759657, Error: 0.6875\n",
      "  Iteration: 10/52, Loss: 0.682376, Error: 0.4375\n",
      "  Iteration: 11/52, Loss: 0.677742, Error: 0.5000\n",
      "  Iteration: 12/52, Loss: 0.686526, Error: 0.4062\n",
      "  Iteration: 13/52, Loss: 0.692535, Error: 0.4062\n",
      "  Iteration: 14/52, Loss: 0.707801, Error: 0.5625\n",
      "  Iteration: 15/52, Loss: 0.720504, Error: 0.6250\n",
      "  Iteration: 16/52, Loss: 0.667279, Error: 0.4062\n",
      "  Iteration: 17/52, Loss: 0.691785, Error: 0.4688\n",
      "  Iteration: 18/52, Loss: 0.695021, Error: 0.4688\n",
      "  Iteration: 19/52, Loss: 0.78141, Error: 0.6875\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-15-e333899a6c5c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m     \u001b[0;31m# clear gradient\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m           \u001b[0;31m# calculate gradients\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m          \u001b[0;31m# update weights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m         \u001b[0;31m# calculate and print error\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/torch/optim/sgd.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    105\u001b[0m                         \u001b[0md_p\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbuf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    106\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 107\u001b[0;31m                 \u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mgroup\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'lr'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0md_p\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    108\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    109\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model.train()     # training mode\n",
    "training_loss = []\n",
    "avg_error = 0\n",
    "avg_error_vec = []\n",
    "best_avg_error = 1\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    print(\"Epoch: %d/%d\" % (epoch+1, num_epochs))\n",
    "\n",
    "    for i, (samples, labels) in enumerate(train_loader):\n",
    "        samples = Variable(samples)\n",
    "        labels = Variable(labels)\n",
    "        output = model(samples)                # forward pass\n",
    "        output = torch.flatten(output)         # resize predicted labels\n",
    "        labels = labels.type(torch.DoubleTensor)\n",
    "        \n",
    "        loss = criterion(output, labels)  # calculate loss\n",
    "        optimizer.zero_grad()     # clear gradient\n",
    "        loss.backward()           # calculate gradients\n",
    "        optimizer.step()          # update weights\n",
    "        \n",
    "        # calculate and print error\n",
    "        out = output\n",
    "\n",
    "        for j in range(0, out.size()[0]):\n",
    "            if out[j] < 0.5:\n",
    "                out[j] = 0\n",
    "            else:\n",
    "                out[j] = 1\n",
    "        error = 1 - torch.sum(output == labels).item() / labels.size()[0]\n",
    "        avg_error += error\n",
    "        training_loss.append(loss.data.numpy())\n",
    "        print(\"  Iteration: %d/%d, Loss: %g, Error: %0.4f\" % \n",
    "              (i+1, np.ceil(X_train.size()[0] / batch_size).astype(int), loss.item(), error))\n",
    "    \n",
    "    avg_error = avg_error / np.ceil(X_train.size()[0] / batch_size)\n",
    "    avg_error_vec.append(avg_error)\n",
    "    print(\"Average Error for this Epoch: %0.4f\" % avg_error)\n",
    "\n",
    "    if avg_error < best_avg_error:\n",
    "        print(\"found a better model!\")\n",
    "        best_avg_error = avg_error\n",
    "        best_model = copy.deepcopy(model)\n",
    "    \n",
    "    avg_error = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.plot(np.linspace(start=1, stop=100, num=100), avg_error_vec)\n",
    "plt.xlabel(\"Number of Epochs\")\n",
    "plt.ylabel(\"Average Training Error\")\n",
    "plt.title(\"Average Training Error for Epochs=1:100\")\n",
    "plt.grid(True)\n",
    "plt.savefig(\"Neural Net Training Error.jpg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model.train()     # training mode\n",
    "training_loss = []\n",
    "avg_error = 0\n",
    "avg_error_vec = []\n",
    "best_avg_error = 1\n",
    "\n",
    "# update learning rate\n",
    "for p in optimizer.param_groups:\n",
    "    p[\"lr\"] = 0.003\n",
    "    p[\"momentum\"] = 0.5\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    print(\"Epoch: %d/%d\" % (epoch+1, num_epochs))\n",
    "\n",
    "    for i, (samples, labels) in enumerate(train_loader):\n",
    "        samples = Variable(samples)\n",
    "        labels = Variable(labels)\n",
    "        output = best_model(samples)                # forward pass\n",
    "        output = torch.flatten(output)         # resize predicted labels\n",
    "        labels = labels.type(torch.DoubleTensor)\n",
    "        \n",
    "        loss = criterion(output, labels)  # calculate loss\n",
    "        optimizer.zero_grad()     # clear gradient\n",
    "        loss.backward()           # calculate gradients\n",
    "        optimizer.step()          # update weights\n",
    "        \n",
    "        # calculate and print error\n",
    "        out = output\n",
    "\n",
    "        for j in range(0, out.size()[0]):\n",
    "            if out[j] < 0.5:\n",
    "                out[j] = 0\n",
    "            else:\n",
    "                out[j] = 1\n",
    "        error = 1 - torch.sum(output == labels).item() / labels.size()[0]\n",
    "        avg_error += error\n",
    "        training_loss.append(loss.data.numpy())\n",
    "        print(\"  Iteration: %d/%d, Loss: %g, Error: %0.4f\" % \n",
    "              (i+1, np.ceil(X_train.size()[0] / batch_size).astype(int), loss.item(), error))\n",
    "    \n",
    "    avg_error = avg_error / np.ceil(X_train.size()[0] / batch_size)\n",
    "    avg_error_vec.append(avg_error)\n",
    "    print(\"Average Error for this Epoch: %0.4f\" % avg_error)\n",
    "\n",
    "    if avg_error < best_avg_error:\n",
    "        print(\"found a better model!\")\n",
    "        best_avg_error = avg_error\n",
    "        bester_model = copy.deepcopy(model)\n",
    "    \n",
    "    avg_error = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate Model on Testing Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model.eval()\n",
    "\n",
    "for i, (samples, labels) in enumerate(test_loader):\n",
    "    samples = Variable(samples)\n",
    "    labels = Variable(labels)\n",
    "    predictions = best_model(samples)\n",
    "    predictions = torch.flatten(predictions)\n",
    "    labels = labels.type(torch.DoubleTensor)\n",
    "\n",
    "    for j in range(0, predictions.size()[0]):\n",
    "        if predictions[j] < 0.5:\n",
    "            predictions[j] = 0\n",
    "        else:\n",
    "            predictions[j] = 1\n",
    "    \n",
    "    error = 1 - torch.sum(predictions == labels).item() / labels.size()[0]\n",
    "    \n",
    "    print(\"Testing set Error: %0.4f\" % error)\n",
    "    \n",
    "model_path = \"./torch_model_3_4_19_lr=\" + str(learning_rate) + \"_dict.pt\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load and Evaluate previous models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = CLANet(input_size, hidden_size, output_size)\n",
    "model.load_state_dict(torch.load(\"torch_model_2_18_19_lr=0.01_dict.pt\"))\n",
    "model.double()     # cast model parameters to double\n",
    "model.eval()\n",
    "\n",
    "for i, (samples, labels) in enumerate(test_loader):\n",
    "    samples = Variable(samples)\n",
    "    labels = Variable(labels)\n",
    "    predictions = model(samples)\n",
    "    predictions = torch.flatten(predictions)\n",
    "    labels = labels.type(torch.DoubleTensor)\n",
    "\n",
    "    for j in range(0, predictions.size()[0]):\n",
    "        if predictions[j] < 0.5:\n",
    "            predictions[j] = 0\n",
    "        else:\n",
    "            predictions[j] = 1\n",
    "    \n",
    "    error = 1 - torch.sum(predictions == labels).item() / labels.size()[0]\n",
    "    \n",
    "    print(\"Testing set Error: %0.4f\" % error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for p in optimizer.param_groups:\n",
    "    p[\"lr\"] = 0.00001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
