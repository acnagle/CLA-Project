{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Network for CLA Project"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import statements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "from sklearn import model_selection\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.utils.data as utils\n",
    "from torch.autograd import Variable\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import errno\n",
    "import os\n",
    "import sys\n",
    "import Constants\n",
    "import copy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# yearly summer dataset parameters\n",
    "# data processing\n",
    "# sample_bias = 0     # adjust the difference in the number of the two types of samples (no algae vs algae)\n",
    "# test_size = 0.2\n",
    "# batch_size = 96    # batch size for the DataLoaders. previously was 100\n",
    "\n",
    "# NN model\n",
    "# num_features = 21\n",
    "# input_size = num_features     # size of input layer\n",
    "# multiplier = 100         # multiplied by num_features to determine the size of each hidden layer. previously was 100\n",
    "# hidden_size = multiplier * input_size\n",
    "# output_size = 1\n",
    "# learning_rate = 0.01         # learning rate of optimizer. previously was 0.01\n",
    "# num_epochs = 100                # number of epochs\n",
    "\n",
    "# all data summer data set hyperparameters\n",
    "# data processing\n",
    "sample_bias = 0     # adjust the difference in the number of the two types of samples (no algae vs algae)\n",
    "test_size = 0.2\n",
    "batch_size = 96    # batch size for the DataLoaders. previously was 100\n",
    "\n",
    "# NN model\n",
    "num_features = 21\n",
    "input_size = num_features     # size of input layer\n",
    "multiplier = 100         # multiplied by num_features to determine the size of each hidden layer. previously was 100\n",
    "hidden_size = multiplier * input_size\n",
    "output_size = 1\n",
    "learning_rate = 0.1         # learning rate of optimizer. previously was 0.01\n",
    "num_epochs = 10                # number of epochs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read in data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.set_printoptions(threshold=np.inf)  # prints a full matrix rather than an abbreviated matrix\n",
    "\n",
    "# define data and destination paths\n",
    "dest_path = \"/Users/Alliot/Documents/CLA-Project/Data/all-data-no-na/neural-network/\"\n",
    "data_path = \"/Users/Alliot/Documents/CLA-Project/Data/data-sets/\"\n",
    "data_set = \"all_data_summer\"\n",
    "\n",
    "# load data sets\n",
    "X = np.load(data_path + data_set + \"_one_hot.npy\")\n",
    "y = np.load(data_path + data_set + \"_labels.npy\")\n",
    "\n",
    "# manipulate data set. labels are converted to 0, +1 for binary classification; samples are removed uniformly \n",
    "# from the data set so that the disproportionately large number of negative samples (no algae) does \n",
    "# not bias the model.\n",
    "\n",
    "num_alg = 0  # count the number of algae instances\n",
    "num_no_alg = 0  # count the number of no algae instances\n",
    "\n",
    "# Convert labels to binary: -1 for no algae and 1 for algae\n",
    "for i in range(0, len(y)):\n",
    "    if y[i] == 0:\n",
    "        num_no_alg += 1\n",
    "    if y[i] == 1 or y[i] == 2:\n",
    "        y[i] = 1\n",
    "        num_alg += 1\n",
    "\n",
    "# oversample the data set by randomly adding occurences of algae until the difference between the number of algae\n",
    "# samples and no algae samples equals sample_bias (defined below)\n",
    "idx = 0\n",
    "sample_bias = 0\n",
    "length_y = len(y)\n",
    "while num_alg != (num_no_alg + sample_bias):\n",
    "    # circle through the data sets until the difference of num_no_alg and num_alg equals\n",
    "    # the value specified by sample_bias\n",
    "    if idx == (length_y - 1):\n",
    "        idx = 0\n",
    "\n",
    "    if y[idx] == 1:\n",
    "        if np.random.rand() >= 0.5:  # add this sample with some probability\n",
    "            y = np.append(y, y[idx])\n",
    "            X = np.append(X, np.reshape(X[idx, :], newshape=(1, num_features)), axis=0)\n",
    "            num_alg += 1\n",
    "        else:\n",
    "            idx += 1\n",
    "    else:\n",
    "        idx += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Process and split data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# standardize data: remove the mean and variance in each sample\n",
    "num_splits = 2   # do not change\n",
    "sss = model_selection.StratifiedShuffleSplit(n_splits=num_splits, test_size=test_size)\n",
    "\n",
    "idx, _ = sss.split(X, y);\n",
    "train_idx = idx[0]\n",
    "test_idx = idx[1]\n",
    "X_train, X_test = X[train_idx], X[test_idx]\n",
    "y_train, y_test = y[train_idx], y[test_idx]\n",
    "\n",
    "X_train = preprocessing.scale(X_train, axis=1, with_mean=True, with_std=True)\n",
    "X_test = preprocessing.scale(X_test, axis=1, with_mean=True, with_std=True)\n",
    "\n",
    "# convert numpy arrays to pytorch tensors\n",
    "train_set_size = X_train.shape\n",
    "test_set_size = X_test.shape\n",
    "X_train, X_test = torch.from_numpy(X_train), torch.from_numpy(X_test)\n",
    "y_train, y_test = torch.from_numpy(y_train), torch.from_numpy(y_test)\n",
    "\n",
    "# convert pytorch tensors to pytorch TensorDataset\n",
    "train_set = utils.TensorDataset(X_train, y_train)\n",
    "test_set = utils.TensorDataset(X_test, y_test)\n",
    "\n",
    "# create DataLoaders\n",
    "train_loader = utils.DataLoader(train_set, batch_size=batch_size, shuffle=True)\n",
    "test_loader = utils.DataLoader(test_set, batch_size=test_set_size[0], shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define neural network model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CLANet(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(CLANet, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(hidden_size, hidden_size)\n",
    "        self.tanh2 = nn.Tanh()\n",
    "        self.fc3 = nn.Linear(hidden_size, hidden_size)\n",
    "        self.relu3 = nn.ReLU()\n",
    "        self.fc4 = nn.Linear(hidden_size, hidden_size)\n",
    "        self.tanh4 = nn.Tanh()\n",
    "        self.fc5 = nn.Linear(hidden_size, hidden_size)\n",
    "        self.relu5 = nn.ReLU()\n",
    "        self.fc6 = nn.Linear(hidden_size, output_size)     # previously, this was output_size\n",
    "#         self.tanh6 = nn.Tanh()                             # previously, this was the line which was commented out\n",
    "#         self.fc7 = nn.Linear(hidden_size, output_size)\n",
    "#         self.relu7 = nn.ReLU()\n",
    "#         self.fc8 = nn.Linear(hidden_size, hidden_size)\n",
    "#         self.relu8 = nn.ReLU()\n",
    "#         self.fc9 = nn.Linear(hidden_size, output_size)\n",
    "#         self.relu9 = nn.ReLU()\n",
    "#         self.fc10 = nn.Linear(hidden_size, hidden_size)\n",
    "#         self.relu10 = nn.ReLU()\n",
    "#         self.fc11 = nn.Linear(hidden_size, hidden_size)\n",
    "#         self.relu11 = nn.ReLU()\n",
    "#         self.fc12 = nn.Linear(hidden_size, output_size)\n",
    "        self.sig1 = nn.Sigmoid()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        out = self.fc1(x)\n",
    "        out = self.relu1(out)\n",
    "        out = self.fc2(out)\n",
    "        out = self.tanh2(out)\n",
    "        out = self.fc3(out)\n",
    "        out = self.relu3(out)\n",
    "        out = self.fc4(out)\n",
    "        out = self.tanh4(out)\n",
    "        out = self.fc5(out)\n",
    "        out = self.relu5(out)\n",
    "        out = self.fc6(out)\n",
    "#         out = self.tanh6(out)\n",
    "#         out = self.fc7(out)\n",
    "#         out = self.relu7(out)\n",
    "#         out = self.fc8(out)\n",
    "#         out = self.relu8(out)\n",
    "#         out = self.fc9(out)\n",
    "#         out = self.relu9(out)\n",
    "#         out = self.fc10(out)\n",
    "#         out = self.relu10(out)\n",
    "#         out = self.fc11(out)\n",
    "#         out = self.relu11(out)\n",
    "#         out = self.fc12(out)\n",
    "        out = self.sig1(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Instantiate the neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = CLANet(input_size, hidden_size, output_size)\n",
    "criterion = nn.BCELoss()\n",
    "\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate, nesterov=True, momentum=1, dampening=0)\n",
    "# optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate, betas=(0.9, 0.999), eps=1e-8)\n",
    "model.double();     # cast model parameters to double"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/10\n",
      "  Iteration: 1/57, Loss: 0.692858, Error: 0.4896\n",
      "  Iteration: 2/57, Loss: 0.695058, Error: 0.5729\n",
      "  Iteration: 3/57, Loss: 0.693605, Error: 0.5104\n",
      "  Iteration: 4/57, Loss: 0.695045, Error: 0.5208\n",
      "  Iteration: 5/57, Loss: 0.693427, Error: 0.5208\n",
      "  Iteration: 6/57, Loss: 0.693152, Error: 0.4688\n",
      "  Iteration: 7/57, Loss: 0.694094, Error: 0.5000\n",
      "  Iteration: 8/57, Loss: 0.6917, Error: 0.3854\n",
      "  Iteration: 9/57, Loss: 0.690993, Error: 0.5104\n",
      "  Iteration: 10/57, Loss: 0.690006, Error: 0.4792\n",
      "  Iteration: 11/57, Loss: 0.691337, Error: 0.4583\n",
      "  Iteration: 12/57, Loss: 0.685587, Error: 0.4688\n",
      "  Iteration: 13/57, Loss: 0.693422, Error: 0.5625\n",
      "  Iteration: 14/57, Loss: 0.693526, Error: 0.4375\n",
      "  Iteration: 15/57, Loss: 0.684046, Error: 0.4167\n",
      "  Iteration: 16/57, Loss: 0.70876, Error: 0.5312\n",
      "  Iteration: 17/57, Loss: 0.684024, Error: 0.4271\n",
      "  Iteration: 18/57, Loss: 0.693783, Error: 0.4792\n",
      "  Iteration: 19/57, Loss: 0.698626, Error: 0.5208\n",
      "  Iteration: 20/57, Loss: 0.692649, Error: 0.4896\n",
      "  Iteration: 21/57, Loss: 0.688116, Error: 0.4583\n",
      "  Iteration: 22/57, Loss: 0.688985, Error: 0.4271\n",
      "  Iteration: 23/57, Loss: 0.706396, Error: 0.5312\n",
      "  Iteration: 24/57, Loss: 0.689473, Error: 0.4583\n",
      "  Iteration: 25/57, Loss: 0.690087, Error: 0.4375\n",
      "  Iteration: 26/57, Loss: 0.706924, Error: 0.5312\n",
      "  Iteration: 27/57, Loss: 0.691562, Error: 0.4688\n",
      "  Iteration: 28/57, Loss: 0.688569, Error: 0.4375\n",
      "  Iteration: 29/57, Loss: 0.698306, Error: 0.4792\n",
      "  Iteration: 30/57, Loss: 0.705398, Error: 0.5104\n",
      "  Iteration: 31/57, Loss: 0.685922, Error: 0.4375\n",
      "  Iteration: 32/57, Loss: 0.694723, Error: 0.5000\n",
      "  Iteration: 33/57, Loss: 0.698617, Error: 0.5625\n",
      "  Iteration: 34/57, Loss: 0.696855, Error: 0.5417\n",
      "  Iteration: 35/57, Loss: 0.688605, Error: 0.4375\n",
      "  Iteration: 36/57, Loss: 0.696904, Error: 0.5417\n",
      "  Iteration: 37/57, Loss: 0.6922, Error: 0.4792\n",
      "  Iteration: 38/57, Loss: 0.68553, Error: 0.4167\n",
      "  Iteration: 39/57, Loss: 0.709167, Error: 0.5417\n",
      "  Iteration: 40/57, Loss: 0.698055, Error: 0.5000\n",
      "  Iteration: 41/57, Loss: 0.689554, Error: 0.4583\n",
      "  Iteration: 42/57, Loss: 0.696773, Error: 0.5312\n",
      "  Iteration: 43/57, Loss: 0.689144, Error: 0.4271\n",
      "  Iteration: 44/57, Loss: 0.691116, Error: 0.4583\n",
      "  Iteration: 45/57, Loss: 0.69438, Error: 0.5104\n",
      "  Iteration: 46/57, Loss: 0.69557, Error: 0.5833\n",
      "  Iteration: 47/57, Loss: 0.688523, Error: 0.3854\n",
      "  Iteration: 48/57, Loss: 0.692714, Error: 0.4792\n",
      "  Iteration: 49/57, Loss: 0.694202, Error: 0.4792\n",
      "  Iteration: 50/57, Loss: 0.684992, Error: 0.4375\n",
      "  Iteration: 51/57, Loss: 0.727257, Error: 0.5833\n",
      "  Iteration: 52/57, Loss: 0.681491, Error: 0.4271\n",
      "  Iteration: 53/57, Loss: 0.689569, Error: 0.4688\n",
      "  Iteration: 54/57, Loss: 0.695358, Error: 0.5000\n",
      "  Iteration: 55/57, Loss: 0.695125, Error: 0.5104\n",
      "  Iteration: 56/57, Loss: 0.702761, Error: 0.6042\n",
      "  Iteration: 57/57, Loss: 0.695121, Error: 0.5263\n",
      "Average Error for this Epoch: 0.4880\n",
      "found a better model!\n",
      "Epoch: 2/10\n",
      "  Iteration: 1/57, Loss: 0.680397, Error: 0.3750\n",
      "  Iteration: 2/57, Loss: 0.714514, Error: 0.5833\n",
      "  Iteration: 3/57, Loss: 0.691676, Error: 0.4896\n",
      "  Iteration: 4/57, Loss: 0.690628, Error: 0.4375\n",
      "  Iteration: 5/57, Loss: 0.709961, Error: 0.5938\n",
      "  Iteration: 6/57, Loss: 0.678447, Error: 0.3646\n",
      "  Iteration: 7/57, Loss: 0.699799, Error: 0.5521\n",
      "  Iteration: 8/57, Loss: 0.691394, Error: 0.4896\n",
      "  Iteration: 9/57, Loss: 0.690217, Error: 0.4792\n",
      "  Iteration: 10/57, Loss: 0.68343, Error: 0.3750\n",
      "  Iteration: 11/57, Loss: 0.69228, Error: 0.5104\n",
      "  Iteration: 12/57, Loss: 0.691695, Error: 0.4688\n",
      "  Iteration: 13/57, Loss: 0.67548, Error: 0.4062\n",
      "  Iteration: 14/57, Loss: 0.69583, Error: 0.4375\n",
      "  Iteration: 15/57, Loss: 0.68143, Error: 0.4062\n",
      "  Iteration: 16/57, Loss: 0.70144, Error: 0.5000\n",
      "  Iteration: 17/57, Loss: 0.67406, Error: 0.4062\n",
      "  Iteration: 18/57, Loss: 0.699737, Error: 0.4792\n",
      "  Iteration: 19/57, Loss: 0.690214, Error: 0.4792\n",
      "  Iteration: 20/57, Loss: 0.661989, Error: 0.3750\n",
      "  Iteration: 21/57, Loss: 0.71955, Error: 0.4792\n",
      "  Iteration: 22/57, Loss: 0.70638, Error: 0.5104\n",
      "  Iteration: 23/57, Loss: 0.676799, Error: 0.4792\n",
      "  Iteration: 24/57, Loss: 0.701356, Error: 0.5208\n",
      "  Iteration: 25/57, Loss: 0.689615, Error: 0.4583\n",
      "  Iteration: 26/57, Loss: 0.690855, Error: 0.5312\n",
      "  Iteration: 27/57, Loss: 0.691123, Error: 0.5000\n",
      "  Iteration: 28/57, Loss: 0.702547, Error: 0.5521\n",
      "  Iteration: 29/57, Loss: 0.688248, Error: 0.5104\n",
      "  Iteration: 30/57, Loss: 0.689344, Error: 0.4479\n",
      "  Iteration: 31/57, Loss: 0.691159, Error: 0.4792\n",
      "  Iteration: 32/57, Loss: 0.692125, Error: 0.4688\n",
      "  Iteration: 33/57, Loss: 0.70937, Error: 0.5938\n",
      "  Iteration: 34/57, Loss: 0.69373, Error: 0.4479\n",
      "  Iteration: 35/57, Loss: 0.698176, Error: 0.5312\n",
      "  Iteration: 36/57, Loss: 0.680543, Error: 0.3646\n",
      "  Iteration: 37/57, Loss: 0.69374, Error: 0.4792\n",
      "  Iteration: 38/57, Loss: 0.697432, Error: 0.5521\n",
      "  Iteration: 39/57, Loss: 0.681216, Error: 0.4375\n",
      "  Iteration: 40/57, Loss: 0.719457, Error: 0.5312\n",
      "  Iteration: 41/57, Loss: 0.690014, Error: 0.4896\n",
      "  Iteration: 42/57, Loss: 0.686847, Error: 0.4271\n",
      "  Iteration: 43/57, Loss: 0.688649, Error: 0.4583\n",
      "  Iteration: 44/57, Loss: 0.695852, Error: 0.4688\n",
      "  Iteration: 45/57, Loss: 0.70588, Error: 0.4896\n",
      "  Iteration: 46/57, Loss: 0.686834, Error: 0.3854\n",
      "  Iteration: 47/57, Loss: 0.692058, Error: 0.4792\n",
      "  Iteration: 48/57, Loss: 0.69146, Error: 0.5000\n",
      "  Iteration: 49/57, Loss: 0.693096, Error: 0.4896\n",
      "  Iteration: 50/57, Loss: 0.684482, Error: 0.4062\n",
      "  Iteration: 51/57, Loss: 0.684412, Error: 0.4271\n",
      "  Iteration: 52/57, Loss: 0.693735, Error: 0.4375\n",
      "  Iteration: 53/57, Loss: 0.691054, Error: 0.4792\n",
      "  Iteration: 54/57, Loss: 0.703783, Error: 0.5000\n",
      "  Iteration: 55/57, Loss: 0.688761, Error: 0.4792\n",
      "  Iteration: 56/57, Loss: 0.746268, Error: 0.6146\n",
      "  Iteration: 57/57, Loss: 0.686735, Error: 0.4737\n",
      "Average Error for this Epoch: 0.4752\n",
      "found a better model!\n",
      "Epoch: 3/10\n",
      "  Iteration: 1/57, Loss: 0.684267, Error: 0.3854\n",
      "  Iteration: 2/57, Loss: 0.691388, Error: 0.5000\n",
      "  Iteration: 3/57, Loss: 0.700723, Error: 0.5417\n",
      "  Iteration: 4/57, Loss: 0.696268, Error: 0.4583\n",
      "  Iteration: 5/57, Loss: 0.699389, Error: 0.5208\n",
      "  Iteration: 6/57, Loss: 0.698779, Error: 0.4688\n",
      "  Iteration: 7/57, Loss: 0.698881, Error: 0.5417\n",
      "  Iteration: 8/57, Loss: 0.702772, Error: 0.5417\n",
      "  Iteration: 9/57, Loss: 0.685832, Error: 0.4688\n",
      "  Iteration: 10/57, Loss: 0.720058, Error: 0.5625\n",
      "  Iteration: 11/57, Loss: 0.686436, Error: 0.4375\n",
      "  Iteration: 12/57, Loss: 0.69423, Error: 0.5208\n",
      "  Iteration: 13/57, Loss: 0.694522, Error: 0.5208\n",
      "  Iteration: 14/57, Loss: 0.690771, Error: 0.4688\n",
      "  Iteration: 15/57, Loss: 0.692881, Error: 0.4271\n",
      "  Iteration: 16/57, Loss: 0.696563, Error: 0.5417\n",
      "  Iteration: 17/57, Loss: 0.699828, Error: 0.5208\n",
      "  Iteration: 18/57, Loss: 0.693397, Error: 0.4792\n",
      "  Iteration: 19/57, Loss: 0.695739, Error: 0.5417\n",
      "  Iteration: 20/57, Loss: 0.693588, Error: 0.5104\n",
      "  Iteration: 21/57, Loss: 0.692665, Error: 0.5000\n",
      "  Iteration: 22/57, Loss: 0.687515, Error: 0.4479\n",
      "  Iteration: 23/57, Loss: 0.689279, Error: 0.4479\n",
      "  Iteration: 24/57, Loss: 0.695254, Error: 0.5417\n",
      "  Iteration: 25/57, Loss: 0.691046, Error: 0.5000\n",
      "  Iteration: 26/57, Loss: 0.713999, Error: 0.5521\n",
      "  Iteration: 27/57, Loss: 0.689804, Error: 0.4792\n",
      "  Iteration: 28/57, Loss: 0.69064, Error: 0.4583\n",
      "  Iteration: 29/57, Loss: 0.685928, Error: 0.5000\n",
      "  Iteration: 30/57, Loss: 0.697393, Error: 0.4688\n",
      "  Iteration: 31/57, Loss: 0.694531, Error: 0.4896\n",
      "  Iteration: 32/57, Loss: 0.690137, Error: 0.4479\n",
      "  Iteration: 33/57, Loss: 0.695703, Error: 0.4896\n",
      "  Iteration: 34/57, Loss: 0.675639, Error: 0.3646\n",
      "  Iteration: 35/57, Loss: 0.687098, Error: 0.4375\n",
      "  Iteration: 36/57, Loss: 0.704916, Error: 0.5104\n",
      "  Iteration: 37/57, Loss: 0.67784, Error: 0.4375\n",
      "  Iteration: 38/57, Loss: 0.680883, Error: 0.4167\n",
      "  Iteration: 39/57, Loss: 0.69638, Error: 0.5312\n",
      "  Iteration: 40/57, Loss: 0.685705, Error: 0.4583\n",
      "  Iteration: 41/57, Loss: 0.696563, Error: 0.5104\n",
      "  Iteration: 42/57, Loss: 0.688842, Error: 0.4688\n",
      "  Iteration: 43/57, Loss: 0.699753, Error: 0.5312\n",
      "  Iteration: 44/57, Loss: 0.689495, Error: 0.4479\n",
      "  Iteration: 45/57, Loss: 0.71534, Error: 0.5417\n",
      "  Iteration: 46/57, Loss: 0.70464, Error: 0.5208\n",
      "  Iteration: 47/57, Loss: 0.676665, Error: 0.4062\n",
      "  Iteration: 48/57, Loss: 0.6874, Error: 0.5000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Iteration: 49/57, Loss: 0.675773, Error: 0.4375\n",
      "  Iteration: 50/57, Loss: 0.701909, Error: 0.5312\n",
      "  Iteration: 51/57, Loss: 0.692141, Error: 0.4062\n",
      "  Iteration: 52/57, Loss: 0.701636, Error: 0.5000\n",
      "  Iteration: 53/57, Loss: 0.67971, Error: 0.3854\n",
      "  Iteration: 54/57, Loss: 0.693864, Error: 0.4792\n",
      "  Iteration: 55/57, Loss: 0.679226, Error: 0.3958\n",
      "  Iteration: 56/57, Loss: 0.679173, Error: 0.4375\n",
      "  Iteration: 57/57, Loss: 0.696313, Error: 0.4912\n",
      "Average Error for this Epoch: 0.4812\n",
      "Epoch: 4/10\n",
      "  Iteration: 1/57, Loss: 0.70517, Error: 0.5312\n",
      "  Iteration: 2/57, Loss: 0.700132, Error: 0.5104\n",
      "  Iteration: 3/57, Loss: 0.686169, Error: 0.3750\n",
      "  Iteration: 4/57, Loss: 0.696271, Error: 0.4688\n",
      "  Iteration: 5/57, Loss: 0.687942, Error: 0.4792\n",
      "  Iteration: 6/57, Loss: 0.689561, Error: 0.4792\n",
      "  Iteration: 7/57, Loss: 0.685453, Error: 0.4896\n",
      "  Iteration: 8/57, Loss: 0.683718, Error: 0.3958\n",
      "  Iteration: 9/57, Loss: 0.681752, Error: 0.4271\n",
      "  Iteration: 10/57, Loss: 0.705373, Error: 0.5417\n",
      "  Iteration: 11/57, Loss: 0.698243, Error: 0.5104\n",
      "  Iteration: 12/57, Loss: 0.69465, Error: 0.4688\n",
      "  Iteration: 13/57, Loss: 0.693074, Error: 0.4896\n",
      "  Iteration: 14/57, Loss: 0.686313, Error: 0.4896\n",
      "  Iteration: 15/57, Loss: 0.709946, Error: 0.4896\n",
      "  Iteration: 16/57, Loss: 0.691995, Error: 0.4583\n",
      "  Iteration: 17/57, Loss: 0.688953, Error: 0.4375\n",
      "  Iteration: 18/57, Loss: 0.701071, Error: 0.5625\n",
      "  Iteration: 19/57, Loss: 0.68864, Error: 0.4375\n",
      "  Iteration: 20/57, Loss: 0.69895, Error: 0.5625\n",
      "  Iteration: 21/57, Loss: 0.699884, Error: 0.5521\n",
      "  Iteration: 22/57, Loss: 0.693067, Error: 0.5104\n",
      "  Iteration: 23/57, Loss: 0.693632, Error: 0.5521\n",
      "  Iteration: 24/57, Loss: 0.691823, Error: 0.4896\n",
      "  Iteration: 25/57, Loss: 0.686809, Error: 0.4167\n",
      "  Iteration: 26/57, Loss: 0.684568, Error: 0.3750\n",
      "  Iteration: 27/57, Loss: 0.705581, Error: 0.5312\n",
      "  Iteration: 28/57, Loss: 0.707644, Error: 0.5312\n",
      "  Iteration: 29/57, Loss: 0.686222, Error: 0.4896\n",
      "  Iteration: 30/57, Loss: 0.685373, Error: 0.4479\n",
      "  Iteration: 31/57, Loss: 0.675506, Error: 0.3958\n",
      "  Iteration: 32/57, Loss: 0.696429, Error: 0.5000\n",
      "  Iteration: 33/57, Loss: 0.675332, Error: 0.4583\n",
      "  Iteration: 34/57, Loss: 0.702158, Error: 0.5625\n",
      "  Iteration: 35/57, Loss: 0.682401, Error: 0.4062\n",
      "  Iteration: 36/57, Loss: 0.695246, Error: 0.5208\n",
      "  Iteration: 37/57, Loss: 0.681801, Error: 0.4375\n",
      "  Iteration: 38/57, Loss: 0.685141, Error: 0.5104\n",
      "  Iteration: 39/57, Loss: 0.693909, Error: 0.4375\n",
      "  Iteration: 40/57, Loss: 0.693637, Error: 0.5000\n",
      "  Iteration: 41/57, Loss: 0.69772, Error: 0.5208\n",
      "  Iteration: 42/57, Loss: 0.683416, Error: 0.4167\n",
      "  Iteration: 43/57, Loss: 0.68406, Error: 0.4167\n",
      "  Iteration: 44/57, Loss: 0.688156, Error: 0.4688\n",
      "  Iteration: 45/57, Loss: 0.694584, Error: 0.4792\n",
      "  Iteration: 46/57, Loss: 0.683252, Error: 0.4375\n",
      "  Iteration: 47/57, Loss: 0.688515, Error: 0.4583\n",
      "  Iteration: 48/57, Loss: 0.674338, Error: 0.4479\n",
      "  Iteration: 49/57, Loss: 0.69984, Error: 0.4792\n",
      "  Iteration: 50/57, Loss: 0.679049, Error: 0.4375\n",
      "  Iteration: 51/57, Loss: 0.682372, Error: 0.4583\n",
      "  Iteration: 52/57, Loss: 0.674608, Error: 0.3958\n",
      "  Iteration: 53/57, Loss: 0.667655, Error: 0.4688\n",
      "  Iteration: 54/57, Loss: 0.685201, Error: 0.4271\n",
      "  Iteration: 55/57, Loss: 0.69052, Error: 0.5104\n",
      "  Iteration: 56/57, Loss: 0.675829, Error: 0.4062\n",
      "  Iteration: 57/57, Loss: 0.692755, Error: 0.4561\n",
      "Average Error for this Epoch: 0.4722\n",
      "found a better model!\n",
      "Epoch: 5/10\n",
      "  Iteration: 1/57, Loss: 0.685337, Error: 0.4479\n",
      "  Iteration: 2/57, Loss: 0.702901, Error: 0.5312\n",
      "  Iteration: 3/57, Loss: 0.693252, Error: 0.4583\n",
      "  Iteration: 4/57, Loss: 0.704723, Error: 0.4792\n",
      "  Iteration: 5/57, Loss: 0.69631, Error: 0.5417\n",
      "  Iteration: 6/57, Loss: 0.688807, Error: 0.5417\n",
      "  Iteration: 7/57, Loss: 0.695336, Error: 0.5208\n",
      "  Iteration: 8/57, Loss: 0.690527, Error: 0.5104\n",
      "  Iteration: 9/57, Loss: 0.692532, Error: 0.5312\n",
      "  Iteration: 10/57, Loss: 0.688765, Error: 0.4375\n",
      "  Iteration: 11/57, Loss: 0.684659, Error: 0.4792\n",
      "  Iteration: 12/57, Loss: 0.70481, Error: 0.5104\n",
      "  Iteration: 13/57, Loss: 0.699213, Error: 0.3750\n",
      "  Iteration: 14/57, Loss: 0.676169, Error: 0.4375\n",
      "  Iteration: 15/57, Loss: 0.681926, Error: 0.4479\n",
      "  Iteration: 16/57, Loss: 0.692447, Error: 0.5312\n",
      "  Iteration: 17/57, Loss: 0.691153, Error: 0.4583\n",
      "  Iteration: 18/57, Loss: 0.696444, Error: 0.4688\n",
      "  Iteration: 19/57, Loss: 0.695432, Error: 0.5312\n",
      "  Iteration: 20/57, Loss: 0.687196, Error: 0.5208\n",
      "  Iteration: 21/57, Loss: 0.685555, Error: 0.4896\n",
      "  Iteration: 22/57, Loss: 0.708985, Error: 0.5417\n",
      "  Iteration: 23/57, Loss: 0.685398, Error: 0.3854\n",
      "  Iteration: 24/57, Loss: 0.685473, Error: 0.4062\n",
      "  Iteration: 25/57, Loss: 0.704725, Error: 0.5208\n",
      "  Iteration: 26/57, Loss: 0.683486, Error: 0.3958\n",
      "  Iteration: 27/57, Loss: 0.683821, Error: 0.4792\n",
      "  Iteration: 28/57, Loss: 0.680008, Error: 0.4583\n",
      "  Iteration: 29/57, Loss: 0.670411, Error: 0.4167\n",
      "  Iteration: 30/57, Loss: 0.748481, Error: 0.5417\n",
      "  Iteration: 31/57, Loss: 0.705355, Error: 0.5208\n",
      "  Iteration: 32/57, Loss: 0.697137, Error: 0.4792\n",
      "  Iteration: 33/57, Loss: 0.676823, Error: 0.4062\n",
      "  Iteration: 34/57, Loss: 0.680734, Error: 0.3854\n",
      "  Iteration: 35/57, Loss: 0.708754, Error: 0.4479\n",
      "  Iteration: 36/57, Loss: 0.679478, Error: 0.4271\n",
      "  Iteration: 37/57, Loss: 0.702856, Error: 0.5417\n",
      "  Iteration: 38/57, Loss: 0.689206, Error: 0.4271\n",
      "  Iteration: 39/57, Loss: 0.675268, Error: 0.4375\n",
      "  Iteration: 40/57, Loss: 0.711697, Error: 0.5000\n",
      "  Iteration: 41/57, Loss: 0.717912, Error: 0.5312\n",
      "  Iteration: 42/57, Loss: 0.683975, Error: 0.4167\n",
      "  Iteration: 43/57, Loss: 0.684891, Error: 0.4271\n",
      "  Iteration: 44/57, Loss: 0.683183, Error: 0.5000\n",
      "  Iteration: 45/57, Loss: 0.690364, Error: 0.4167\n",
      "  Iteration: 46/57, Loss: 0.683766, Error: 0.5000\n",
      "  Iteration: 47/57, Loss: 0.691551, Error: 0.5104\n",
      "  Iteration: 48/57, Loss: 0.69193, Error: 0.4688\n",
      "  Iteration: 49/57, Loss: 0.693454, Error: 0.4896\n",
      "  Iteration: 50/57, Loss: 0.704719, Error: 0.5208\n",
      "  Iteration: 51/57, Loss: 0.697712, Error: 0.5521\n",
      "  Iteration: 52/57, Loss: 0.685795, Error: 0.4583\n",
      "  Iteration: 53/57, Loss: 0.693182, Error: 0.5521\n",
      "  Iteration: 54/57, Loss: 0.693099, Error: 0.5312\n",
      "  Iteration: 55/57, Loss: 0.691625, Error: 0.4583\n",
      "  Iteration: 56/57, Loss: 0.697894, Error: 0.5208\n",
      "  Iteration: 57/57, Loss: 0.69131, Error: 0.4561\n",
      "Average Error for this Epoch: 0.4786\n",
      "Epoch: 6/10\n",
      "  Iteration: 1/57, Loss: 0.691751, Error: 0.4688\n",
      "  Iteration: 2/57, Loss: 0.692832, Error: 0.4896\n",
      "  Iteration: 3/57, Loss: 0.683739, Error: 0.4271\n",
      "  Iteration: 4/57, Loss: 0.69625, Error: 0.4896\n",
      "  Iteration: 5/57, Loss: 0.69429, Error: 0.4688\n",
      "  Iteration: 6/57, Loss: 0.69326, Error: 0.5000\n",
      "  Iteration: 7/57, Loss: 0.698591, Error: 0.4583\n",
      "  Iteration: 8/57, Loss: 0.702314, Error: 0.5000\n",
      "  Iteration: 9/57, Loss: 0.69443, Error: 0.4375\n",
      "  Iteration: 10/57, Loss: 0.690178, Error: 0.5000\n",
      "  Iteration: 11/57, Loss: 0.69104, Error: 0.5104\n",
      "  Iteration: 12/57, Loss: 0.686348, Error: 0.4896\n",
      "  Iteration: 13/57, Loss: 0.68598, Error: 0.4479\n",
      "  Iteration: 14/57, Loss: 0.686827, Error: 0.4792\n",
      "  Iteration: 15/57, Loss: 0.704905, Error: 0.5104\n",
      "  Iteration: 16/57, Loss: 0.688801, Error: 0.5000\n",
      "  Iteration: 17/57, Loss: 0.693028, Error: 0.5208\n",
      "  Iteration: 18/57, Loss: 0.679677, Error: 0.4688\n",
      "  Iteration: 19/57, Loss: 0.703902, Error: 0.4896\n",
      "  Iteration: 20/57, Loss: 0.691136, Error: 0.5208\n",
      "  Iteration: 21/57, Loss: 0.690784, Error: 0.4167\n",
      "  Iteration: 22/57, Loss: 0.695479, Error: 0.5938\n",
      "  Iteration: 23/57, Loss: 0.6877, Error: 0.4479\n",
      "  Iteration: 24/57, Loss: 0.696693, Error: 0.4375\n",
      "  Iteration: 25/57, Loss: 0.677115, Error: 0.4062\n",
      "  Iteration: 26/57, Loss: 0.703915, Error: 0.5208\n",
      "  Iteration: 27/57, Loss: 0.695664, Error: 0.5417\n",
      "  Iteration: 28/57, Loss: 0.703098, Error: 0.5417\n",
      "  Iteration: 29/57, Loss: 0.689834, Error: 0.5104\n",
      "  Iteration: 30/57, Loss: 0.691924, Error: 0.5000\n",
      "  Iteration: 31/57, Loss: 0.686319, Error: 0.4688\n",
      "  Iteration: 32/57, Loss: 0.695261, Error: 0.5312\n",
      "  Iteration: 33/57, Loss: 0.6927, Error: 0.4792\n",
      "  Iteration: 34/57, Loss: 0.702519, Error: 0.5312\n",
      "  Iteration: 35/57, Loss: 0.685107, Error: 0.4375\n",
      "  Iteration: 36/57, Loss: 0.687728, Error: 0.4479\n",
      "  Iteration: 37/57, Loss: 0.695421, Error: 0.4896\n",
      "  Iteration: 38/57, Loss: 0.685488, Error: 0.4688\n",
      "  Iteration: 39/57, Loss: 0.714637, Error: 0.5417\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Iteration: 40/57, Loss: 0.720641, Error: 0.5625\n",
      "  Iteration: 41/57, Loss: 0.689874, Error: 0.4792\n",
      "  Iteration: 42/57, Loss: 0.686544, Error: 0.5104\n",
      "  Iteration: 43/57, Loss: 0.697649, Error: 0.5417\n",
      "  Iteration: 44/57, Loss: 0.676205, Error: 0.3854\n",
      "  Iteration: 45/57, Loss: 0.689326, Error: 0.4792\n",
      "  Iteration: 46/57, Loss: 0.698385, Error: 0.4271\n",
      "  Iteration: 47/57, Loss: 0.69167, Error: 0.4896\n",
      "  Iteration: 48/57, Loss: 0.693276, Error: 0.3854\n",
      "  Iteration: 49/57, Loss: 0.693046, Error: 0.4896\n",
      "  Iteration: 50/57, Loss: 0.68448, Error: 0.4271\n",
      "  Iteration: 51/57, Loss: 0.693925, Error: 0.5000\n",
      "  Iteration: 52/57, Loss: 0.690028, Error: 0.4479\n",
      "  Iteration: 53/57, Loss: 0.674902, Error: 0.3854\n",
      "  Iteration: 54/57, Loss: 0.687338, Error: 0.4896\n",
      "  Iteration: 55/57, Loss: 0.686172, Error: 0.4792\n",
      "  Iteration: 56/57, Loss: 0.688713, Error: 0.4375\n",
      "  Iteration: 57/57, Loss: 0.683199, Error: 0.4737\n",
      "Average Error for this Epoch: 0.4803\n",
      "Epoch: 7/10\n",
      "  Iteration: 1/57, Loss: 0.668283, Error: 0.3646\n",
      "  Iteration: 2/57, Loss: 0.692545, Error: 0.5208\n",
      "  Iteration: 3/57, Loss: 0.676915, Error: 0.4062\n",
      "  Iteration: 4/57, Loss: 0.680104, Error: 0.4062\n",
      "  Iteration: 5/57, Loss: 0.728567, Error: 0.5521\n",
      "  Iteration: 6/57, Loss: 0.743226, Error: 0.5312\n",
      "  Iteration: 7/57, Loss: 0.70225, Error: 0.4896\n",
      "  Iteration: 8/57, Loss: 0.690172, Error: 0.4792\n",
      "  Iteration: 9/57, Loss: 0.683058, Error: 0.4583\n",
      "  Iteration: 10/57, Loss: 0.686192, Error: 0.4479\n",
      "  Iteration: 11/57, Loss: 0.685775, Error: 0.4688\n",
      "  Iteration: 12/57, Loss: 0.670377, Error: 0.4062\n",
      "  Iteration: 13/57, Loss: 0.804352, Error: 0.5000\n",
      "  Iteration: 14/57, Loss: 0.65364, Error: 0.3646\n",
      "  Iteration: 15/57, Loss: 0.74912, Error: 0.5208\n",
      "  Iteration: 16/57, Loss: 0.731807, Error: 0.5000\n",
      "  Iteration: 17/57, Loss: 0.661566, Error: 0.3750\n",
      "  Iteration: 18/57, Loss: 0.734372, Error: 0.5208\n",
      "  Iteration: 19/57, Loss: 0.723905, Error: 0.5208\n",
      "  Iteration: 20/57, Loss: 0.737462, Error: 0.6042\n",
      "  Iteration: 21/57, Loss: 0.690816, Error: 0.4688\n",
      "  Iteration: 22/57, Loss: 0.693371, Error: 0.5104\n",
      "  Iteration: 23/57, Loss: 0.699342, Error: 0.5521\n",
      "  Iteration: 24/57, Loss: 0.743041, Error: 0.5000\n",
      "  Iteration: 25/57, Loss: 0.691701, Error: 0.5000\n",
      "  Iteration: 26/57, Loss: 0.697993, Error: 0.5104\n",
      "  Iteration: 27/57, Loss: 0.696552, Error: 0.5521\n",
      "  Iteration: 28/57, Loss: 0.6949, Error: 0.4688\n",
      "  Iteration: 29/57, Loss: 0.685511, Error: 0.4167\n",
      "  Iteration: 30/57, Loss: 0.686728, Error: 0.4062\n",
      "  Iteration: 31/57, Loss: 0.68793, Error: 0.4479\n",
      "  Iteration: 32/57, Loss: 0.69729, Error: 0.4896\n",
      "  Iteration: 33/57, Loss: 0.678176, Error: 0.3646\n",
      "  Iteration: 34/57, Loss: 0.759746, Error: 0.5625\n",
      "  Iteration: 35/57, Loss: 0.701713, Error: 0.4896\n",
      "  Iteration: 36/57, Loss: 0.704569, Error: 0.5625\n",
      "  Iteration: 37/57, Loss: 0.683579, Error: 0.4062\n",
      "  Iteration: 38/57, Loss: 0.714681, Error: 0.4479\n",
      "  Iteration: 39/57, Loss: 5.15115, Error: 0.5833\n",
      "  Iteration: 40/57, Loss: 0.778518, Error: 0.5521\n",
      "  Iteration: 41/57, Loss: 0.757757, Error: 0.5312\n",
      "  Iteration: 42/57, Loss: 0.920238, Error: 0.4792\n",
      "  Iteration: 43/57, Loss: 0.70254, Error: 0.6354\n",
      "  Iteration: 44/57, Loss: 0.720276, Error: 0.5833\n",
      "  Iteration: 45/57, Loss: 0.700681, Error: 0.5312\n",
      "  Iteration: 46/57, Loss: 0.693914, Error: 0.5000\n",
      "  Iteration: 47/57, Loss: 0.693836, Error: 0.5938\n",
      "  Iteration: 48/57, Loss: 0.70073, Error: 0.5833\n",
      "  Iteration: 49/57, Loss: 0.691207, Error: 0.4583\n",
      "  Iteration: 50/57, Loss: 0.728449, Error: 0.5312\n",
      "  Iteration: 51/57, Loss: 0.691616, Error: 0.4688\n",
      "  Iteration: 52/57, Loss: 0.700781, Error: 0.4896\n",
      "  Iteration: 53/57, Loss: 0.693933, Error: 0.5417\n",
      "  Iteration: 54/57, Loss: 0.694406, Error: 0.4583\n",
      "  Iteration: 55/57, Loss: 1.14299, Error: 0.4896\n",
      "  Iteration: 56/57, Loss: 14.679, Error: 0.5312\n",
      "  Iteration: 57/57, Loss: 14.0579, Error: 0.5088\n",
      "Average Error for this Epoch: 0.4938\n",
      "Epoch: 8/10\n",
      "  Iteration: 1/57, Loss: 12.3764, Error: 0.4479\n",
      "  Iteration: 2/57, Loss: 15.5424, Error: 0.5625\n",
      "  Iteration: 3/57, Loss: 14.679, Error: 0.5312\n",
      "  Iteration: 4/57, Loss: 12.952, Error: 0.4688\n",
      "  Iteration: 5/57, Loss: 14.3912, Error: 0.5208\n",
      "  Iteration: 6/57, Loss: 15.2546, Error: 0.5521\n",
      "  Iteration: 7/57, Loss: 12.952, Error: 0.4688\n",
      "  Iteration: 8/57, Loss: 14.1033, Error: 0.5104\n",
      "  Iteration: 9/57, Loss: 15.2546, Error: 0.5521\n",
      "  Iteration: 10/57, Loss: 12.3764, Error: 0.4479\n",
      "  Iteration: 11/57, Loss: 13.2399, Error: 0.4792\n",
      "  Iteration: 12/57, Loss: 12.3764, Error: 0.4479\n",
      "  Iteration: 13/57, Loss: 12.3764, Error: 0.4479\n",
      "  Iteration: 14/57, Loss: 14.679, Error: 0.5312\n",
      "  Iteration: 15/57, Loss: 13.5277, Error: 0.4896\n",
      "  Iteration: 16/57, Loss: 13.8155, Error: 0.5000\n",
      "  Iteration: 17/57, Loss: 13.2399, Error: 0.4792\n",
      "  Iteration: 18/57, Loss: 15.2546, Error: 0.5521\n",
      "  Iteration: 19/57, Loss: 14.3912, Error: 0.5208\n",
      "  Iteration: 20/57, Loss: 14.1033, Error: 0.5104\n",
      "  Iteration: 21/57, Loss: 14.679, Error: 0.5312\n",
      "  Iteration: 22/57, Loss: 14.1033, Error: 0.5104\n",
      "  Iteration: 23/57, Loss: 12.6642, Error: 0.4583\n",
      "  Iteration: 24/57, Loss: 14.3912, Error: 0.5208\n",
      "  Iteration: 25/57, Loss: 12.0886, Error: 0.4375\n",
      "  Iteration: 26/57, Loss: 14.1033, Error: 0.5104\n",
      "  Iteration: 27/57, Loss: 13.2399, Error: 0.4792\n",
      "  Iteration: 28/57, Loss: 15.8303, Error: 0.5729\n",
      "  Iteration: 29/57, Loss: 12.6642, Error: 0.4583\n",
      "  Iteration: 30/57, Loss: 12.0886, Error: 0.4375\n",
      "  Iteration: 31/57, Loss: 12.6642, Error: 0.4583\n",
      "  Iteration: 32/57, Loss: 13.2399, Error: 0.4792\n",
      "  Iteration: 33/57, Loss: 12.6642, Error: 0.4583\n",
      "  Iteration: 34/57, Loss: 14.9668, Error: 0.5417\n",
      "  Iteration: 35/57, Loss: 13.8155, Error: 0.5000\n",
      "  Iteration: 36/57, Loss: 14.9668, Error: 0.5417\n",
      "  Iteration: 37/57, Loss: 14.3912, Error: 0.5208\n",
      "  Iteration: 38/57, Loss: 12.952, Error: 0.4688\n",
      "  Iteration: 39/57, Loss: 12.6642, Error: 0.4583\n",
      "  Iteration: 40/57, Loss: 13.5277, Error: 0.4896\n",
      "  Iteration: 41/57, Loss: 14.3912, Error: 0.5208\n",
      "  Iteration: 42/57, Loss: 14.679, Error: 0.5312\n",
      "  Iteration: 43/57, Loss: 14.1033, Error: 0.5104\n",
      "  Iteration: 44/57, Loss: 13.2399, Error: 0.4792\n",
      "  Iteration: 45/57, Loss: 17.5572, Error: 0.6354\n",
      "  Iteration: 46/57, Loss: 15.2546, Error: 0.5521\n",
      "  Iteration: 47/57, Loss: 14.9668, Error: 0.5417\n",
      "  Iteration: 48/57, Loss: 12.6642, Error: 0.4583\n",
      "  Iteration: 49/57, Loss: 11.2251, Error: 0.4062\n",
      "  Iteration: 50/57, Loss: 13.5277, Error: 0.4896\n",
      "  Iteration: 51/57, Loss: 14.679, Error: 0.5312\n",
      "  Iteration: 52/57, Loss: 12.3764, Error: 0.4479\n",
      "  Iteration: 53/57, Loss: 12.6642, Error: 0.4583\n",
      "  Iteration: 54/57, Loss: 14.1033, Error: 0.5104\n",
      "  Iteration: 55/57, Loss: 12.952, Error: 0.4688\n",
      "  Iteration: 56/57, Loss: 14.9668, Error: 0.5417\n",
      "  Iteration: 57/57, Loss: 16.4817, Error: 0.5965\n",
      "Average Error for this Epoch: 0.5006\n",
      "Epoch: 9/10\n",
      "  Iteration: 1/57, Loss: 12.952, Error: 0.4688\n",
      "  Iteration: 2/57, Loss: 13.5277, Error: 0.4896\n",
      "  Iteration: 3/57, Loss: 13.8155, Error: 0.5000\n",
      "  Iteration: 4/57, Loss: 12.6642, Error: 0.4583\n",
      "  Iteration: 5/57, Loss: 11.2251, Error: 0.4062\n",
      "  Iteration: 6/57, Loss: 12.952, Error: 0.4688\n",
      "  Iteration: 7/57, Loss: 12.3764, Error: 0.4479\n",
      "  Iteration: 8/57, Loss: 13.5277, Error: 0.4896\n",
      "  Iteration: 9/57, Loss: 13.8155, Error: 0.5000\n",
      "  Iteration: 10/57, Loss: 12.0886, Error: 0.4375\n",
      "  Iteration: 11/57, Loss: 12.3764, Error: 0.4479\n",
      "  Iteration: 12/57, Loss: 14.679, Error: 0.5312\n",
      "  Iteration: 13/57, Loss: 11.2251, Error: 0.4062\n",
      "  Iteration: 14/57, Loss: 13.2399, Error: 0.4792\n",
      "  Iteration: 15/57, Loss: 15.2546, Error: 0.5521\n",
      "  Iteration: 16/57, Loss: 14.9668, Error: 0.5417\n",
      "  Iteration: 17/57, Loss: 13.5277, Error: 0.4896\n",
      "  Iteration: 18/57, Loss: 14.3912, Error: 0.5208\n",
      "  Iteration: 19/57, Loss: 12.0886, Error: 0.4375\n",
      "  Iteration: 20/57, Loss: 15.5424, Error: 0.5625\n",
      "  Iteration: 21/57, Loss: 13.8155, Error: 0.5000\n",
      "  Iteration: 22/57, Loss: 16.1181, Error: 0.5833\n",
      "  Iteration: 23/57, Loss: 15.2546, Error: 0.5521\n",
      "  Iteration: 24/57, Loss: 16.1181, Error: 0.5833\n",
      "  Iteration: 25/57, Loss: 13.8155, Error: 0.5000\n",
      "  Iteration: 26/57, Loss: 14.3912, Error: 0.5208\n",
      "  Iteration: 27/57, Loss: 15.8303, Error: 0.5729\n",
      "  Iteration: 28/57, Loss: 16.1181, Error: 0.5833\n",
      "  Iteration: 29/57, Loss: 10.9373, Error: 0.3958\n",
      "  Iteration: 30/57, Loss: 14.1033, Error: 0.5104\n",
      "  Iteration: 31/57, Loss: 15.8303, Error: 0.5729\n",
      "  Iteration: 32/57, Loss: 13.5277, Error: 0.4896\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Iteration: 33/57, Loss: 13.8155, Error: 0.5000\n",
      "  Iteration: 34/57, Loss: 13.8155, Error: 0.5000\n",
      "  Iteration: 35/57, Loss: 14.679, Error: 0.5312\n",
      "  Iteration: 36/57, Loss: 12.952, Error: 0.4688\n",
      "  Iteration: 37/57, Loss: 12.6642, Error: 0.4583\n",
      "  Iteration: 38/57, Loss: 14.1033, Error: 0.5104\n",
      "  Iteration: 39/57, Loss: 13.8155, Error: 0.5000\n",
      "  Iteration: 40/57, Loss: 12.3764, Error: 0.4479\n",
      "  Iteration: 41/57, Loss: 12.3764, Error: 0.4479\n",
      "  Iteration: 42/57, Loss: 16.4059, Error: 0.5938\n",
      "  Iteration: 43/57, Loss: 12.3764, Error: 0.4479\n",
      "  Iteration: 44/57, Loss: 14.9668, Error: 0.5417\n",
      "  Iteration: 45/57, Loss: 12.0886, Error: 0.4375\n",
      "  Iteration: 46/57, Loss: 14.3912, Error: 0.5208\n",
      "  Iteration: 47/57, Loss: 17.2694, Error: 0.6250\n",
      "  Iteration: 48/57, Loss: 12.6642, Error: 0.4583\n",
      "  Iteration: 49/57, Loss: 12.952, Error: 0.4688\n",
      "  Iteration: 50/57, Loss: 12.952, Error: 0.4688\n",
      "  Iteration: 51/57, Loss: 14.3912, Error: 0.5208\n",
      "  Iteration: 52/57, Loss: 14.1033, Error: 0.5104\n",
      "  Iteration: 53/57, Loss: 15.5424, Error: 0.5625\n",
      "  Iteration: 54/57, Loss: 12.6642, Error: 0.4583\n",
      "  Iteration: 55/57, Loss: 13.5277, Error: 0.4896\n",
      "  Iteration: 56/57, Loss: 15.2546, Error: 0.5521\n",
      "  Iteration: 57/57, Loss: 12.6036, Error: 0.4561\n",
      "Average Error for this Epoch: 0.4996\n",
      "Epoch: 10/10\n",
      "  Iteration: 1/57, Loss: 13.8155, Error: 0.5000\n",
      "  Iteration: 2/57, Loss: 14.3912, Error: 0.5208\n",
      "  Iteration: 3/57, Loss: 11.5129, Error: 0.4167\n",
      "  Iteration: 4/57, Loss: 12.6642, Error: 0.4583\n",
      "  Iteration: 5/57, Loss: 16.4059, Error: 0.5938\n",
      "  Iteration: 6/57, Loss: 11.5129, Error: 0.4167\n",
      "  Iteration: 7/57, Loss: 12.3764, Error: 0.4479\n",
      "  Iteration: 8/57, Loss: 12.0886, Error: 0.4375\n",
      "  Iteration: 9/57, Loss: 13.5277, Error: 0.4896\n",
      "  Iteration: 10/57, Loss: 13.8155, Error: 0.5000\n",
      "  Iteration: 11/57, Loss: 14.3912, Error: 0.5208\n",
      "  Iteration: 12/57, Loss: 12.6642, Error: 0.4583\n",
      "  Iteration: 13/57, Loss: 13.5277, Error: 0.4896\n",
      "  Iteration: 14/57, Loss: 13.8155, Error: 0.5000\n",
      "  Iteration: 15/57, Loss: 13.2399, Error: 0.4792\n",
      "  Iteration: 16/57, Loss: 14.1033, Error: 0.5104\n",
      "  Iteration: 17/57, Loss: 12.6642, Error: 0.4583\n",
      "  Iteration: 18/57, Loss: 14.9668, Error: 0.5417\n",
      "  Iteration: 19/57, Loss: 11.5129, Error: 0.4167\n",
      "  Iteration: 20/57, Loss: 15.5424, Error: 0.5625\n",
      "  Iteration: 21/57, Loss: 15.5424, Error: 0.5625\n",
      "  Iteration: 22/57, Loss: 13.8155, Error: 0.5000\n",
      "  Iteration: 23/57, Loss: 14.3912, Error: 0.5208\n",
      "  Iteration: 24/57, Loss: 12.3764, Error: 0.4479\n",
      "  Iteration: 25/57, Loss: 13.8155, Error: 0.5000\n",
      "  Iteration: 26/57, Loss: 13.2399, Error: 0.4792\n",
      "  Iteration: 27/57, Loss: 14.3912, Error: 0.5208\n",
      "  Iteration: 28/57, Loss: 12.952, Error: 0.4688\n",
      "  Iteration: 29/57, Loss: 14.3912, Error: 0.5208\n",
      "  Iteration: 30/57, Loss: 12.952, Error: 0.4688\n",
      "  Iteration: 31/57, Loss: 13.8155, Error: 0.5000\n",
      "  Iteration: 32/57, Loss: 12.952, Error: 0.4688\n",
      "  Iteration: 33/57, Loss: 10.9373, Error: 0.3958\n",
      "  Iteration: 34/57, Loss: 14.9668, Error: 0.5417\n",
      "  Iteration: 35/57, Loss: 14.1033, Error: 0.5104\n",
      "  Iteration: 36/57, Loss: 14.3912, Error: 0.5208\n",
      "  Iteration: 37/57, Loss: 15.5424, Error: 0.5625\n",
      "  Iteration: 38/57, Loss: 16.6937, Error: 0.6042\n",
      "  Iteration: 39/57, Loss: 11.5129, Error: 0.4167\n",
      "  Iteration: 40/57, Loss: 14.679, Error: 0.5312\n",
      "  Iteration: 41/57, Loss: 16.9816, Error: 0.6146\n",
      "  Iteration: 42/57, Loss: 15.5424, Error: 0.5625\n",
      "  Iteration: 43/57, Loss: 12.3764, Error: 0.4479\n",
      "  Iteration: 44/57, Loss: 14.1033, Error: 0.5104\n",
      "  Iteration: 45/57, Loss: 14.9668, Error: 0.5417\n",
      "  Iteration: 46/57, Loss: 14.3912, Error: 0.5208\n",
      "  Iteration: 47/57, Loss: 15.8303, Error: 0.5729\n",
      "  Iteration: 48/57, Loss: 14.1033, Error: 0.5104\n",
      "  Iteration: 49/57, Loss: 12.3764, Error: 0.4479\n",
      "  Iteration: 50/57, Loss: 14.9668, Error: 0.5417\n",
      "  Iteration: 51/57, Loss: 12.3764, Error: 0.4479\n",
      "  Iteration: 52/57, Loss: 15.8303, Error: 0.5729\n",
      "  Iteration: 53/57, Loss: 13.8155, Error: 0.5000\n",
      "  Iteration: 54/57, Loss: 14.3912, Error: 0.5208\n",
      "  Iteration: 55/57, Loss: 13.5277, Error: 0.4896\n",
      "  Iteration: 56/57, Loss: 11.8007, Error: 0.4271\n",
      "  Iteration: 57/57, Loss: 14.0579, Error: 0.5088\n",
      "Average Error for this Epoch: 0.5000\n"
     ]
    }
   ],
   "source": [
    "model.train()     # training mode\n",
    "training_loss = []\n",
    "avg_error = 0\n",
    "avg_error_vec = []\n",
    "best_avg_error = 1\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    print(\"Epoch: %d/%d\" % (epoch+1, num_epochs))\n",
    "\n",
    "    for i, (samples, labels) in enumerate(train_loader):\n",
    "        samples = Variable(samples)\n",
    "        labels = Variable(labels)\n",
    "        output = model(samples)                # forward pass\n",
    "        output = torch.flatten(output)         # resize predicted labels\n",
    "        labels = labels.type(torch.DoubleTensor)\n",
    "        \n",
    "        loss = criterion(output, labels)  # calculate loss\n",
    "        optimizer.zero_grad()     # clear gradient\n",
    "        loss.backward()           # calculate gradients\n",
    "        optimizer.step()          # update weights\n",
    "        \n",
    "        # calculate and print error\n",
    "        out = output\n",
    "\n",
    "        for j in range(0, out.size()[0]):\n",
    "            if out[j] < 0.5:\n",
    "                out[j] = 0\n",
    "            else:\n",
    "                out[j] = 1\n",
    "        error = 1 - torch.sum(output == labels).item() / labels.size()[0]\n",
    "        avg_error += error\n",
    "        training_loss.append(loss.data.numpy())\n",
    "        print(\"  Iteration: %d/%d, Loss: %g, Error: %0.4f\" % \n",
    "              (i+1, np.ceil(X_train.size()[0] / batch_size).astype(int), loss.item(), error))\n",
    "    \n",
    "    avg_error = avg_error / np.ceil(X_train.size()[0] / batch_size)\n",
    "    avg_error_vec.append(avg_error)\n",
    "    print(\"Average Error for this Epoch: %0.4f\" % avg_error)\n",
    "\n",
    "    if avg_error < best_avg_error:\n",
    "        print(\"found a better model!\")\n",
    "        best_avg_error = avg_error\n",
    "        best_model = copy.deepcopy(model)\n",
    "    \n",
    "    avg_error = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure()\n",
    "plt.plot(np.linspace(start=1, stop=num_epochs, num=num_epochs), avg_error_vec)\n",
    "plt.xlabel(\"Number of Epochs\")\n",
    "plt.ylabel(\"Average Training Error\")\n",
    "plt.title(\"Average Training Error for Epochs=1:100\")\n",
    "plt.grid(True)\n",
    "# plt.savefig(\"Neural Net Training Error.jpg\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate model on testing set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model.eval()\n",
    "\n",
    "for i, (samples, labels) in enumerate(test_loader):\n",
    "    samples = Variable(samples)\n",
    "    labels = Variable(labels)\n",
    "    predictions = best_model(samples)\n",
    "    predictions = torch.flatten(predictions)\n",
    "    labels = labels.type(torch.DoubleTensor)\n",
    "\n",
    "    for j in range(0, predictions.size()[0]):\n",
    "        if predictions[j] < 0.5:\n",
    "            predictions[j] = 0\n",
    "        else:\n",
    "            predictions[j] = 1\n",
    "    \n",
    "    error = 1 - torch.sum(predictions == labels).item() / labels.size()[0]\n",
    "    \n",
    "    print(\"Testing set Error: %0.4f\" % error)\n",
    "\n",
    "model_path = dest_path + \"torch_model_3_24_19_lr=\" + str(learning_rate) + \"all_data_one_hot_dict.pt\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load and Evaluate previous models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = CLANet(input_size, hidden_size, output_size)\n",
    "model.load_state_dict(torch.load(dest_path + \"torch_model_3_23_19_lr=0.01_one_hot_dict.pt\"))\n",
    "model.double()     # cast model parameters to double\n",
    "model.eval()\n",
    "\n",
    "for i, (samples, labels) in enumerate(test_loader):\n",
    "    samples = Variable(samples)\n",
    "    labels = Variable(labels)\n",
    "    conf = model(samples)\n",
    "    conf = torch.flatten(conf)\n",
    "    labels = labels.type(torch.DoubleTensor)\n",
    "    \n",
    "    for j in range(conf.size()[0]):\n",
    "        if conf[j] < 0.5:\n",
    "            conf[j] = 0\n",
    "        else:\n",
    "            conf[j] = 1\n",
    "                \n",
    "    error = 1 - torch.sum(conf == labels).item() / labels.size()[0]\n",
    "    \n",
    "    print(\"Testing set Error: %0.4f\" % error)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Best Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = CLANet(input_size, hidden_size, output_size)\n",
    "model.load_state_dict(torch.load(dest_path + \"torch_model_3_23_19_lr=0.01_one_hot_dict.pt\"))\n",
    "model.double()     # cast model parameters to double\n",
    "model.eval()\n",
    "\n",
    "for i, (samples, labels) in enumerate(test_loader):\n",
    "    samples = Variable(samples)\n",
    "    labels = Variable(labels)\n",
    "    conf = model(samples)    # confidence that a certain instance is predicted correctly\n",
    "    conf = torch.flatten(conf)\n",
    "    labels = labels.type(torch.DoubleTensor)\n",
    "    \n",
    "# convert to numpy arrays\n",
    "conf = conf.detach().numpy()\n",
    "labels = labels.numpy()\n",
    "\n",
    "# sort arrays according to the predicted confidence (high confidence to low confidence)\n",
    "sort_idx = np.argsort(-conf, kind='mergesort')\n",
    "conf = conf[sort_idx]\n",
    "labels = labels[sort_idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ROC Curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_pos = 0\n",
    "num_neg = 0\n",
    "\n",
    "for label in labels:\n",
    "    if label < 0.5:\n",
    "        num_neg += 1\n",
    "    else:\n",
    "        num_pos += 1\n",
    "        \n",
    "tp = 0\n",
    "fp = 0\n",
    "last_tp = 0\n",
    "fpr = []\n",
    "tpr = []\n",
    "\n",
    "for i in range(len(labels)):\n",
    "    if (i > 1) and (conf[i] != conf[i-1]) and (labels[i] == 0) and (tp > last_tp):\n",
    "        fpr.append(fp / num_neg)\n",
    "        tpr.append(tp / num_pos)\n",
    "        last_tp = tp\n",
    "    if labels[i] == 1:\n",
    "        tp += 1\n",
    "    else:\n",
    "        fp += 1\n",
    "        \n",
    "fpr.append(fp / num_neg)\n",
    "tpr.append(tp / num_pos)\n",
    "\n",
    "plt.figure(figsize=(12, 8))\n",
    "plt.plot(fpr, tpr)\n",
    "plt.xlabel('False Postive Rate (FPR)', fontsize=20)\n",
    "plt.ylabel('True Positive Rate (TPR)', fontsize=20)\n",
    "plt.axis([0, 1, 0, 1.001])\n",
    "plt.title('ROC Curve', fontsize=20)\n",
    "plt.tight_layout()\n",
    "plt.grid(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PR Curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, (samples, labels) in enumerate(test_loader):\n",
    "    samples = Variable(samples)\n",
    "    labels = Variable(labels)\n",
    "    pred = model(samples)\n",
    "    pred = torch.flatten(pred)\n",
    "    labels = labels.type(torch.DoubleTensor)\n",
    "\n",
    "# convert to numpy arrays\n",
    "pred = pred.detach().numpy()\n",
    "labels = labels.numpy()\n",
    "\n",
    "# sort arrays according to the predicted confidence (high confidence to low confidence)\n",
    "sort_idx = np.argsort(-pred, kind='mergesort')\n",
    "pred = pred[sort_idx]\n",
    "labels = labels[sort_idx]\n",
    "\n",
    "num_pred_pos = 0\n",
    "num_actual_pos = 0\n",
    "num_tp = 0\n",
    "precision = []\n",
    "recall = []\n",
    "\n",
    "for confidence in conf:\n",
    "    for i in range(len(pred)):\n",
    "        if pred[i] >= confidence:\n",
    "            num_pred_pos += 1\n",
    "            \n",
    "            if labels[i] == 1:\n",
    "                num_tp += 1\n",
    "        \n",
    "        if labels[i] == 1:\n",
    "            num_actual_pos += 1\n",
    "\n",
    "    precision.append(num_tp / num_pred_pos)\n",
    "    recall.append(num_tp / num_actual_pos)\n",
    "    \n",
    "    num_pred_pos = 0\n",
    "    num_actual_pos = 0\n",
    "    num_tp = 0\n",
    "\n",
    "plt.figure(figsize=(12, 8))\n",
    "plt.plot(recall, precision)\n",
    "plt.xlabel('Recall', fontsize=20)\n",
    "plt.ylabel('Precision', fontsize=20)\n",
    "plt.axis([0, 1.001, 0.4, 1])\n",
    "plt.title('PR Curve', fontsize=20)\n",
    "plt.tight_layout()\n",
    "plt.grid(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test best model on other data sets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read in data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define data path\n",
    "data_path = \"/Users/Alliot/Documents/CLA-Project/Data/data-sets/\"\n",
    "data_set = \"data_2016_summer\"\n",
    "\n",
    "# load data sets\n",
    "X = np.load(data_path + data_set + \"_one_hot.npy\")\n",
    "y = np.load(data_path + data_set + \"_labels.npy\")\n",
    "\n",
    "# manipulate data set. labels are converted to 0, +1 for binary classification; samples are removed uniformly \n",
    "# from the data set so that the disproportionately large number of negative samples (no algae) does \n",
    "# not bias the model.\n",
    "\n",
    "num_alg = 0  # count the number of algae instances\n",
    "num_no_alg = 0  # count the number of no algae instances\n",
    "\n",
    "# Convert labels to binary: -1 for no algae and 1 for algae\n",
    "for i in range(0, len(y)):\n",
    "    if y[i] == 0:\n",
    "        num_no_alg += 1\n",
    "    if y[i] == 1 or y[i] == 2:\n",
    "        y[i] = 1\n",
    "        num_alg += 1\n",
    "\n",
    "# oversample the data set by randomly adding occurences of algae until the difference between the number of algae\n",
    "# samples and no algae samples equals sample_bias (defined below)\n",
    "idx = 0\n",
    "sample_bias = 0\n",
    "length_y = len(y)\n",
    "while num_alg != (num_no_alg + sample_bias):\n",
    "    # circle through the data sets until the difference of num_no_alg and num_alg equals\n",
    "    # the value specified by sample_bias\n",
    "    if idx == (length_y - 1):\n",
    "        idx = 0\n",
    "\n",
    "    if y[idx] == 1:\n",
    "        if np.random.rand() >= 0.5:  # add this sample with some probability\n",
    "            y = np.append(y, y[idx])\n",
    "            X = np.append(X, np.reshape(X[idx, :], newshape=(1, num_features)), axis=0)\n",
    "            num_alg += 1\n",
    "        else:\n",
    "            idx += 1\n",
    "    else:\n",
    "        idx += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Process and split data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# standardize data: remove the mean and variance in each sample\n",
    "num_splits = 2   # do not change\n",
    "sss = model_selection.StratifiedShuffleSplit(n_splits=num_splits, test_size=test_size)\n",
    "\n",
    "idx, _ = sss.split(X, y);\n",
    "train_idx = idx[0]\n",
    "test_idx = idx[1]\n",
    "X_train, X_test = X[train_idx], X[test_idx]\n",
    "y_train, y_test = y[train_idx], y[test_idx]\n",
    "\n",
    "X_train = preprocessing.scale(X_train, axis=1, with_mean=True, with_std=True)\n",
    "X_test = preprocessing.scale(X_test, axis=1, with_mean=True, with_std=True)\n",
    "\n",
    "# convert numpy arrays to pytorch tensors\n",
    "train_set_size = X_train.shape\n",
    "test_set_size = X_test.shape\n",
    "X_train, X_test = torch.from_numpy(X_train), torch.from_numpy(X_test)\n",
    "y_train, y_test = torch.from_numpy(y_train), torch.from_numpy(y_test)\n",
    "\n",
    "# convert pytorch tensors to pytorch TensorDataset\n",
    "train_set = utils.TensorDataset(X_train, y_train)\n",
    "test_set = utils.TensorDataset(X_test, y_test)\n",
    "\n",
    "# create DataLoaders\n",
    "train_loader = utils.DataLoader(train_set, batch_size=batch_size, shuffle=True)\n",
    "test_loader = utils.DataLoader(test_set, batch_size=test_set_size[0], shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate best model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, (samples, labels) in enumerate(test_loader):\n",
    "    samples = Variable(samples)\n",
    "    labels = Variable(labels)\n",
    "    conf = model(samples)\n",
    "    conf = torch.flatten(conf)\n",
    "    labels = labels.type(torch.DoubleTensor)\n",
    "    \n",
    "    for j in range(conf.size()[0]):\n",
    "        if conf[j] < 0.5:\n",
    "            conf[j] = 0\n",
    "        else:\n",
    "            conf[j] = 1\n",
    "                \n",
    "    error = 1 - torch.sum(conf == labels).item() / labels.size()[0]\n",
    "    \n",
    "    print(\"Testing set Error: %0.4f\" % error)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimizer parameter adjust"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "for p in optimizer.param_groups:\n",
    "    p[\"lr\"] = 0.01\n",
    "    p[\"momentum\"] = 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for p in optimizer.param_groups:\n",
    "    p[\"lr\"] = 0.003\n",
    "    p[\"momentum\"] = 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for p in optimizer.param_groups:\n",
    "    p[\"lr\"] = 0.001\n",
    "    p[\"momentum\"] = 0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for p in optimizer.param_groups:\n",
    "    p[\"lr\"] = 0.00001\n",
    "    p[\"momentum\"] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for p in optimizer.param_groups:\n",
    "    p[\"lr\"] = 0.0000005"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
