{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Network for CLA Project"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import statements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "from sklearn import model_selection\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.utils.data as utils\n",
    "from torch.autograd import Variable\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import errno\n",
    "import os\n",
    "import sys\n",
    "import Constants\n",
    "import copy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data processing\n",
    "sample_bias = 0     # adjust the difference in the number of the two types of samples (no algae vs algae)\n",
    "test_size = 0.2\n",
    "batch_size = 96    # batch size for the DataLoaders\n",
    "\n",
    "# NN model\n",
    "num_features = 21\n",
    "input_size = num_features     # size of input layer\n",
    "multiplier = 100         # multiplied by num_features to determine the size of each hidden layer\n",
    "hidden_size = multiplier * input_size\n",
    "output_size = 2\n",
    "learning_rate = 0.001         # learning rate of optimizer\n",
    "num_epochs = 100                # number of epochs\n",
    "\n",
    "# training the model\n",
    "use_previous_best_model = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read in data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.set_printoptions(threshold=np.inf)  # prints a full matrix rather than an abbreviated matrix\n",
    "\n",
    "# define data and destination paths\n",
    "data_path_hourly = '../Data/hourly-data-sets/'\n",
    "# data_path = \"/Users/Alliot/Documents/CLA-Project/Data/data-sets/\"\n",
    "# data_set_hourly = \"hourly_data_2017\"\n",
    "# data_set = \"all_data_summer\"\n",
    "# data_path = \"../Data/data/\"\n",
    "X_2015 = np.load(data_path_hourly + 'hourly_X_2015.npy')\n",
    "X_2016 = np.load(data_path_hourly + 'hourly_X_2016.npy')\n",
    "X_2017 = np.load(data_path_hourly + 'hourly_X_2017.npy')\n",
    "X_2018 = np.load(data_path_hourly + 'hourly_X_2018.npy')  # WHAT IS THE FIRST COLUMN??? WHERE IS THE ''??????\n",
    "\n",
    "y_2015 = np.load(data_path_hourly + 'hourly_y_2015.npy')\n",
    "y_2016 = np.load(data_path_hourly + 'hourly_y_2016.npy')\n",
    "y_2017 = np.load(data_path_hourly + 'hourly_y_2017.npy')\n",
    "y_2018 = np.load(data_path_hourly + 'hourly_y_2018.npy')\n",
    "\n",
    "# X = np.vstack((X_2015, X_2016, X_2017, X_2018)).astype(float)\n",
    "# y = np.hstack((y_2015, y_2016, y_2017, y_2018))\n",
    "X = X_2018.astype(float)\n",
    "y = y_2018\n",
    "# manipulate data set. labels are converted to 0, +1 for binary classification; samples are removed uniformly \n",
    "# from the data set so that the disproportionately large number of negative samples (no algae) does \n",
    "# not bias the model.\n",
    "\n",
    "num_alg = 0  # count the number of algae instances\n",
    "num_no_alg = 0  # count the number of no algae instances\n",
    "\n",
    "# Convert labels to binary: -1 for no algae and 1 for algae\n",
    "for i in range(0, len(y)):\n",
    "    if y[i] == 0:\n",
    "        num_no_alg += 1\n",
    "    if y[i] == 1 or y[i] == 2:\n",
    "        y[i] = 1\n",
    "        num_alg += 1\n",
    "\n",
    "# oversample the data set by randomly adding occurences of algae until the difference between the number of algae\n",
    "# samples and no algae samples equals sample_bias (defined below)\n",
    "# idx = 0\n",
    "# sample_bias = 0\n",
    "# length_y = len(y)\n",
    "# while num_alg != (num_no_alg + sample_bias):\n",
    "#     # circle through the data sets until the difference of num_no_alg and num_alg equals\n",
    "#     # the value specified by sample_bias\n",
    "#     if idx == (length_y - 1):\n",
    "#         idx = 0\n",
    "\n",
    "#     if y[idx] == 1:\n",
    "#         if np.random.rand() >= 0.5:  # add this sample with some probability\n",
    "#             y = np.append(y, y[idx])\n",
    "#             X = np.append(X, np.reshape(X[idx, :], newshape=(1, num_features)), axis=0)\n",
    "#             num_alg += 1\n",
    "#         else:\n",
    "#             idx += 1\n",
    "#     else:\n",
    "#         idx += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Process and split data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# standardize data: remove the mean and variance in each sample\n",
    "num_splits = 2   # do not change\n",
    "sss = model_selection.StratifiedShuffleSplit(n_splits=num_splits, test_size=test_size)\n",
    "\n",
    "idx, _ = sss.split(X, y)\n",
    "train_idx = idx[0]\n",
    "test_idx = idx[1]\n",
    "X_train, X_test = X[train_idx], X[test_idx]\n",
    "y_train, y_test = y[train_idx], y[test_idx]\n",
    "\n",
    "X_train = preprocessing.scale(X_train, axis=1, with_mean=True, with_std=True)\n",
    "X_test = preprocessing.scale(X_test, axis=1, with_mean=True, with_std=True)\n",
    "\n",
    "# convert numpy arrays to pytorch tensors\n",
    "train_set_size = X_train.shape\n",
    "test_set_size = X_test.shape\n",
    "X_train, X_test = torch.from_numpy(X_train), torch.from_numpy(X_test)\n",
    "y_train, y_test = torch.from_numpy(y_train), torch.from_numpy(y_test)\n",
    "\n",
    "# convert pytorch tensors to pytorch TensorDataset\n",
    "train_set = utils.TensorDataset(X_train, y_train)\n",
    "test_set = utils.TensorDataset(X_test, y_test)\n",
    "\n",
    "# create DataLoaders\n",
    "train_loader = utils.DataLoader(train_set, batch_size=batch_size, shuffle=True)\n",
    "test_loader = utils.DataLoader(test_set, batch_size=test_set_size[0], shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define neural network model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CLANet(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(CLANet, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(hidden_size, hidden_size)\n",
    "        self.tanh2 = nn.Tanh()\n",
    "        self.fc3 = nn.Linear(hidden_size, hidden_size)\n",
    "        self.relu3 = nn.ReLU()\n",
    "        self.fc4 = nn.Linear(hidden_size, hidden_size)\n",
    "        self.tanh4 = nn.Tanh()\n",
    "        self.fc5 = nn.Linear(hidden_size, hidden_size)\n",
    "        self.relu5 = nn.ReLU()\n",
    "        self.fc6 = nn.Linear(hidden_size, output_size)     # previously, this was output_size\n",
    "#         self.tanh6 = nn.Tanh()                             # previously, this was the line which was commented out\n",
    "#         self.fc7 = nn.Linear(hidden_size, output_size)\n",
    "#         self.relu7 = nn.ReLU()\n",
    "#         self.fc8 = nn.Linear(hidden_size, hidden_size)\n",
    "#         self.relu8 = nn.ReLU()\n",
    "#         self.fc9 = nn.Linear(hidden_size, output_size)\n",
    "#         self.relu9 = nn.ReLU()\n",
    "#         self.fc10 = nn.Linear(hidden_size, hidden_size)\n",
    "#         self.relu10 = nn.ReLU()\n",
    "#         self.fc11 = nn.Linear(hidden_size, hidden_size)\n",
    "#         self.relu11 = nn.ReLU()\n",
    "#         self.fc12 = nn.Linear(hidden_size, output_size)\n",
    "        self.sig1 = nn.Softmax(dim=1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        out = self.fc1(x)\n",
    "        out = self.relu1(out)\n",
    "        out = self.fc2(out)\n",
    "        out = self.tanh2(out)\n",
    "        out = self.fc3(out)\n",
    "        out = self.relu3(out)\n",
    "        out = self.fc4(out)\n",
    "        out = self.tanh4(out)\n",
    "        out = self.fc5(out)\n",
    "        out = self.relu5(out)\n",
    "        out = self.fc6(out)\n",
    "#         out = self.tanh6(out)\n",
    "#         out = self.fc7(out)\n",
    "#         out = self.relu7(out)\n",
    "#         out = self.fc8(out)\n",
    "#         out = self.relu8(out)\n",
    "#         out = self.fc9(out)\n",
    "#         out = self.relu9(out)\n",
    "#         out = self.fc10(out)\n",
    "#         out = self.relu10(out)\n",
    "#         out = self.fc11(out)\n",
    "#         out = self.relu11(out)\n",
    "#         out = self.fc12(out)\n",
    "        out = self.sig1(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Instantiate the neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# determine weight vector for the each class; used in the loss function\n",
    "num_pos = y.tolist().count(0)\n",
    "weight_pos = 10\n",
    "num_neg = y.tolist().count(1)\n",
    "weight_neg = 1\n",
    "weight = torch.tensor([weight_neg, weight_pos]).type(torch.DoubleTensor)\n",
    "\n",
    "# define model\n",
    "model = CLANet(input_size, hidden_size, output_size)\n",
    "criterion = nn.CrossEntropyLoss(weight=weight)\n",
    "\n",
    "# optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate, nesterov=True, momentum=1, dampening=0)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate, betas=(0.9, 0.999), eps=1e-8)\n",
    "model.double();     # cast model parameters to double"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/100\n",
      "  Iteration: 1/15, Loss: 0.692998, Error: 0.6458\n",
      "  Iteration: 2/15, Loss: 0.481, Error: 0.6667\n",
      "  Iteration: 3/15, Loss: 0.544031, Error: 0.7500\n",
      "  Iteration: 4/15, Loss: 0.486595, Error: 0.6771\n",
      "  Iteration: 5/15, Loss: 0.456119, Error: 0.6250\n",
      "  Iteration: 6/15, Loss: 0.508664, Error: 0.7083\n",
      "  Iteration: 7/15, Loss: 0.456119, Error: 0.6250\n",
      "  Iteration: 8/15, Loss: 0.516802, Error: 0.7188\n",
      "  Iteration: 9/15, Loss: 0.473567, Error: 0.6562\n",
      "  Iteration: 10/15, Loss: 0.486595, Error: 0.6771\n",
      "  Iteration: 11/15, Loss: 0.516802, Error: 0.7188\n",
      "  Iteration: 12/15, Loss: 0.534445, Error: 0.7396\n",
      "  Iteration: 13/15, Loss: 0.467491, Error: 0.6458\n",
      "  Iteration: 14/15, Loss: 0.525383, Error: 0.7292\n",
      "  Iteration: 15/15, Loss: 0.513262, Error: 0.7143\n",
      "Average Error for this Epoch: 0.6865\n",
      "found a better model!\n",
      "Epoch: 2/100\n",
      "  Iteration: 1/15, Loss: 0.554186, Error: 0.7604\n",
      "  Iteration: 2/15, Loss: 0.486595, Error: 0.6771\n",
      "  Iteration: 3/15, Loss: 0.57642, Error: 0.7812\n",
      "  Iteration: 4/15, Loss: 0.486595, Error: 0.6771\n",
      "  Iteration: 5/15, Loss: 0.500937, Error: 0.6979\n",
      "  Iteration: 6/15, Loss: 0.516802, Error: 0.7188\n",
      "  Iteration: 7/15, Loss: 0.508664, Error: 0.7083\n",
      "  Iteration: 8/15, Loss: 0.500937, Error: 0.6979\n",
      "  Iteration: 9/15, Loss: 0.508664, Error: 0.7083\n",
      "  Iteration: 10/15, Loss: 0.456119, Error: 0.6250\n",
      "  Iteration: 11/15, Loss: 0.554186, Error: 0.7604\n",
      "  Iteration: 12/15, Loss: 0.456119, Error: 0.6250\n",
      "  Iteration: 13/15, Loss: 0.486595, Error: 0.6771\n",
      "  Iteration: 14/15, Loss: 0.500937, Error: 0.6979\n",
      "  Iteration: 15/15, Loss: 0.441467, Error: 0.5952\n",
      "Average Error for this Epoch: 0.6938\n",
      "Epoch: 3/100\n",
      "  Iteration: 1/15, Loss: 0.467491, Error: 0.6458\n",
      "  Iteration: 2/15, Loss: 0.525383, Error: 0.7292\n",
      "  Iteration: 3/15, Loss: 0.456119, Error: 0.6250\n",
      "  Iteration: 4/15, Loss: 0.508664, Error: 0.7083\n",
      "  Iteration: 5/15, Loss: 0.525383, Error: 0.7292\n",
      "  Iteration: 6/15, Loss: 0.525383, Error: 0.7292\n",
      "  Iteration: 7/15, Loss: 0.49359, Error: 0.6875\n",
      "  Iteration: 8/15, Loss: 0.57642, Error: 0.7812\n",
      "  Iteration: 9/15, Loss: 0.467491, Error: 0.6458\n",
      "  Iteration: 10/15, Loss: 0.49359, Error: 0.6875\n",
      "  Iteration: 11/15, Loss: 0.554186, Error: 0.7604\n",
      "  Iteration: 12/15, Loss: 0.473567, Error: 0.6562\n",
      "  Iteration: 13/15, Loss: 0.49359, Error: 0.6875\n",
      "  Iteration: 14/15, Loss: 0.508664, Error: 0.7083\n",
      "  Iteration: 15/15, Loss: 0.459267, Error: 0.6310\n",
      "Average Error for this Epoch: 0.6941\n",
      "Epoch: 4/100\n",
      "  Iteration: 1/15, Loss: 0.467491, Error: 0.6458\n",
      "  Iteration: 2/15, Loss: 0.525383, Error: 0.7292\n",
      "  Iteration: 3/15, Loss: 0.456119, Error: 0.6250\n",
      "  Iteration: 4/15, Loss: 0.534445, Error: 0.7396\n",
      "  Iteration: 5/15, Loss: 0.508664, Error: 0.7083\n",
      "  Iteration: 6/15, Loss: 0.479928, Error: 0.6667\n",
      "  Iteration: 7/15, Loss: 0.508664, Error: 0.7083\n",
      "  Iteration: 8/15, Loss: 0.534445, Error: 0.7396\n",
      "  Iteration: 9/15, Loss: 0.508664, Error: 0.7083\n",
      "  Iteration: 10/15, Loss: 0.49359, Error: 0.6875\n",
      "  Iteration: 11/15, Loss: 0.544031, Error: 0.7500\n",
      "  Iteration: 12/15, Loss: 0.486595, Error: 0.6771\n",
      "  Iteration: 13/15, Loss: 0.486595, Error: 0.6771\n",
      "  Iteration: 14/15, Loss: 0.49359, Error: 0.6875\n",
      "  Iteration: 15/15, Loss: 0.479928, Error: 0.6667\n",
      "Average Error for this Epoch: 0.6944\n",
      "Epoch: 5/100\n",
      "  Iteration: 1/15, Loss: 0.49359, Error: 0.6875\n",
      "  Iteration: 2/15, Loss: 0.544031, Error: 0.7500\n",
      "  Iteration: 3/15, Loss: 0.534445, Error: 0.7396\n",
      "  Iteration: 4/15, Loss: 0.456119, Error: 0.6250\n",
      "  Iteration: 5/15, Loss: 0.500937, Error: 0.6979\n",
      "  Iteration: 6/15, Loss: 0.473567, Error: 0.6562\n",
      "  Iteration: 7/15, Loss: 0.554186, Error: 0.7604\n",
      "  Iteration: 8/15, Loss: 0.467491, Error: 0.6458\n",
      "  Iteration: 9/15, Loss: 0.479928, Error: 0.6667\n",
      "  Iteration: 10/15, Loss: 0.486595, Error: 0.6771\n",
      "  Iteration: 11/15, Loss: 0.564962, Error: 0.7708\n",
      "  Iteration: 12/15, Loss: 0.49359, Error: 0.6875\n",
      "  Iteration: 13/15, Loss: 0.49359, Error: 0.6875\n",
      "  Iteration: 14/15, Loss: 0.508664, Error: 0.7083\n",
      "  Iteration: 15/15, Loss: 0.472682, Error: 0.6548\n",
      "Average Error for this Epoch: 0.6943\n",
      "Epoch: 6/100\n",
      "  Iteration: 1/15, Loss: 0.456119, Error: 0.6250\n",
      "  Iteration: 2/15, Loss: 0.473567, Error: 0.6562\n",
      "  Iteration: 3/15, Loss: 0.516802, Error: 0.7188\n",
      "  Iteration: 4/15, Loss: 0.516802, Error: 0.7188\n",
      "  Iteration: 5/15, Loss: 0.473567, Error: 0.6562\n",
      "  Iteration: 6/15, Loss: 0.554186, Error: 0.7604\n",
      "  Iteration: 7/15, Loss: 0.479928, Error: 0.6667\n",
      "  Iteration: 8/15, Loss: 0.486595, Error: 0.6771\n",
      "  Iteration: 9/15, Loss: 0.486595, Error: 0.6771\n",
      "  Iteration: 10/15, Loss: 0.467491, Error: 0.6458\n",
      "  Iteration: 11/15, Loss: 0.486595, Error: 0.6771\n",
      "  Iteration: 12/15, Loss: 0.544031, Error: 0.7500\n",
      "  Iteration: 13/15, Loss: 0.508664, Error: 0.7083\n",
      "  Iteration: 14/15, Loss: 0.564962, Error: 0.7708\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-23-08aac20ea42b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     19\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m     \u001b[0;31m# clear gradient\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m           \u001b[0;31m# calculate gradients\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m          \u001b[0;31m# update weights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m         \u001b[0;31m# calculate and print error\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Applications/anaconda3/lib/python3.7/site-packages/torch/optim/adam.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m     92\u001b[0m                 \u001b[0;31m# Decay the first and second moment running average coefficient\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m                 \u001b[0mexp_avg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmul_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbeta1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mbeta1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 94\u001b[0;31m                 \u001b[0mexp_avg_sq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmul_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbeta2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maddcmul_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mbeta2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     95\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mamsgrad\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m                     \u001b[0;31m# Maintains the maximum of all 2nd moment running avg. till now\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model.train()     # training mode\n",
    "if not use_previous_best_model:\n",
    "    avg_error = 0\n",
    "    best_avg_error = 1\n",
    "\n",
    "avg_error_vec = []\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    print(\"Epoch: %d/%d\" % (epoch+1, num_epochs))\n",
    "\n",
    "    for i, (samples, labels) in enumerate(train_loader):\n",
    "        samples = Variable(samples)\n",
    "        labels = Variable(labels)\n",
    "        output = model(samples)                # forward pass\n",
    "#         output = torch.flatten(output)         # resize predicted labels\n",
    "        labels = labels.type(torch.long)\n",
    "        \n",
    "        loss = criterion(output, labels)  # calculate loss\n",
    "        optimizer.zero_grad()     # clear gradient\n",
    "        loss.backward()           # calculate gradients\n",
    "        optimizer.step()          # update weights\n",
    "        \n",
    "        # calculate and print error\n",
    "        out = output\n",
    "\n",
    "        out = torch.argmax(output, dim=1)   # convert output of network to labels for accuracy calculation\n",
    "        error = 1 - torch.sum(out == labels).item() / labels.size()[0]\n",
    "        avg_error += error\n",
    "\n",
    "        print(\"  Iteration: %d/%d, Loss: %g, Error: %0.4f\" % \n",
    "              (i+1, np.ceil(X_train.size()[0] / batch_size).astype(int), loss.item(), error))\n",
    "    \n",
    "    avg_error = avg_error / np.ceil(X_train.size()[0] / batch_size)\n",
    "    avg_error_vec.append(avg_error)\n",
    "    print(\"Average Error for this Epoch: %0.4f\" % avg_error)\n",
    "\n",
    "    if avg_error < best_avg_error:\n",
    "        print(\"found a better model!\")\n",
    "        best_avg_error = avg_error\n",
    "        best_model = copy.deepcopy(model)\n",
    "        use_previous_best_model = True\n",
    "    \n",
    "    avg_error = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.plot(np.linspace(start=1, stop=num_epochs, num=num_epochs), avg_error_vec)\n",
    "plt.xlabel(\"Number of Epochs\")\n",
    "plt.ylabel(\"Average Training Error\")\n",
    "plt.title(\"Average Training Error for Epochs=1:\" + str(num_epochs))\n",
    "plt.grid(True)\n",
    "# plt.savefig(\"Neural Net Training Error.jpg\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate model on testing set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing set Error: 0.6947\n"
     ]
    }
   ],
   "source": [
    "best_model.eval()\n",
    "conf = torch.tensor([])\n",
    "target = torch.tensor([])\n",
    "\n",
    "for i, (samples, labels) in enumerate(test_loader):\n",
    "    samples = Variable(samples)\n",
    "    labels = Variable(labels)\n",
    "    predictions = best_model(samples)\n",
    "#     predictions = torch.flatten(predictions)\n",
    "    labels = labels.type(torch.long)\n",
    "\n",
    "    predictions = torch.argmax(predictions, dim=1)   # convert output of network to labels for accuracy calculation\n",
    "    \n",
    "    error = 1 - torch.sum(predictions == labels).item() / labels.size()[0]\n",
    "    \n",
    "    print(\"Testing set Error: %0.4f\" % error)\n",
    "    \n",
    "    conf = torch.cat((conf, predictions.type(torch.FloatTensor)))\n",
    "    target = torch.cat((target, labels.type(torch.FloatTensor)))\n",
    "    \n",
    "# convert to numpy arrays  TODO: MAKE SURE THAT YOU USE CONF FROM EVERY PREDICTION\n",
    "conf = conf.detach().numpy()\n",
    "labels = labels.numpy()\n",
    "\n",
    "# sort arrays according to the predicted confidence (high confidence to low confidence)\n",
    "sort_idx = np.argsort(-conf, kind='mergesort')\n",
    "conf = conf[sort_idx]\n",
    "labels = labels[sort_idx]\n",
    "    \n",
    "# model_path = dest_path + \"torch_model_4_4_19_lr=\" + str(learning_rate) + \"_hourly_dict.pt\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load and Evaluate previous models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = CLANet(input_size, hidden_size, output_size)\n",
    "model.load_state_dict(torch.load(dest_path + \"torch_model_4_4_19_lr=0.001_hourly_dict.pt\"))\n",
    "model.double()     # cast model parameters to double\n",
    "model.eval()\n",
    "\n",
    "for i, (samples, labels) in enumerate(test_loader):\n",
    "    samples = Variable(samples)\n",
    "    labels = Variable(labels)\n",
    "    conf = model(samples)\n",
    "    conf = torch.flatten(conf)\n",
    "    labels = labels.type(torch.DoubleTensor)\n",
    "    \n",
    "    for j in range(conf.size()[0]):\n",
    "        if conf[j] < 0.5:\n",
    "            conf[j] = 0\n",
    "        else:\n",
    "            conf[j] = 1\n",
    "                \n",
    "    error = 1 - torch.sum(conf == labels).item() / labels.size()[0]\n",
    "    \n",
    "    print(\"Testing set Error: %0.4f\" % error)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Best Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = CLANet(input_size, hidden_size, output_size)\n",
    "model.load_state_dict(torch.load(dest_path + \"torch_model_4_4_19_lr=0.001_hourly_dict.pt\"))\n",
    "model.double()     # cast model parameters to double\n",
    "model.eval()\n",
    "\n",
    "for i, (samples, labels) in enumerate(test_loader):\n",
    "    samples = Variable(samples)\n",
    "    labels = Variable(labels)\n",
    "    conf = model(samples)    # confidence that a certain instance is predicted correctly\n",
    "    conf = torch.flatten(conf)\n",
    "    labels = labels.type(torch.DoubleTensor)\n",
    "    \n",
    "# convert to numpy arrays\n",
    "conf = conf.detach().numpy()\n",
    "labels = labels.numpy()\n",
    "\n",
    "# sort arrays according to the predicted confidence (high confidence to low confidence)\n",
    "sort_idx = np.argsort(-conf, kind='mergesort')\n",
    "conf = conf[sort_idx]\n",
    "labels = labels[sort_idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ROC Curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 864x576 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fpr, tpr, _ = roc_curve(labels, conf, pos_label=1)\n",
    "\n",
    "plt.figure(figsize=(12, 8))\n",
    "plt.plot(fpr, tpr)\n",
    "plt.xlabel('False Postive Rate (FPR)', fontsize=20)\n",
    "plt.ylabel('True Positive Rate (TPR)', fontsize=20)\n",
    "plt.axis([0, 1, 0, 1.001])\n",
    "plt.title('ROC Curve', fontsize=20)\n",
    "plt.tight_layout()\n",
    "plt.grid(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PR Curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, (samples, labels) in enumerate(test_loader):\n",
    "    samples = Variable(samples)\n",
    "    labels = Variable(labels)\n",
    "    pred = model(samples)\n",
    "    pred = torch.flatten(pred)\n",
    "    labels = labels.type(torch.DoubleTensor)\n",
    "\n",
    "# convert to numpy arrays\n",
    "pred = pred.detach().numpy()\n",
    "labels = labels.numpy()\n",
    "\n",
    "# sort arrays according to the predicted confidence (high confidence to low confidence)\n",
    "sort_idx = np.argsort(-pred, kind='mergesort')\n",
    "pred = pred[sort_idx]\n",
    "labels = labels[sort_idx]\n",
    "\n",
    "num_pred_pos = 0\n",
    "num_actual_pos = 0\n",
    "num_tp = 0\n",
    "precision = []\n",
    "recall = []\n",
    "\n",
    "for confidence in conf:\n",
    "    for i in range(len(pred)):\n",
    "        if pred[i] >= confidence:\n",
    "            num_pred_pos += 1\n",
    "            \n",
    "            if labels[i] == 1:\n",
    "                num_tp += 1\n",
    "        \n",
    "        if labels[i] == 1:\n",
    "            num_actual_pos += 1\n",
    "\n",
    "    precision.append(num_tp / num_pred_pos)\n",
    "    recall.append(num_tp / num_actual_pos)\n",
    "    \n",
    "    num_pred_pos = 0\n",
    "    num_actual_pos = 0\n",
    "    num_tp = 0\n",
    "\n",
    "plt.figure(figsize=(12, 8))\n",
    "plt.plot(recall, precision)\n",
    "plt.xlabel('Recall', fontsize=20)\n",
    "plt.ylabel('Precision', fontsize=20)\n",
    "plt.axis([0, 1.001, 0.4, 1])\n",
    "plt.title('PR Curve', fontsize=20)\n",
    "plt.tight_layout()\n",
    "plt.grid(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test best model on other data sets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read in data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define data path\n",
    "data_path = \"/Users/Alliot/Documents/CLA-Project/Data/hourly-data-sets/\"\n",
    "data_set = \"hourly_data_2014\"\n",
    "\n",
    "# load data sets\n",
    "X = np.load(data_path + data_set + \".npy\")\n",
    "y = np.load(data_path + data_set + \"_labels.npy\")\n",
    "\n",
    "# manipulate data set. labels are converted to 0, +1 for binary classification; samples are removed uniformly \n",
    "# from the data set so that the disproportionately large number of negative samples (no algae) does \n",
    "# not bias the model.\n",
    "\n",
    "num_alg = 0  # count the number of algae instances\n",
    "num_no_alg = 0  # count the number of no algae instances\n",
    "\n",
    "# Convert labels to binary: -1 for no algae and 1 for algae\n",
    "for i in range(0, len(y)):\n",
    "    if y[i] == 0:\n",
    "        num_no_alg += 1\n",
    "    if y[i] == 1 or y[i] == 2:\n",
    "        y[i] = 1\n",
    "        num_alg += 1\n",
    "\n",
    "# oversample the data set by randomly adding occurences of algae until the difference between the number of algae\n",
    "# samples and no algae samples equals sample_bias (defined below)\n",
    "idx = 0\n",
    "sample_bias = 0\n",
    "length_y = len(y)\n",
    "while num_alg != (num_no_alg + sample_bias):\n",
    "    # circle through the data sets until the difference of num_no_alg and num_alg equals\n",
    "    # the value specified by sample_bias\n",
    "    if idx == (length_y - 1):\n",
    "        idx = 0\n",
    "\n",
    "    if y[idx] == 1:\n",
    "        if np.random.rand() >= 0.5:  # add this sample with some probability\n",
    "            y = np.append(y, y[idx])\n",
    "            X = np.append(X, np.reshape(X[idx, :], newshape=(1, num_features)), axis=0)\n",
    "            num_alg += 1\n",
    "        else:\n",
    "            idx += 1\n",
    "    else:\n",
    "        idx += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Process and split data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# standardize data: remove the mean and variance in each sample\n",
    "num_splits = 2   # do not change\n",
    "sss = model_selection.StratifiedShuffleSplit(n_splits=num_splits, test_size=test_size)\n",
    "\n",
    "idx, _ = sss.split(X, y);\n",
    "train_idx = idx[0]\n",
    "test_idx = idx[1]\n",
    "X_train, X_test = X[train_idx], X[test_idx]\n",
    "y_train, y_test = y[train_idx], y[test_idx]\n",
    "\n",
    "X_train = preprocessing.scale(X_train, axis=1, with_mean=True, with_std=True)\n",
    "X_test = preprocessing.scale(X_test, axis=1, with_mean=True, with_std=True)\n",
    "\n",
    "# convert numpy arrays to pytorch tensors\n",
    "train_set_size = X_train.shape\n",
    "test_set_size = X_test.shape\n",
    "X_train, X_test = torch.from_numpy(X_train), torch.from_numpy(X_test)\n",
    "y_train, y_test = torch.from_numpy(y_train), torch.from_numpy(y_test)\n",
    "\n",
    "# convert pytorch tensors to pytorch TensorDataset\n",
    "train_set = utils.TensorDataset(X_train, y_train)\n",
    "test_set = utils.TensorDataset(X_test, y_test)\n",
    "\n",
    "# create DataLoaders\n",
    "train_loader = utils.DataLoader(train_set, batch_size=batch_size, shuffle=True)\n",
    "test_loader = utils.DataLoader(test_set, batch_size=test_set_size[0], shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate best model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, (samples, labels) in enumerate(test_loader):\n",
    "    samples = Variable(samples)\n",
    "    labels = Variable(labels)\n",
    "    conf = model(samples)\n",
    "    conf = torch.flatten(conf)\n",
    "    labels = labels.type(torch.DoubleTensor)\n",
    "    \n",
    "    for j in range(conf.size()[0]):\n",
    "        if conf[j] < 0.5:\n",
    "            conf[j] = 0\n",
    "        else:\n",
    "            conf[j] = 1\n",
    "                \n",
    "    error = 1 - torch.sum(conf == labels).item() / labels.size()[0]\n",
    "    \n",
    "    print(\"Testing set Error: %0.4f\" % error)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimizer parameter adjust"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for p in optimizer.param_groups:\n",
    "    p[\"lr\"] = 0.003\n",
    "    p[\"momentum\"] = 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for p in optimizer.param_groups:\n",
    "    p[\"lr\"] = 0.001\n",
    "    p[\"momentum\"] = 0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for p in optimizer.param_groups:\n",
    "    p[\"lr\"] = 0.00001\n",
    "    p[\"momentum\"] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for p in optimizer.param_groups:\n",
    "    p[\"lr\"] = 0.0000005"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for p in optimizer.param_groups:\n",
    "    p[\"lr\"] = 0.0003\n",
    "    p[\"momentum\"] = 0.5"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
