{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Statements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import StratifiedShuffleSplit, GridSearchCV, train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import (f1_score, roc_auc_score, confusion_matrix, roc_curve, precision_recall_curve, \n",
    "                             accuracy_score, balanced_accuracy_score)\n",
    "from sklearn.naive_bayes import GaussianNB, MultinomialNB\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "from matplotlib import rcParams\n",
    "rcParams['font.family'] = 'sans-serif'\n",
    "rcParams['font.sans-serif'] = ['Tahoma']\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "from pandas.plotting import register_matplotlib_converters\n",
    "register_matplotlib_converters()\n",
    "\n",
    "import seaborn as sns\n",
    "\n",
    "pd.options.mode.chained_assignment = None\n",
    "# np.random.seed(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read in Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = '../Data/'\n",
    "\n",
    "filenames = [\n",
    "#     'CM2014_edit.csv',\n",
    "    'CM2015_edit.csv',\n",
    "    'CM2016_edit.csv',\n",
    "    'CM2017_edit.csv',\n",
    "    'CM2018_edit.csv',\n",
    "    'mdcp.csv',\n",
    "    'major_ion.csv',\n",
    "    'Weather_Data.csv'\n",
    "]\n",
    "\n",
    "# cla_2014 = pd.read_csv(data_path + filenames[0], low_memory=False)\n",
    "cla_2015_raw = pd.read_csv(data_path + filenames[0], low_memory=False)\n",
    "cla_2016_raw = pd.read_csv(data_path + filenames[1], low_memory=False)\n",
    "cla_2017_raw = pd.read_csv(data_path + filenames[2], low_memory=False)\n",
    "cla_2018_raw = pd.read_csv(data_path + filenames[3], low_memory=False)\n",
    "mdcp_raw = pd.read_csv(data_path + filenames[4], low_memory=False)    # Mendota buoy\n",
    "weather_raw = pd.read_csv(data_path + filenames[6], error_bad_lines=False, low_memory=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CLA Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "keep15 = [     # features to keep for years 2015-2017\n",
    "    'correct_timestamp',\n",
    "    'collectionSiteId',\n",
    "    'lake',\n",
    "    'algalBloom',\n",
    "    'algalBloomSheen',\n",
    "    'turbidity',\n",
    "#     'waterTemp',\n",
    "#     'waveIntensity',\n",
    "    'lat',\n",
    "    'long'\n",
    "]\n",
    "\n",
    "keep18 = [    # features to keep for 2018\n",
    "    'sample_collection_time',\n",
    "    'collectionSiteId',\n",
    "    'lake',\n",
    "    'algalBloom',\n",
    "    'algalBloomSheen',\n",
    "    'turbidity',\n",
    "#     'waterTemp',\n",
    "#     'waveIntensity',\n",
    "    'latitiude',\n",
    "    'longitude'\n",
    "]\n",
    "\n",
    "rename15 = {   # rename features for 2015-2017\n",
    "    'collectionSiteId': 'site',\n",
    "    'lat': 'latitude',\n",
    "    'long': 'longitude',\n",
    "    'correct_timestamp': 'date'\n",
    "}\n",
    "\n",
    "rename18 = {   # renamce features for 2018\n",
    "    'collectionSiteId': 'site',\n",
    "    'sample_collection_time': 'date',\n",
    "    'latitiude': 'latitude'\n",
    "}\n",
    "\n",
    "cla_2015 = cla_2015_raw[keep15]\n",
    "cla_2016 = cla_2016_raw[keep15]\n",
    "cla_2017 = cla_2017_raw[keep15]\n",
    "cla_2018 = cla_2018_raw[keep18]\n",
    "\n",
    "cla_2015.rename(rename15, axis='columns', inplace=True)\n",
    "cla_2016.rename(rename15, axis='columns', inplace=True)\n",
    "cla_2017.rename(rename15, axis='columns', inplace=True)\n",
    "cla_2018.rename(rename18, axis='columns', inplace=True)\n",
    "\n",
    "# change data types\n",
    "numeric = [    # list of numeric features\n",
    "    'algalBloom',\n",
    "    'algalBloomSheen',\n",
    "    'turbidity',\n",
    "#     'waterTemp',\n",
    "#     'waveIntensity',\n",
    "    'latitude',\n",
    "    'longitude'\n",
    "]\n",
    "\n",
    "# convert data types\n",
    "features = cla_2015.columns.values\n",
    "\n",
    "for feat in features:\n",
    "    if feat in numeric:\n",
    "        cla_2015[feat] = pd.to_numeric(cla_2015[feat], errors='coerce')\n",
    "        cla_2016[feat] = pd.to_numeric(cla_2016[feat], errors='coerce')\n",
    "        cla_2017[feat] = pd.to_numeric(cla_2017[feat], errors='coerce')\n",
    "        cla_2018[feat] = pd.to_numeric(cla_2018[feat], errors='coerce')\n",
    "    \n",
    "    if feat in ['site', 'lake']:\n",
    "        cla_2015[feat] = cla_2015[feat].astype(str)\n",
    "        cla_2016[feat] = cla_2016[feat].astype(str)\n",
    "        cla_2017[feat] = cla_2017[feat].astype(str)\n",
    "        cla_2018[feat] = cla_2018[feat].astype(str)\n",
    "    \n",
    "    if feat == 'date':\n",
    "        cla_2015[feat] = pd.to_datetime(cla_2015[feat], errors='coerce')\n",
    "        cla_2016[feat] = pd.to_datetime(cla_2016[feat], errors='coerce')\n",
    "        cla_2017[feat] = pd.to_datetime(cla_2017[feat], errors='coerce')\n",
    "        cla_2018[feat] = pd.to_datetime(cla_2018[feat], errors='coerce')\n",
    "        \n",
    "# remove nans\n",
    "cla_2015.dropna(axis='rows', how='any', inplace=True)\n",
    "cla_2016.dropna(axis='rows', how='any', inplace=True)\n",
    "cla_2017.dropna(axis='rows', how='any', inplace=True)\n",
    "cla_2018.dropna(axis='rows', how='any', inplace=True)\n",
    "\n",
    "# remove any data point not on lake mendota\n",
    "cla_2015 = cla_2015[cla_2015['lake'].str.contains('Mendota')]\n",
    "cla_2016 = cla_2016[cla_2016['lake'].str.contains('Mendota')]\n",
    "cla_2017 = cla_2017[cla_2017['lake'].str.contains('Mendota')]\n",
    "cla_2018 = cla_2018[cla_2018['lake'].str.contains('Mendota')]\n",
    "\n",
    "# set date as index\n",
    "cla_2015.set_index('date', inplace=True)\n",
    "cla_2016.set_index('date', inplace=True)\n",
    "cla_2017.set_index('date', inplace=True)\n",
    "cla_2018.set_index('date', inplace=True)\n",
    "\n",
    "# sort data by dates\n",
    "cla_2015.sort_values(by='date', inplace=True)\n",
    "cla_2016.sort_values(by='date', inplace=True)\n",
    "cla_2017.sort_values(by='date', inplace=True)\n",
    "cla_2018.sort_values(by='date', inplace=True)\n",
    "\n",
    "# resample, ffill and bfill\n",
    "cla_2015 = cla_2015.resample('D').mean()\n",
    "cla_2015.ffill(inplace=True)\n",
    "cla_2015.bfill(inplace=True)\n",
    "\n",
    "for date in cla_2015.index:\n",
    "    if cla_2015.loc[date, 'algalBloomSheen'] > 0:\n",
    "        cla_2015.loc[date, 'algalBloomSheen'] = 1\n",
    "\n",
    "cla_2016 = cla_2016.resample('D').mean()\n",
    "cla_2016.ffill(inplace=True)\n",
    "cla_2016.bfill(inplace=True)\n",
    "\n",
    "for date in cla_2016.index:\n",
    "    if cla_2016.loc[date, 'algalBloomSheen'] > 0:\n",
    "        cla_2016.loc[date, 'algalBloomSheen'] = 1\n",
    "\n",
    "cla_2017 = cla_2017.resample('D').mean()\n",
    "cla_2017.ffill(inplace=True)\n",
    "cla_2017.bfill(inplace=True)\n",
    "\n",
    "for date in cla_2017.index:\n",
    "    if cla_2017.loc[date, 'algalBloomSheen'] > 0:\n",
    "        cla_2017.loc[date, 'algalBloomSheen'] = 1\n",
    "\n",
    "cla_2018 = cla_2018.resample('D').mean()\n",
    "cla_2018.ffill(inplace=True)\n",
    "cla_2018.bfill(inplace=True)\n",
    "\n",
    "for date in cla_2018.index:\n",
    "    if cla_2018.loc[date, 'algalBloomSheen'] > 0:\n",
    "        cla_2018.loc[date, 'algalBloomSheen'] = 1\n",
    "        \n",
    "# only keep the dates of the official sampling season of each year\n",
    "# cla_2015 = cla_2015[(cla_2015.index >= '2015-5-18') & (cla_2015.index <= '2015-9-4')]\n",
    "# cla_2016 = cla_2016[(cla_2016.index >= '2016-5-25') & (cla_2016.index <= '2016-9-4')]\n",
    "# cla_2017 = cla_2017[(cla_2017.index >= '2017-5-25') & (cla_2017.index <= '2017-9-4')]\n",
    "# cla_2018 = cla_2018[(cla_2018.index >= '2018-5-25') & (cla_2018.index <= '2018-9-4')]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MDCP Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "keep_mdcp = [\n",
    "    'sampledate',\n",
    "    'sampletime',\n",
    "    'air_temp',\n",
    "    'rel_hum',\n",
    "    'wind_speed',\n",
    "    'wind_dir',\n",
    "    'chlor',\n",
    "    'phycocyanin',\n",
    "    'do_raw',\n",
    "    'do_sat',\n",
    "    'do_wtemp',\n",
    "    'pco2_ppm',\n",
    "    'par',\n",
    "    'par_below'\n",
    "]\n",
    "\n",
    "mdcp = mdcp_raw[keep_mdcp]\n",
    "mdcp.ffill(inplace=True)\n",
    "mdcp.bfill(inplace=True)\n",
    "\n",
    "mdcp['date'] = mdcp['sampledate'] + ' ' + mdcp['sampletime']\n",
    "mdcp['date'] = pd.to_datetime(mdcp['date'], errors='coerce')\n",
    "mdcp.dropna(axis='rows', how='any', inplace=True)\n",
    "\n",
    "mdcp = mdcp[[\n",
    "    'date',\n",
    "    'air_temp',\n",
    "    'rel_hum',\n",
    "    'wind_speed',\n",
    "    'wind_dir',\n",
    "    'chlor', \n",
    "    'phycocyanin',\n",
    "    'do_raw',\n",
    "    'do_sat',\n",
    "    'do_wtemp',\n",
    "    'pco2_ppm',\n",
    "    'par',\n",
    "    'par_below'\n",
    "]]\n",
    "mdcp.set_index('date', inplace=True)\n",
    "\n",
    "mdcp = mdcp.resample('D').mean()\n",
    "mdcp.ffill(inplace=True)\n",
    "mdcp.bfill(inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Weather Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "keep_weather = [\n",
    "    'DATE',\n",
    "    'REPORTTPYE',\n",
    "    'DAILYMaximumDryBulbTemp',\n",
    "    'DAILYMinimumDryBulbTemp',\n",
    "    'DAILYAverageDryBulbTemp',\n",
    "    'DAILYDeptFromNormalAverageTemp',\n",
    "    'DAILYAverageDewPointTemp',\n",
    "    'DAILYAverageWetBulbTemp',\n",
    "    'DAILYPrecip',\n",
    "    'DAILYAverageStationPressure',\n",
    "    'DAILYAverageSeaLevelPressure'\n",
    "]\n",
    "\n",
    "weather = weather_raw[keep_weather]\n",
    "# weather['REPORTTPYE'].dropna(axis='rows', how='any', inplace=True)\n",
    "weather = weather.iloc[:-1]  # remove last entry since it has NaN in REPORTTPYE\n",
    "\n",
    "weather = weather[weather['REPORTTPYE'].str.contains('SOD')]    # only keep summary of day (SOD) info\n",
    "weather = weather.drop(['REPORTTPYE'], axis='columns')\n",
    "weather['DATE'] = pd.to_datetime(weather['DATE'], errors='coerce')\n",
    "\n",
    "weather.set_index('DATE', inplace=True)\n",
    "weather = weather.resample('D').ffill()\n",
    "weather.ffill(inplace=True)\n",
    "weather.bfill(inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Join CLA, MDCP, and Weather Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Append CLA data\n",
    "cla = cla_2015.append(cla_2016)\n",
    "cla = cla.append(cla_2017)\n",
    "cla = cla.append(cla_2018)\n",
    "\n",
    "# Insert MDCP data\n",
    "data = cla.join(mdcp, how='inner')\n",
    "\n",
    "# Insert weather data\n",
    "data = data.join(weather, how='inner')\n",
    "\n",
    "# sine/cosine transformation of month of year and wind direction\n",
    "data['cos_month'] = np.cos(2 * np.pi * (data.index.month.values / 12))\n",
    "data['sin_month'] = np.sin(2 * np.pi * (data.index.month.values / 12))\n",
    "\n",
    "data['cos_wind_dir'] = np.cos(2 * np.pi * (data['wind_dir'] / 12))\n",
    "data['sin_wind_dir'] = np.sin(2 * np.pi * (data['wind_dir'] / 12))\n",
    "data = data.drop(['wind_dir'], axis='columns')\n",
    "\n",
    "# Replace 'T' and 's' in 'DAILYPrecip' column\n",
    "for date in data.index:\n",
    "    if 'T' in data.loc[date, 'DAILYPrecip']:\n",
    "        data.loc[date, 'DAILYPrecip'] = 0.01\n",
    "    elif 's' in data.loc[date, 'DAILYPrecip']:\n",
    "        data.loc[date, 'DAILYPrecip'] = 0\n",
    "\n",
    "# Make every feature numeric\n",
    "for col in data.columns.values:\n",
    "    if type(data[col].values[0]) != np.float64:\n",
    "        data[col] = pd.to_numeric(data[col], errors='coerce')\n",
    "        \n",
    "# create indicator features for whether there was rain or a bloom one day ago, or within three days or a week ago\n",
    "precip = (data['DAILYPrecip'] > 0).astype(int)   # convert precipitation to boolean values\n",
    "# data['DAILYPrecip_one_day'] = precip.shift(1)\n",
    "# data['DAILYPrecip_three_day'] = precip.rolling(window=3, min_periods=1).sum()    # NOTE THAT THIS IS DEPENDENT ON CURRENT VALUE\n",
    "# data['DAILYPrecip_one_week'] = precip.rolling(window=7, min_periods=1).sum()\n",
    "\n",
    "# data['algalBloomSheen_one_day'] = data['algalBloomSheen'].shift(1)\n",
    "# data['algalBloomSheen_three_day'] = data[['algalBloomSheen']].shift(1).rolling(3).sum()\n",
    "# data['algalBloomSheen_one_week'] = data[['algalBloomSheen']].shift(1).rolling(7).sum()\n",
    "\n",
    "# shift algalbloomsheen by -1 to predict next day algal bloom\n",
    "data['DAILYPrecip_one_day'] = precip\n",
    "data['DAILYPrecip_three_day'] = precip.rolling(window=3, min_periods=1).sum()    # NOTE THAT THIS IS DEPENDENT ON CURRENT VALUE\n",
    "data['DAILYPrecip_one_week'] = precip.rolling(window=7, min_periods=1).sum()\n",
    "data['algalBloomSheen_one_day'] = data['algalBloomSheen']\n",
    "data['algalBloomSheen_three_day'] = data[['algalBloomSheen']].rolling(3, min_periods=1).sum()\n",
    "data['algalBloomSheen_one_week'] = data[['algalBloomSheen']].rolling(7, min_periods=1).sum()\n",
    "data['algalBloomSheen'] = data['algalBloomSheen'].shift(-1)\n",
    "\n",
    "data.dropna(axis='rows', how='any', inplace=True)\n",
    "\n",
    "# display(data[['DAILYPrecip',\n",
    "#       'DAILYPrecip_one_day',\n",
    "#       'DAILYPrecip_three_day',\n",
    "#       'DAILYPrecip_one_week',\n",
    "#       'algalBloomSheen',\n",
    "#       'algalBloomSheen_one_day',\n",
    "#       'algalBloomSheen_three_day',\n",
    "#       'algalBloomSheen_one_week'\n",
    "#      ]].head(15))\n",
    "\n",
    "labels = data[['algalBloomSheen']]\n",
    "data = data.drop(['latitude', 'longitude', 'algalBloom', 'algalBloomSheen'], axis='columns')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "log = LogisticRegression(    # gives more false positives, but not so many false negatives!!!!!\n",
    "    penalty='l1',              # this model might be better without weather data actually\n",
    "    tol=0.0001,\n",
    "    C=1,\n",
    "    fit_intercept=True,\n",
    "    class_weight='balanced',\n",
    "    solver='liblinear'\n",
    ")\n",
    "\n",
    "svc = SVC(                     # gives even slightly fewer false negatives than logic\n",
    "    C=1,\n",
    "    kernel='poly',\n",
    "    degree=6,\n",
    "    gamma='auto',\n",
    "    coef0=1,\n",
    "    probability=True,\n",
    "    tol=0.001,\n",
    "    class_weight='balanced',\n",
    "    max_iter=10000\n",
    ")\n",
    "\n",
    "rfc = RandomForestClassifier(       # BEST MODEL\n",
    "    n_estimators=1000,\n",
    "    max_depth=4,\n",
    "    criterion='gini',\n",
    "    bootstrap=True,\n",
    "    class_weight='balanced'\n",
    ")\n",
    "\n",
    "mlp = MLPClassifier(\n",
    "    hidden_layer_sizes=(20, 50, 50, 20),\n",
    "    activation='relu',\n",
    "    solver='adam',\n",
    "    alpha=0.0001,\n",
    "    batch_size=8,\n",
    "    learning_rate='adaptive',\n",
    "    learning_rate_init=0.001,\n",
    "    power_t=0.5,\n",
    "    max_iter=200,\n",
    "    shuffle=True,\n",
    "    tol=0.0001,\n",
    "    momentum=0.9,\n",
    "    nesterovs_momentum=True,\n",
    "    early_stopping=False,\n",
    "    validation_fraction=0.1,\n",
    "    beta_1=0.9,\n",
    "    beta_2=0.999,\n",
    "    epsilon=1e-8,\n",
    "    n_iter_no_change=10\n",
    ")\n",
    "\n",
    "########################################\n",
    "########################################\n",
    "########################################\n",
    "# TODO TRY PARAMETER TUNING THE SVC, WILL POLY SVC WORK??                       -- DONE!!\n",
    "# TODO TRY CHANGING ROLLING AND SHIFTED BOOLEAN PRECIP DATA TO ACTUAL AMOUNTS   -- DONE!!\n",
    "# TODO TRY LOG TRANSFORMS AND MOVING AVERAGE AND STD ON SOME FEATURES\n",
    "# TODO CROSS VALIDATED TRAINING CURVE                                           -- NOT APPLICABLE!!\n",
    "# TODO TRY TRAINING ON ALL DATA BEFORE 2018, AND TEST ON DATA AFTER             -- DONE!!\n",
    "# TODO TRY USING DATA ONLY FROM SAMPLING SEASON                                 -- DONE!!\n",
    "# TODO TRY USING 2014 DATA                                                      -- DONE!!\n",
    "# TODO ALSO TRY PARAMETER TUNING ON LOGISTIC REGRESSION                         -- DONE!! \n",
    "# TODO TRY ONLY OPTIMIZING F1 SCORE WHEN PARAMETER TUNING                       -- DONE!!\n",
    "# TODO TRY FORECASTING MODEL                                                    -- DONE!!\n",
    "# TODO CHECK CORRELATION AMONG VARIABLES                                        -- DONE!!\n",
    "# TODO TRY REMOVING REDUNDANT FEATURES. THE PROBLEM IS THAT PREDICTIONS ARE\n",
    "#    MADE ON AVERAGE DAILIES, WHICH IS SORT OF LIKE USING FUTURE DATA. TRY\n",
    "#    ONLY USING BINARY OR DISCRETE FEATURES FOR DAILY AVERAGES, ETC DID IT\n",
    "#    RAIN OR NOT?\n",
    "# TODO HYPERPARAMETER TUNE MODELS FOR PREDICTING NEXT DAY \n",
    "########################################\n",
    "########################################\n",
    "########################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GridSearchCV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'class_weight': 'balanced_subsample', 'criterion': 'gini', 'max_depth': 2, 'n_estimators': 20}\n"
     ]
    }
   ],
   "source": [
    "rfc_params = {\n",
    "    'n_estimators': [10, 20, 50, 100, 200, 500, 1000, 2000],\n",
    "    'criterion': ('gini', 'entropy'),\n",
    "    'max_depth': [2, 3, 4, 5, 6, 7],\n",
    "    'class_weight': ('balanced', 'balanced_subsample')\n",
    "}\n",
    "\n",
    "rfc_grid = GridSearchCV(\n",
    "    estimator=rfc,\n",
    "    param_grid=rfc_params,\n",
    "    scoring='balanced_accuracy',    # or f1\n",
    "    iid=False,\n",
    "    n_jobs=3,\n",
    "    cv=5\n",
    ")\n",
    "\n",
    "scaler = StandardScaler()\n",
    "data_vals = scaler.fit_transform(data.values)\n",
    "\n",
    "rfc_grid.fit(data_vals, labels.values.ravel())\n",
    "\n",
    "print(rfc_grid.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7696\n",
      "Balanced Accuracy: 0.7696\n",
      "F1 Score: 0.7027\n",
      "\n",
      "Confusion Matrix:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>95</td>\n",
       "      <td>33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>11</td>\n",
       "      <td>52</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    0   1\n",
       "0  95  33\n",
       "1  11  52"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    data.values,\n",
    "    labels.values.ravel(),\n",
    "    train_size=0.7,\n",
    "    test_size=0.3,\n",
    "    shuffle=True,\n",
    "    stratify=labels.values.ravel()\n",
    ")\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "model = rfc_grid.best_estimator_\n",
    "\n",
    "model.fit(X_train, y_train)\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "acc = accuracy_score(y_test, y_pred)\n",
    "balanced_acc = accuracy_score(y_test, y_pred)\n",
    "f1 = f1_score(y_test, y_pred)\n",
    "conf_matrix = pd.DataFrame(confusion_matrix(y_test, y_pred))\n",
    "\n",
    "print('Accuracy: %0.4f' % acc)\n",
    "print('Balanced Accuracy: %0.4f' % balanced_acc)\n",
    "print('F1 Score: %0.4f' % f1)\n",
    "print('\\nConfusion Matrix:')\n",
    "display(conf_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run 1\n",
    "{'class_weight': 'balanced', 'criterion': 'gini', 'max_depth': 2, 'n_estimators': 10}\n",
    "\n",
    "Accuracy: 0.7249\n",
    "Balanced Accuracy: 0.7249\n",
    "F1 Score: 0.6790\n",
    "\n",
    "Confusion Matrix:\n",
    "\n",
    "  0  1\n",
    "0 82 44\n",
    "1 8  55\n",
    "\n",
    "Run 2\n",
    "{'class_weight': 'balanced_subsample', 'criterion': 'gini', 'max_depth': 2, 'n_estimators': 20}\n",
    "\n",
    "Accuracy: 0.7853\n",
    "Balanced Accuracy: 0.7853\n",
    "F1 Score: 0.7172\n",
    "\n",
    "Confusion Matrix:\n",
    "    0\t1\n",
    "0\t98\t30\n",
    "1\t11\t52"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'C': 1, 'penalty': 'l2'}\n"
     ]
    }
   ],
   "source": [
    "log_params = {\n",
    "    'penalty': ('l1', 'l2'),\n",
    "    'C': [1, 2, 5, 10, 20, 50, 100, 200, 500, 1000, 2000],\n",
    "}\n",
    "\n",
    "log_grid = GridSearchCV(\n",
    "    estimator=log,\n",
    "    param_grid=log_params,\n",
    "    scoring='balanced_accuracy',    # or f1\n",
    "    iid=False,\n",
    "    n_jobs=3,\n",
    "    cv=5\n",
    ")\n",
    "\n",
    "scaler = StandardScaler()\n",
    "data_vals = scaler.fit_transform(data.values)\n",
    "\n",
    "log_grid.fit(data_vals, labels.values.ravel())\n",
    "\n",
    "print(log_grid.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7853\n",
      "Balanced Accuracy: 0.7853\n",
      "F1 Score: 0.6963\n",
      "\n",
      "Confusion Matrix:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>103</td>\n",
       "      <td>25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>16</td>\n",
       "      <td>47</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     0   1\n",
       "0  103  25\n",
       "1   16  47"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    data.values,\n",
    "    labels.values.ravel(),\n",
    "    train_size=0.7,\n",
    "    test_size=0.3,\n",
    "    shuffle=True,\n",
    "    stratify=labels.values.ravel()\n",
    ")\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "model = log_grid.best_estimator_\n",
    "\n",
    "model.fit(X_train, y_train)\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "acc = accuracy_score(y_test, y_pred)\n",
    "balanced_acc = accuracy_score(y_test, y_pred)\n",
    "f1 = f1_score(y_test, y_pred)\n",
    "conf_matrix = pd.DataFrame(confusion_matrix(y_test, y_pred))\n",
    "\n",
    "print('Accuracy: %0.4f' % acc)\n",
    "print('Balanced Accuracy: %0.4f' % balanced_acc)\n",
    "print('F1 Score: %0.4f' % f1)\n",
    "print('\\nConfusion Matrix:')\n",
    "display(conf_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature weighting for logistic regression\n",
      "\n",
      "\t-0.8971 do_raw\n",
      "\t 0.8843 do_sat\n",
      "\t 0.5896 algalBloomSheen_one_week\n",
      "\t 0.5215 sin_month\n",
      "\t-0.4930 DAILYAverageSeaLevelPressure\n",
      "\t 0.3825 algalBloomSheen_one_day\n",
      "\t 0.3630 DAILYAverageStationPressure\n",
      "\t 0.2800 par_below\n",
      "\t-0.2313 DAILYDeptFromNormalAverageTemp\n",
      "\t-0.1865 pco2_ppm\n",
      "\t 0.1809 DAILYAverageWetBulbTemp\n",
      "\t 0.1782 turbidity\n",
      "\t 0.1527 cos_wind_dir\n",
      "\t 0.1489 sin_wind_dir\n",
      "\t-0.1439 DAILYPrecip_one_day\n",
      "\t 0.1425 DAILYPrecip\n",
      "\t 0.1415 algalBloomSheen_three_day\n",
      "\t-0.1280 DAILYAverageDewPointTemp\n",
      "\t-0.1217 par\n",
      "\t 0.0959 phycocyanin\n",
      "\t 0.0903 DAILYPrecip_three_day\n",
      "\t-0.0825 wind_speed\n",
      "\t 0.0552 DAILYMinimumDryBulbTemp\n",
      "\t 0.0479 DAILYAverageDryBulbTemp\n",
      "\t-0.0464 rel_hum\n",
      "\t 0.0422 DAILYPrecip_one_week\n",
      "\t 0.0330 do_wtemp\n",
      "\t 0.0296 air_temp\n",
      "\t 0.0178 chlor\n",
      "\t-0.0085 DAILYMaximumDryBulbTemp\n",
      "\t-0.0042 cos_month\n"
     ]
    }
   ],
   "source": [
    "# for use with logistic regression only\n",
    "coef_sort_idx = np.argsort(-np.abs(model.coef_[0]), kind='mergesort')\n",
    "\n",
    "print('Feature weighting for logistic regression\\n')\n",
    "for idx in coef_sort_idx:\n",
    "    coef = model.coef_[0][idx]\n",
    "    \n",
    "    if coef < 0:\n",
    "        print('\\t%0.4f' % model.coef_[0][idx], data.columns[idx])\n",
    "    else:\n",
    "        print('\\t %0.4f' % model.coef_[0][idx], data.columns[idx])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run 1\n",
    "{'C': 1, 'penalty': 'l1'}\n",
    "\n",
    "Accuracy: 0.7566\n",
    "Balanced Accuracy: 0.7566\n",
    "F1 Score: 0.6667\n",
    "\n",
    "Confusion Matrix:\n",
    "    0\t1\n",
    "0\t97\t29\n",
    "1\t17\t46"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multi-layer Perceptron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Support Vector Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
