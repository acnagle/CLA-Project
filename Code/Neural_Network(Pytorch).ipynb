{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Network for CLA Project"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import statements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "from sklearn import model_selection\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.utils.data as utils\n",
    "import numpy as np\n",
    "import errno\n",
    "import os\n",
    "import sys\n",
    "import Constants"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data processing\n",
    "sample_bias = 0     # adjust the difference in the number of the two types of samples (no algae vs algae)\n",
    "test_size = 0.2\n",
    "batch_size = 60    # batch size for the DataLoaders\n",
    "\n",
    "# NN model\n",
    "num_features = 17\n",
    "input_size = num_features     # size of input layer\n",
    "multiplier = 12               # multiplied by num_features to determine the size of each hidden layer\n",
    "learning_rate = 0.001         # learning rate of optimizer\n",
    "num_epochs = 3                # number of epochs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read in data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.set_printoptions(threshold=np.inf)  # prints a full matrix rather than an abbreviated matrix\n",
    "\n",
    "# define data and destination paths\n",
    "dest_path = \"/Users/Alliot/Documents/CLA-Project/Data/all-data-no-na/neural-network/\"\n",
    "data_path = \"/Users/Alliot/Documents/CLA-Project/Data/data-sets/\"\n",
    "data_set = \"data_2017_summer\"\n",
    "\n",
    "# if dest_path does not exist, create it\n",
    "if not os.path.exists(dest_path):\n",
    "    try:\n",
    "        os.makedirs(dest_path)\n",
    "    except OSError as e:\n",
    "        if e.errno != errno.EEXIST:\n",
    "            raise\n",
    "\n",
    "# load data sets\n",
    "X = np.load(data_path + data_set + \".npy\")\n",
    "y = np.load(data_path + data_set + \"_labels.npy\")\n",
    "\n",
    "# manipulate data set. labels are converted to -1, +1 for binary classification; samples are removed uniformly \n",
    "# from the data set so that the disproportionately large number of negative samples (no algae) does \n",
    "# not bias the model.\n",
    "\n",
    "num_alg = 0  # count the number of algae instances\n",
    "num_no_alg = 0  # count the number of no algae instances\n",
    "\n",
    "# Convert labels to binary: -1 for no algae and 1 for algae\n",
    "for i in range(0, len(y)):\n",
    "    if y[i] == 0:\n",
    "        y[i] = -1\n",
    "        num_no_alg += 1\n",
    "    if y[i] == 1 or y[i] == 2:\n",
    "        y[i] = 1\n",
    "        num_alg += 1\n",
    "\n",
    "# shrink the data set by randomly removing occurences of no algae until the number of no algae samples equals the\n",
    "# number of algae samples minus the sample_bias\n",
    "idx = 0  # index for the data set\n",
    "while num_no_alg != (num_alg - sample_bias):\n",
    "    # circle through the data set until the difference of num_no_alg and num_alg equals\n",
    "    # the value specified by sample_bias\n",
    "    if idx == (len(y) - 1):\n",
    "        idx = 0\n",
    "        \n",
    "    if y[idx] == -1:\n",
    "        if np.random.rand() >= 0.5:  # remove this sample with some probability\n",
    "            y = np.delete(y, obj=idx)\n",
    "            X = np.delete(X, obj=idx, axis=Constants.ROWS)\n",
    "            num_no_alg -= 1\n",
    "        else:\n",
    "            idx += 1\n",
    "    else:\n",
    "        idx += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Process and split data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# standardize data: remove the mean and variance in each sample\n",
    "X = preprocessing.scale(X, axis=1, with_mean=True, with_std=True)\n",
    "\n",
    "num_splits = 2   # do not change\n",
    "sss = model_selection.StratifiedShuffleSplit(n_splits=num_splits, test_size=test_size)\n",
    "\n",
    "idx, _ = sss.split(X, y);\n",
    "train_idx = idx[0]\n",
    "test_idx = idx[1]\n",
    "X_train, X_test = X[train_idx], X[test_idx]\n",
    "y_train, y_test = y[train_idx], y[test_idx]\n",
    "\n",
    "# convert numpy arrays to pytorch tensors\n",
    "X_train, X_test = torch.from_numpy(X_train), torch.from_numpy(X_test)\n",
    "y_train, y_test = torch.from_numpy(y_train), torch.from_numpy(y_test)\n",
    "\n",
    "# convert pytorch tensors to pytorch TensorDataset\n",
    "train_set = utils.TensorDataset(X_train, y_train)\n",
    "test_set = utils.TensorDataset(X_test, y_test)\n",
    "\n",
    "# create DataLoaders\n",
    "train_loader = utils.DataLoader(train_set, batch_size=batch_size, shuffle=True)\n",
    "test_loader = utils.DataLoader(test_set, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define neural network model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CLANet(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(CLANet, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(hidden_size, hidden_size)\n",
    "        self.relu2 = nn.ReLU()\n",
    "        self.fc3 = nn.Linear(hidden_size, hidden_size)\n",
    "        self.relu3 = nn.ReLU()\n",
    "        self.fc4 = nn.Linear(hidden_size, hidden_size)\n",
    "        self.relu4 = nn.ReLU()\n",
    "        self.fc5 = nn.Linear(hidden_size, hidden_size)\n",
    "        self.relu5 = nn.ReLU()\n",
    "        self.fc6 = nn.Linear(hidden_size, hidden_size)\n",
    "        self.relu6 = nn.ReLU()\n",
    "        self.fc7 = nn.Linear(hidden_size, hidden_size)\n",
    "        self.relu7 = nn.ReLU()\n",
    "        self.fc8 = nn.Linear(hidden_size, hidden_size)\n",
    "        self.relu8 = nn.ReLU()\n",
    "        self.fc9 = nn.Linear(hidden_size, hidden_size)\n",
    "        self.relu9 = nn.ReLU()\n",
    "        self.fc10 = nn.Linear(hidden_size, hidden_size)\n",
    "        self.relu10 = nn.ReLU()\n",
    "        self.fc11 = nn.Linear(hidden_size, hidden_size)\n",
    "        self.relu11 = nn.ReLU()\n",
    "        self.fc12 = nn.Linear(hidden_size, output_size)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        out = self.fc1(x)\n",
    "        out = self.relu1(out)\n",
    "        out = self.fc2(out)\n",
    "        out = self.relu2(out)\n",
    "        out = self.fc3(out)\n",
    "        out = self.relu3(out)\n",
    "        out = self.fc4(out)\n",
    "        out = self.relu4(out)\n",
    "        out = self.fc5(out)\n",
    "        out = self.relu5(out)\n",
    "        out = self.fc6(out)\n",
    "        out = self.relu6(out)\n",
    "        out = self.fc7(out)\n",
    "        out = self.relu7(out)\n",
    "        out = self.fc8(out)\n",
    "        out = self.relu8(out)\n",
    "        out = self.fc9(out)\n",
    "        out = self.relu9(out)\n",
    "        out = self.fc10(out)\n",
    "        out = self.relu10(out)\n",
    "        out = self.fc11(out)\n",
    "        out = self.relu11(out)\n",
    "        out = self.fc12(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Instantiate the neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = CLANet(num_features, multiplier * num_features, 1)\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate, nesterov=True, momentum=0.9, dampening=0)\n",
    "model.double();     # cast model parameters to double"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/3\n",
      "  Iteration: 1/8, Error: 0.6000, Loss: 0.513262\n",
      "  Iteration: 2/8, Error: 0.6000, Loss: 0.513262\n",
      "  Iteration: 3/8, Error: 0.5000, Loss: 0.313262\n",
      "  Iteration: 4/8, Error: 0.3833, Loss: 0.0799284\n",
      "  Iteration: 5/8, Error: 0.4500, Loss: 0.213262\n",
      "  Iteration: 6/8, Error: 0.4333, Loss: 0.179928\n",
      "  Iteration: 7/8, Error: 0.5667, Loss: 0.446595\n",
      "  Iteration: 8/8, Error: 0.4600, Loss: 0.233262\n",
      "Epoch: 2/3\n",
      "  Iteration: 1/8, Error: 0.4333, Loss: 0.179928\n",
      "  Iteration: 2/8, Error: 0.4500, Loss: 0.213262\n",
      "  Iteration: 3/8, Error: 0.4833, Loss: 0.279928\n",
      "  Iteration: 4/8, Error: 0.4167, Loss: 0.146595\n",
      "  Iteration: 5/8, Error: 0.6000, Loss: 0.513262\n",
      "  Iteration: 6/8, Error: 0.5000, Loss: 0.313262\n",
      "  Iteration: 7/8, Error: 0.5500, Loss: 0.413262\n",
      "  Iteration: 8/8, Error: 0.5800, Loss: 0.473262\n",
      "Epoch: 3/3\n",
      "  Iteration: 1/8, Error: 0.5833, Loss: 0.479928\n",
      "  Iteration: 2/8, Error: 0.5667, Loss: 0.446595\n",
      "  Iteration: 3/8, Error: 0.5500, Loss: 0.413262\n",
      "  Iteration: 4/8, Error: 0.3833, Loss: 0.0799284\n",
      "  Iteration: 5/8, Error: 0.4500, Loss: 0.213262\n",
      "  Iteration: 6/8, Error: 0.5167, Loss: 0.346595\n",
      "  Iteration: 7/8, Error: 0.4667, Loss: 0.246595\n",
      "  Iteration: 8/8, Error: 0.4800, Loss: 0.273262\n"
     ]
    }
   ],
   "source": [
    "model.train()     # training mode\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    print(\"Epoch: %d/%d\" %(epoch+1, num_epochs))\n",
    "    \n",
    "    for i, (samples, labels) in enumerate(train_loader):\n",
    "        optimizer.zero_grad()     # clear gradient\n",
    "        \n",
    "        output = torch.sign(model(samples))    # forward pass\n",
    "        output = torch.flatten(output)         # resize predicted labels\n",
    "        labels = labels.type(torch.DoubleTensor)\n",
    "        \n",
    "        loss = criterion(output, labels)  # calculate loss\n",
    "        loss.backward()           # calculate gradients\n",
    "        optimizer.step()          # update weights\n",
    "        \n",
    "        # calculate and print error\n",
    "        error = 1 - torch.sum(output == labels).item() / labels.size()[0]\n",
    "        print(\"  Iteration: %d/%d, Error: %0.4f, Loss: %g\" % \n",
    "              (i+1, np.ceil(X_train.size()[0] / batch_size).astype(int), error, loss.item()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
