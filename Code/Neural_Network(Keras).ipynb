{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Network for CLA Project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# import statements\n",
    "from tensorflow import nn\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "from keras import optimizers\n",
    "from sklearn import preprocessing\n",
    "from sklearn import model_selection\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import numpy as np\n",
    "import errno\n",
    "import os\n",
    "import sys\n",
    "import Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This method calculates the Balanced Error Rate (BER), and the error rates for no algae and algae prediction. This\n",
    "# method accepts an array of predicted labels, pred_labels, and an array of target labels, target_labels. This method\n",
    "# returns ber (the balanced error rate), no_alg_error (error rate for no algae prediction), and alg_error (error\n",
    "# rate for algae prediction). The confusion matrix, mat_conf, is returned as well (see first comment in method for a\n",
    "# description of a confusion matrix).\n",
    "def calculate_error(pred_labels, target_labels):\n",
    "    # Construct a confusion matrix, mat_conf. A confusion matrix consists of the true labels for the data points\n",
    "    # along its rows, and the predicted labels from k-nearest neighbors along its columns. The confusion matrix will\n",
    "    # be necessary to calculate BER and other relevant errors for evaluation of the kernel trick with linear\n",
    "    # classification. mat_conf is a 2x2 matrix because we only have two labels: no algae and algae. Each entry in\n",
    "    # mat_conf is the sum of occurrences of each predicted label for each true label, aka the confusion matrix.\n",
    "    mat_conf = np.zeros(shape=(2, 2), dtype=int)\n",
    "\n",
    "    if len(pred_labels) != len(target_labels):\n",
    "        print(\"Predicted and target label arrays are not the same length!\")\n",
    "        sys.exit()\n",
    "\n",
    "    mat_conf = confusion_matrix(y_test, y_predict)  # get the confusion matrix\n",
    "\n",
    "    # Calculate relevant errors and accuracies\n",
    "    # Given a confusion matrix as follows:\n",
    "    # [ a b ]\n",
    "    # [ c d ]\n",
    "    # We can define the following equations:\n",
    "    # Balanced Error Rate (BER) = (b / (a + b) + c / (c + d)) / 2\n",
    "    # error per label = each of the terms in the numerator of BER. ex: b / (a + b)\n",
    "\n",
    "    ber = (mat_conf[0, 1] / (mat_conf[0, 0] + mat_conf[0, 1]) + mat_conf[1, 0] / (mat_conf[1, 1] + mat_conf[1, 0])) / 2\n",
    "\n",
    "    no_alg_error = mat_conf[0, 1] / (mat_conf[0, 0] + mat_conf[0, 1])\n",
    "    alg_error = mat_conf[1, 0] / (mat_conf[1, 1] + mat_conf[1, 0])\n",
    "\n",
    "    ber = float(\"%0.4f\" % ber)\n",
    "    no_alg_error = float(\"%0.4f\" % no_alg_error)\n",
    "    alg_error = float(\"%0.4f\" % alg_error)\n",
    "\n",
    "    return ber, no_alg_error, alg_error, mat_conf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.set_printoptions(threshold=np.inf)  # prints a full matrix rather than an abbreviated matrix\n",
    "\n",
    "# read in data\n",
    "\n",
    "# define data and destination paths\n",
    "dest_path = \"/Users/Alliot/Documents/CLA-Project/Data/all-data-no-na/neural-network/\"\n",
    "data_path = \"/Users/Alliot/Documents/CLA-Project/Data/data-sets/\"\n",
    "data_set = \"data_2017_summer\"\n",
    "\n",
    "# if dest_path does not exist, create it\n",
    "if not os.path.exists(dest_path):\n",
    "    try:\n",
    "        os.makedirs(dest_path)\n",
    "    except OSError as e:\n",
    "        if e.errno != errno.EEXIST:\n",
    "            raise\n",
    "\n",
    "# load data sets\n",
    "X = np.load(data_path + data_set + \".npy\")\n",
    "y = np.load(data_path + data_set + \"_labels.npy\")\n",
    "\n",
    "# manipulate data set. labels are converted to -1, +1 for binary classification; samples are removed uniformly \n",
    "# from the data set so that the disproportionately large number of negative samples (no algae) does \n",
    "# not bias the model.\n",
    "\n",
    "num_alg = 0  # count the number of algae instances\n",
    "num_no_alg = 0  # count the number of no algae instances\n",
    "\n",
    "# Convert labels to binary: -1 for no algae and 1 for algae\n",
    "for i in range(0, len(y)):\n",
    "    if y[i] == 0:\n",
    "        y[i] = -1\n",
    "        num_no_alg += 1\n",
    "    if y[i] == 1 or y[i] == 2:\n",
    "        y[i] = 1\n",
    "        num_alg += 1\n",
    "\n",
    "# shrink the data set by randomly removing occurences of no algae until the number of no algae samples equals the\n",
    "# number of algae samples minus the sample_bias\n",
    "idx = 0  # index for the data set\n",
    "sample_bias = 0  # adjust the difference in the number of the two types of samples (no_alg and alg)\n",
    "while num_no_alg != (num_alg - sample_bias):\n",
    "    # circle through the data set until the difference of num_no_alg and num_alg equals\n",
    "    # the value specified by sample_bias\n",
    "    if idx == (len(y) - 1):\n",
    "        idx = 0\n",
    "        \n",
    "    if y[idx] == -1:\n",
    "        if np.random.rand() >= 0.5:  # remove this sample with some probability\n",
    "            y = np.delete(y, obj=idx)\n",
    "            X = np.delete(X, obj=idx, axis=Constants.ROWS)\n",
    "            num_no_alg -= 1\n",
    "        else:\n",
    "            idx += 1\n",
    "    else:\n",
    "        idx += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# process and split data set\n",
    "X = preprocessing.scale(X, axis=1)  # standardize data: remove the mean and variance in each sample\n",
    "\n",
    "num_splits = 2\n",
    "test_size = 0.2\n",
    "sss = model_selection.StratifiedShuffleSplit(n_splits=num_splits, test_size=test_size)\n",
    "\n",
    "idx, _ = sss.split(X, y);\n",
    "train_idx = idx[0]\n",
    "test_idx = idx[1]\n",
    "X_train, X_test = X[train_idx], X[test_idx]\n",
    "y_train, y_test = y[train_idx], y[test_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "('Could not interpret optimizer identifier:', <keras.optimizers.SGD object at 0x1a39aedf98>)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-6784baf09a5e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[0moptimizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msgd\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[0mloss\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"binary_crossentropy\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m     \u001b[0mmetrics\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"accuracy\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m )\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mcompile\u001b[0;34m(self, optimizer, loss, metrics, loss_weights, sample_weight_mode, weighted_metrics, target_tensors, **kwargs)\u001b[0m\n\u001b[1;32m    177\u001b[0m       \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Only TF native optimizers are supported in Eager mode.'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    178\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 179\u001b[0;31m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moptimizers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    180\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    181\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetrics\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmetrics\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/keras/optimizers.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(identifier)\u001b[0m\n\u001b[1;32m    850\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0midentifier\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    851\u001b[0m   \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 852\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Could not interpret optimizer identifier:'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0midentifier\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m: ('Could not interpret optimizer identifier:', <keras.optimizers.SGD object at 0x1a39aedf98>)"
     ]
    }
   ],
   "source": [
    "# neural network model\n",
    "\n",
    "num_features = X.shape[1]\n",
    "multiplier = 20\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(num_features * multiplier, input_shape=(num_features,), activation=nn.relu))\n",
    "model.add(Dense(num_features * multiplier, input_shape=(num_features,), activation=nn.relu))\n",
    "model.add(Dense(num_features * multiplier, input_shape=(num_features,), activation=nn.relu))\n",
    "model.add(Dense(num_features * multiplier, input_shape=(num_features,), activation=nn.relu))\n",
    "model.add(Dense(num_features * multiplier, input_shape=(num_features,), activation=nn.relu))\n",
    "model.add(Dense(num_features * multiplier, input_shape=(num_features,), activation=nn.relu))\n",
    "model.add(Dense(num_features * multiplier, activation=nn.relu))\n",
    "model.add(Dense(1, activation=nn.softmax))\n",
    "\n",
    "# sgd = optimizers.SGD(lr=0.01, momentum=0.0, decay=0.0, nesterov=False)\n",
    "\n",
    "model.compile(\n",
    "    optimizer=\"sgd\",\n",
    "    loss=\"binary_crossentropy\",\n",
    "    metrics=[\"accuracy\"]\n",
    ")\n",
    "\n",
    "model.fit(X_train[0:5, :], y_train[0:5], epochs=30);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply trained model to test sets\n",
    "\n",
    "y_predict = model.predict(X_test)\n",
    "\n",
    "ber, no_alg_error, alg_error, mat_conf = calculate_error(y_predict, y_test)\n",
    "\n",
    "print(\"Results:\")\n",
    "print(\"BER:\", ber)\n",
    "print(\"No Algae Prediction Error:\", no_alg_error)\n",
    "print(\"Algae Prediction Error:\", alg_error)\n",
    "print(\"Confusion Matrix:\")\n",
    "print(mat_conf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
