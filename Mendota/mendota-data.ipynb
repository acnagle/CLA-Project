{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Statements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import StratifiedShuffleSplit, GridSearchCV, train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import (f1_score, roc_auc_score, confusion_matrix, roc_curve, precision_recall_curve, \n",
    "                             accuracy_score, balanced_accuracy_score)\n",
    "from sklearn.naive_bayes import GaussianNB, MultinomialNB\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "import torch.utils.data as d\n",
    "from torchvision import datasets, transforms \n",
    "\n",
    "from matplotlib import rcParams\n",
    "rcParams['font.family'] = 'sans-serif'\n",
    "rcParams['font.sans-serif'] = ['Tahoma']\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "from pandas.plotting import register_matplotlib_converters\n",
    "register_matplotlib_converters()\n",
    "\n",
    "import seaborn as sns\n",
    "\n",
    "pd.options.mode.chained_assignment = None\n",
    "# np.random.seed(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read in Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = '../Data/'\n",
    "\n",
    "filenames = [\n",
    "#     'CM2014_edit.csv',\n",
    "    'CM2015_edit.csv',\n",
    "    'CM2016_edit.csv',\n",
    "    'CM2017_edit.csv',\n",
    "    'CM2018_edit.csv',\n",
    "    'mdcp.csv',\n",
    "    'major_ion.csv',\n",
    "    'Weather_Data.csv'\n",
    "]\n",
    "\n",
    "# cla_2014 = pd.read_csv(data_path + filenames[0], low_memory=False)\n",
    "cla_2015_raw = pd.read_csv(data_path + filenames[0], low_memory=False)\n",
    "cla_2016_raw = pd.read_csv(data_path + filenames[1], low_memory=False)\n",
    "cla_2017_raw = pd.read_csv(data_path + filenames[2], low_memory=False)\n",
    "cla_2018_raw = pd.read_csv(data_path + filenames[3], low_memory=False)\n",
    "mdcp_raw = pd.read_csv(data_path + filenames[4], low_memory=False)    # Mendota buoy\n",
    "weather_raw = pd.read_csv(data_path + filenames[6], error_bad_lines=False, low_memory=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CLA Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keep15 = [     # features to keep for years 2015-2017\n",
    "    'correct_timestamp',\n",
    "    'collectionSiteId',\n",
    "    'lake',\n",
    "    'algalBloom',\n",
    "    'algalBloomSheen',\n",
    "    'turbidity',\n",
    "#     'waterTemp',\n",
    "#     'waveIntensity',\n",
    "    'lat',\n",
    "    'long'\n",
    "]\n",
    "\n",
    "keep18 = [    # features to keep for 2018\n",
    "    'sample_collection_time',\n",
    "    'collectionSiteId',\n",
    "    'lake',\n",
    "    'algalBloom',\n",
    "    'algalBloomSheen',\n",
    "    'turbidity',\n",
    "#     'waterTemp',\n",
    "#     'waveIntensity',\n",
    "    'latitiude',\n",
    "    'longitude'\n",
    "]\n",
    "\n",
    "rename15 = {   # rename features for 2015-2017\n",
    "    'collectionSiteId': 'site',\n",
    "    'lat': 'latitude',\n",
    "    'long': 'longitude',\n",
    "    'correct_timestamp': 'date'\n",
    "}\n",
    "\n",
    "rename18 = {   # renamce features for 2018\n",
    "    'collectionSiteId': 'site',\n",
    "    'sample_collection_time': 'date',\n",
    "    'latitiude': 'latitude'\n",
    "}\n",
    "\n",
    "cla_2015 = cla_2015_raw[keep15]\n",
    "cla_2016 = cla_2016_raw[keep15]\n",
    "cla_2017 = cla_2017_raw[keep15]\n",
    "cla_2018 = cla_2018_raw[keep18]\n",
    "\n",
    "cla_2015.rename(rename15, axis='columns', inplace=True)\n",
    "cla_2016.rename(rename15, axis='columns', inplace=True)\n",
    "cla_2017.rename(rename15, axis='columns', inplace=True)\n",
    "cla_2018.rename(rename18, axis='columns', inplace=True)\n",
    "\n",
    "# change data types\n",
    "numeric = [    # list of numeric features\n",
    "    'algalBloom',\n",
    "    'algalBloomSheen',\n",
    "    'turbidity',\n",
    "#     'waterTemp',\n",
    "#     'waveIntensity',\n",
    "    'latitude',\n",
    "    'longitude'\n",
    "]\n",
    "\n",
    "# convert data types\n",
    "features = cla_2015.columns.values\n",
    "\n",
    "for feat in features:\n",
    "    if feat in numeric:\n",
    "        cla_2015[feat] = pd.to_numeric(cla_2015[feat], errors='coerce')\n",
    "        cla_2016[feat] = pd.to_numeric(cla_2016[feat], errors='coerce')\n",
    "        cla_2017[feat] = pd.to_numeric(cla_2017[feat], errors='coerce')\n",
    "        cla_2018[feat] = pd.to_numeric(cla_2018[feat], errors='coerce')\n",
    "    \n",
    "    if feat in ['site', 'lake']:\n",
    "        cla_2015[feat] = cla_2015[feat].astype(str)\n",
    "        cla_2016[feat] = cla_2016[feat].astype(str)\n",
    "        cla_2017[feat] = cla_2017[feat].astype(str)\n",
    "        cla_2018[feat] = cla_2018[feat].astype(str)\n",
    "    \n",
    "    if feat == 'date':\n",
    "        cla_2015[feat] = pd.to_datetime(cla_2015[feat], errors='coerce')\n",
    "        cla_2016[feat] = pd.to_datetime(cla_2016[feat], errors='coerce')\n",
    "        cla_2017[feat] = pd.to_datetime(cla_2017[feat], errors='coerce')\n",
    "        cla_2018[feat] = pd.to_datetime(cla_2018[feat], errors='coerce')\n",
    "        \n",
    "# remove nans\n",
    "cla_2015.dropna(axis='rows', how='any', inplace=True)\n",
    "cla_2016.dropna(axis='rows', how='any', inplace=True)\n",
    "cla_2017.dropna(axis='rows', how='any', inplace=True)\n",
    "cla_2018.dropna(axis='rows', how='any', inplace=True)\n",
    "\n",
    "# remove any data point not on lake mendota\n",
    "cla_2015 = cla_2015[cla_2015['lake'].str.contains('Mendota')]\n",
    "cla_2016 = cla_2016[cla_2016['lake'].str.contains('Mendota')]\n",
    "cla_2017 = cla_2017[cla_2017['lake'].str.contains('Mendota')]\n",
    "cla_2018 = cla_2018[cla_2018['lake'].str.contains('Mendota')]\n",
    "\n",
    "# set date as index\n",
    "cla_2015.set_index('date', inplace=True)\n",
    "cla_2016.set_index('date', inplace=True)\n",
    "cla_2017.set_index('date', inplace=True)\n",
    "cla_2018.set_index('date', inplace=True)\n",
    "\n",
    "# sort data by dates\n",
    "cla_2015.sort_values(by='date', inplace=True)\n",
    "cla_2016.sort_values(by='date', inplace=True)\n",
    "cla_2017.sort_values(by='date', inplace=True)\n",
    "cla_2018.sort_values(by='date', inplace=True)\n",
    "\n",
    "# resample, ffill and bfill\n",
    "cla_2015 = cla_2015.resample('D').mean()\n",
    "cla_2015.ffill(inplace=True)\n",
    "cla_2015.bfill(inplace=True)\n",
    "\n",
    "for date in cla_2015.index:\n",
    "    if cla_2015.loc[date, 'algalBloomSheen'] > 0:\n",
    "        cla_2015.loc[date, 'algalBloomSheen'] = 1\n",
    "\n",
    "cla_2016 = cla_2016.resample('D').mean()\n",
    "cla_2016.ffill(inplace=True)\n",
    "cla_2016.bfill(inplace=True)\n",
    "\n",
    "for date in cla_2016.index:\n",
    "    if cla_2016.loc[date, 'algalBloomSheen'] > 0:\n",
    "        cla_2016.loc[date, 'algalBloomSheen'] = 1\n",
    "\n",
    "cla_2017 = cla_2017.resample('D').mean()\n",
    "cla_2017.ffill(inplace=True)\n",
    "cla_2017.bfill(inplace=True)\n",
    "\n",
    "for date in cla_2017.index:\n",
    "    if cla_2017.loc[date, 'algalBloomSheen'] > 0:\n",
    "        cla_2017.loc[date, 'algalBloomSheen'] = 1\n",
    "\n",
    "cla_2018 = cla_2018.resample('D').mean()\n",
    "cla_2018.ffill(inplace=True)\n",
    "cla_2018.bfill(inplace=True)\n",
    "\n",
    "for date in cla_2018.index:\n",
    "    if cla_2018.loc[date, 'algalBloomSheen'] > 0:\n",
    "        cla_2018.loc[date, 'algalBloomSheen'] = 1\n",
    "        \n",
    "# only keep the dates of the official sampling season of each year\n",
    "# cla_2015 = cla_2015[(cla_2015.index >= '2015-5-18') & (cla_2015.index <= '2015-9-4')]\n",
    "# cla_2016 = cla_2016[(cla_2016.index >= '2016-5-25') & (cla_2016.index <= '2016-9-4')]\n",
    "# cla_2017 = cla_2017[(cla_2017.index >= '2017-5-25') & (cla_2017.index <= '2017-9-4')]\n",
    "# cla_2018 = cla_2018[(cla_2018.index >= '2018-5-25') & (cla_2018.index <= '2018-9-4')]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MDCP Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keep_mdcp = [\n",
    "    'sampledate',\n",
    "    'sampletime',\n",
    "    'air_temp',\n",
    "    'rel_hum',\n",
    "    'wind_speed',\n",
    "    'wind_dir',\n",
    "    'chlor',\n",
    "    'phycocyanin',\n",
    "    'do_raw',\n",
    "    'do_sat',\n",
    "    'do_wtemp',\n",
    "    'pco2_ppm',\n",
    "    'par',\n",
    "    'par_below'\n",
    "]\n",
    "\n",
    "mdcp = mdcp_raw[keep_mdcp]\n",
    "mdcp.ffill(inplace=True)\n",
    "mdcp.bfill(inplace=True)\n",
    "\n",
    "mdcp['date'] = mdcp['sampledate'] + ' ' + mdcp['sampletime']\n",
    "mdcp['date'] = pd.to_datetime(mdcp['date'], errors='coerce')\n",
    "mdcp.dropna(axis='rows', how='any', inplace=True)\n",
    "\n",
    "mdcp = mdcp[[\n",
    "    'date',\n",
    "    'air_temp',\n",
    "    'rel_hum',\n",
    "    'wind_speed',\n",
    "    'wind_dir',\n",
    "    'chlor', \n",
    "    'phycocyanin',\n",
    "    'do_raw',\n",
    "    'do_sat',\n",
    "    'do_wtemp',\n",
    "    'pco2_ppm',\n",
    "    'par',\n",
    "    'par_below'\n",
    "]]\n",
    "mdcp.set_index('date', inplace=True)\n",
    "\n",
    "mdcp = mdcp.resample('D').mean()\n",
    "mdcp.ffill(inplace=True)\n",
    "mdcp.bfill(inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Weather Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keep_weather = [\n",
    "    'DATE',\n",
    "    'REPORTTPYE',\n",
    "    'DAILYMaximumDryBulbTemp',\n",
    "    'DAILYMinimumDryBulbTemp',\n",
    "    'DAILYAverageDryBulbTemp',\n",
    "    'DAILYDeptFromNormalAverageTemp',\n",
    "    'DAILYAverageDewPointTemp',\n",
    "    'DAILYAverageWetBulbTemp',\n",
    "    'DAILYPrecip',\n",
    "    'DAILYAverageStationPressure',\n",
    "    'DAILYAverageSeaLevelPressure'\n",
    "]\n",
    "\n",
    "weather = weather_raw[keep_weather]\n",
    "# weather['REPORTTPYE'].dropna(axis='rows', how='any', inplace=True)\n",
    "weather = weather.iloc[:-1]  # remove last entry since it has NaN in REPORTTPYE\n",
    "\n",
    "weather = weather[weather['REPORTTPYE'].str.contains('SOD')]    # only keep summary of day (SOD) info\n",
    "weather = weather.drop(['REPORTTPYE'], axis='columns')\n",
    "weather['DATE'] = pd.to_datetime(weather['DATE'], errors='coerce')\n",
    "\n",
    "weather.set_index('DATE', inplace=True)\n",
    "weather = weather.resample('D').ffill()\n",
    "weather.ffill(inplace=True)\n",
    "weather.bfill(inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Join CLA, MDCP, and Weather Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Append CLA data\n",
    "cla = cla_2015.append(cla_2016)\n",
    "cla = cla.append(cla_2017)\n",
    "cla = cla.append(cla_2018)\n",
    "\n",
    "# Insert MDCP data\n",
    "data = cla.join(mdcp, how='inner')\n",
    "\n",
    "# Insert weather data\n",
    "data = data.join(weather, how='inner')\n",
    "\n",
    "# sine/cosine transformation of month of year and wind direction\n",
    "data['cos_month'] = np.cos(2 * np.pi * (data.index.month.values / 12))\n",
    "data['sin_month'] = np.sin(2 * np.pi * (data.index.month.values / 12))\n",
    "\n",
    "data['cos_wind_dir'] = np.cos(2 * np.pi * (data['wind_dir'] / 12))\n",
    "data['sin_wind_dir'] = np.sin(2 * np.pi * (data['wind_dir'] / 12))\n",
    "data = data.drop(['wind_dir'], axis='columns')\n",
    "\n",
    "# Replace 'T' and 's' in 'DAILYPrecip' column\n",
    "for date in data.index:\n",
    "    if 'T' in data.loc[date, 'DAILYPrecip']:\n",
    "        data.loc[date, 'DAILYPrecip'] = 0.01\n",
    "    elif 's' in data.loc[date, 'DAILYPrecip']:\n",
    "        data.loc[date, 'DAILYPrecip'] = 0\n",
    "\n",
    "# Make every feature numeric\n",
    "for col in data.columns.values:\n",
    "    if type(data[col].values[0]) != np.float64:\n",
    "        data[col] = pd.to_numeric(data[col], errors='coerce')\n",
    "        \n",
    "# create indicator features for whether there was rain or a bloom one day ago, or within three days or a week ago\n",
    "precip = (data['DAILYPrecip'] > 0).astype(int)   # convert precipitation to boolean values\n",
    "# data['DAILYPrecip_one_day'] = precip.shift(1)\n",
    "# data['DAILYPrecip_three_day'] = precip.rolling(window=3, min_periods=1).sum()    # NOTE THAT THIS IS DEPENDENT ON CURRENT VALUE\n",
    "# data['DAILYPrecip_one_week'] = precip.rolling(window=7, min_periods=1).sum()\n",
    "\n",
    "# data['algalBloomSheen_one_day'] = data['algalBloomSheen'].shift(1)\n",
    "# data['algalBloomSheen_three_day'] = data[['algalBloomSheen']].shift(1).rolling(3).sum()\n",
    "# data['algalBloomSheen_one_week'] = data[['algalBloomSheen']].shift(1).rolling(7).sum()\n",
    "\n",
    "# shift algalbloomsheen by -1 to predict next day algal bloom\n",
    "data['DAILYPrecip_one_day'] = precip\n",
    "data['DAILYPrecip_three_day'] = precip.rolling(window=3, min_periods=1).sum()    # NOTE THAT THIS IS DEPENDENT ON CURRENT VALUE\n",
    "data['DAILYPrecip_one_week'] = precip.rolling(window=7, min_periods=1).sum()\n",
    "data['algalBloomSheen_one_day'] = data['algalBloomSheen']\n",
    "data['algalBloomSheen_three_day'] = data[['algalBloomSheen']].rolling(3, min_periods=1).sum()\n",
    "data['algalBloomSheen_one_week'] = data[['algalBloomSheen']].rolling(7, min_periods=1).sum()\n",
    "data['algalBloomSheen'] = data['algalBloomSheen'].shift(-1)\n",
    "\n",
    "data.dropna(axis='rows', how='any', inplace=True)\n",
    "\n",
    "# display(data[['DAILYPrecip',\n",
    "#       'DAILYPrecip_one_day',\n",
    "#       'DAILYPrecip_three_day',\n",
    "#       'DAILYPrecip_one_week',\n",
    "#       'algalBloomSheen',\n",
    "#       'algalBloomSheen_one_day',\n",
    "#       'algalBloomSheen_three_day',\n",
    "#       'algalBloomSheen_one_week'\n",
    "#      ]].head(15))\n",
    "\n",
    "labels = data[['algalBloomSheen']]\n",
    "data = data.drop(['latitude', 'longitude', 'algalBloom', 'algalBloomSheen'], axis='columns')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Format data into 2D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'torch.utils.data' has no attribute 'index'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-27-b8d302e89163>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mpad\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m636\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# TODO TRY USING DIFFERENT FUNCTIONS OF DIFFERENT FEATURES... ONE HOT ENCODING??\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mdata_pad\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpad\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhow\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'inner'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: module 'torch.utils.data' has no attribute 'index'"
     ]
    }
   ],
   "source": [
    "pad = pd.DataFrame(np.zeros((636, 5)), index=data.index)\n",
    "\n",
    "# TODO TRY USING DIFFERENT FUNCTIONS OF DIFFERENT FEATURES... ONE HOT ENCODING??\n",
    "data_pad = data.join(pad, how='inner')\n",
    "\n",
    "label_2d = labels.values\n",
    "data_2d = []\n",
    "\n",
    "for i in range(data_pad.shape[0]):\n",
    "    data_2d.append(np.reshape(data_pad.iloc[i].values, newshape=(6, 6)))\n",
    "\n",
    "n = np.random.permutation(len(data_2d))\n",
    "len_train = 0.7\n",
    "train_idx = int(len_train * n)\n",
    "\n",
    "X_train = []\n",
    "y_train = []\n",
    "X_test = []\n",
    "y_test = []\n",
    "\n",
    "for idx in n[train_idx:]:\n",
    "    X_train.append(data_2d[idx])\n",
    "    y_train.append(label_2d[idx])\n",
    "    \n",
    "for idx in n[:train_idx]:\n",
    "    X_test.append(data_2d[idx])\n",
    "    y_test.append(label_2d[idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
